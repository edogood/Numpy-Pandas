{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LEZIONE 12 — Overfitting e Underfitting\n",
        "\n",
        "## Obiettivi della Lezione\n",
        "\n",
        "Al termine di questa lezione sarai in grado di:\n",
        "\n",
        "1. Comprendere il trade-off tra bias e varianza.\n",
        "2. Riconoscere i segnali di overfitting e underfitting nei dati.\n",
        "3. Interpretare le curve di apprendimento (learning curves).\n",
        "4. Diagnosticare problemi di generalizzazione nei modelli.\n",
        "5. Scegliere la complessità appropriata per un modello.\n",
        "\n",
        "## Prerequisiti\n",
        "\n",
        "- Conoscenza di base di regressione e classificazione.\n",
        "- Familiarità con train/test split e metriche semplici.\n",
        "- Lettura di grafici di performance.\n",
        "\n",
        "## Indice\n",
        "\n",
        "1. Teoria concettuale approfondita\n",
        "2. Schema mentale / mappa decisionale\n",
        "3. Notebook dimostrativo\n",
        "4. Metodi spiegati\n",
        "5. Esercizi risolti (guidati)\n",
        "6. Glossario\n",
        "7. Errori comuni e debug rapido\n",
        "8. Conclusione operativa\n",
        "9. End-of-lesson checklist\n",
        "10. Didactic changelog\n",
        "\n",
        "## Librerie Utilizzate\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import learning_curve, train_test_split, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEZIONE 1 — Teoria Concettuale Approfondita\n",
        "\n",
        "## 1.1 — Il problema della generalizzazione\n",
        "\n",
        "L'obiettivo del machine learning non è predire bene i dati già visti, ma predire bene dati nuovi. Questa capacità si chiama generalizzazione.\n",
        "\n",
        "```\n",
        "               TRAINING DATA                    NUOVI DATI\n",
        "              ┌─────────────┐                 ┌─────────────┐\n",
        "              │ ● ● ● ● ● ● │                 │ ? ? ? ? ? ? │\n",
        "              │ ● ● ● ● ● ● │  ─► Modello ─►  │ ? ? ? ? ? ? │\n",
        "              │ ● ● ● ● ● ● │                 │ ? ? ? ? ? ? │\n",
        "              └─────────────┘                 └─────────────┘\n",
        "                   Noti                          Ignoti\n",
        "\n",
        "         Performance qui ≠ Performance qui!\n",
        "```\n",
        "\n",
        "## 1.2 — Due modi per fallire: underfitting e overfitting\n",
        "\n",
        "### Underfitting (bias alto)\n",
        "\n",
        "Il modello è troppo semplice e non cattura i pattern reali.\n",
        "\n",
        "- Errore alto sul training set\n",
        "- Errore alto sul test set\n",
        "- I due errori sono simili\n",
        "\n",
        "### Overfitting (varianza alta)\n",
        "\n",
        "Il modello è troppo complesso e memorizza il rumore.\n",
        "\n",
        "- Errore basso sul training set\n",
        "- Errore alto sul test set\n",
        "- Gap grande tra train e test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 — Bias e varianza: il trade-off\n",
        "\n",
        "L'errore totale di un modello può essere scomposto in tre componenti:\n",
        "\n",
        "$$\text{Errore Totale} = \text{Bias}^2 + \text{Varianza} + \text{Rumore Irriducibile}$$\n",
        "\n",
        "**Spiegazione in parole semplici:**\n",
        "- Bias misura quanto il modello è sistematicamente lontano dal vero.\n",
        "- Varianza misura quanto il modello cambia se cambiano i dati di training.\n",
        "- Il rumore irriducibile è la parte che non possiamo eliminare.\n",
        "\n",
        "Collegheremo questa formula al codice quando confronteremo modelli con diverse complessità.\n",
        "\n",
        "## 1.4 — Learning curves: lettura pratica\n",
        "\n",
        "Le learning curves mostrano come cambia la performance al variare della quantità di dati di training. Confrontando train e validation possiamo diagnosticare:\n",
        "\n",
        "- Underfitting: entrambi i punteggi sono bassi e vicini.\n",
        "- Overfitting: train alto e validation più basso con gap ampio.\n",
        "- Buona generalizzazione: entrambi alti e vicini.\n",
        "\n",
        "## 1.5 — Segnali pratici e rimedi\n",
        "\n",
        "**Segnali tipici:**\n",
        "- Overfitting: accuracy training molto alta e validation molto più bassa.\n",
        "- Underfitting: performance deludente anche sul training set.\n",
        "\n",
        "**Rimedi principali:**\n",
        "- Overfitting: ridurre complessità, regolarizzare, aumentare dati.\n",
        "- Underfitting: aumentare complessità, aggiungere feature, migliorare preprocessing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEZIONE 2 — Schema Mentale / Mappa Decisionale\n",
        "\n",
        "## Diagnosi rapida: albero decisionale\n",
        "\n",
        "```\n",
        "HO ADDESTRATO UN MODELLO\n",
        "          │\n",
        "          ▼\n",
        "    ┌─────────────────┐\n",
        "    │ Train score     │\n",
        "    │    alto?        │\n",
        "    └────────┬────────┘\n",
        "             │\n",
        "     ┌───────┴───────┐\n",
        "     │               │\n",
        "    NO              SI\n",
        "     │               │\n",
        "     ▼               ▼\n",
        "┌────────────┐   ┌─────────────┐\n",
        "│UNDERFITTING│   │Test score    │\n",
        "│            │   │alto?         │\n",
        "│Aumenta     │   └─────┬────────┘\n",
        "│complessita │         │\n",
        "└────────────┘   ┌─────┴───────┐\n",
        "                 │             │\n",
        "                SI            NO\n",
        "                 │             │\n",
        "                 ▼             ▼\n",
        "          ┌──────────┐   ┌─────────────┐\n",
        "          │ OTTIMO   │   │ OVERFITTING │\n",
        "          │          │   │             │\n",
        "          │Generalizza│  │ Riduci      │\n",
        "          └──────────┘   │complessita  │\n",
        "                         │o piu dati   │\n",
        "                         └─────────────┘\n",
        "```\n",
        "\n",
        "## Checklist operativa\n",
        "\n",
        "- Ho abbastanza dati rispetto al numero di feature?\n",
        "- Ho fatto lo split train/test prima del preprocessing?\n",
        "- Il gap train-test è accettabile (circa 5-10%)?\n",
        "- La cross-validation mostra variabilità contenuta?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEZIONE 3 — Notebook Dimostrativo\n",
        "\n",
        "## 3.1 — Bias vs varianza con regressione polinomiale\n",
        "\n",
        "**Perché lo facciamo:** vogliamo vedere come cambia la complessita del modello e come si riflette su bias e varianza. Questo collega la teoria al grafico finale.\n",
        "\n",
        "**Cosa mostra il grafico:** tre curve di modello con gradi diversi, sovrapposte ai dati reali.\n",
        "\n",
        "**Perché conta:** la forma della curva mostra se il modello e troppo semplice (underfitting) o troppo complesso (overfitting).\n",
        "\n",
        "**Come interpretarlo:**\n",
        "- Curva quasi lineare e lontana dai dati: underfitting.\n",
        "- Curva molto ondulata che passa per ogni punto: overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Dimostrazione: Bias vs Varianza con Regressione Polinomiale\n",
        "\n",
        "# Importiamo le librerie necessarie\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Impostiamo il seed per riproducibilita dei risultati\n",
        "np.random.seed(42)\n",
        "\n",
        "# Definiamo la funzione vera (che il modello non conosce)\n",
        "def true_function(x):\n",
        "    # Funzione non lineare usata per generare i dati\n",
        "    return np.sin(1.5 * x)\n",
        "\n",
        "# Generiamo dati con rumore\n",
        "n_samples = 30\n",
        "X = np.sort(np.random.uniform(0, 5, n_samples))\n",
        "y = true_function(X) + np.random.normal(0, 0.3, n_samples)\n",
        "\n",
        "# Trasformiamo X in matrice 2D per scikit-learn\n",
        "X = X.reshape(-1, 1)\n",
        "\n",
        "# Micro-checkpoint: verifichiamo dimensioni e assenza di NaN\n",
        "assert X.shape[0] == y.shape[0], \"X e y devono avere lo stesso numero di righe\"\n",
        "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
        "print(f\"NaN in X: {np.isnan(X).any()}, NaN in y: {np.isnan(y).any()}\")\n",
        "\n",
        "# Punti per la visualizzazione continua\n",
        "X_plot = np.linspace(0, 5, 200).reshape(-1, 1)\n",
        "\n",
        "# Confrontiamo 3 gradi di polinomio\n",
        "degrees = [1, 4, 15]\n",
        "titles = [\n",
        "    \"Grado 1: UNDERFITTING (Bias alto, Varianza bassa)\",\n",
        "    \"Grado 4: Bilanciato (Bias e Varianza moderati)\",\n",
        "    \"Grado 15: OVERFITTING (Bias basso, Varianza alta)\",\n",
        "]\n",
        "\n",
        "# Creiamo i grafici\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
        "\n",
        "for ax, degree, title in zip(axes, degrees, titles):\n",
        "    # Creiamo una pipeline: trasformazione polinomiale + regressione lineare\n",
        "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "\n",
        "    # Alleniamo il modello\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Predizioni su dati continui\n",
        "    y_plot = model.predict(X_plot)\n",
        "\n",
        "    # Calcoliamo l'errore sui dati di training\n",
        "    y_train_pred = model.predict(X)\n",
        "    mse = mean_squared_error(y, y_train_pred)\n",
        "\n",
        "    # Visualizziamo dati e curva del modello\n",
        "    ax.scatter(X, y, color=\"black\", label=\"Dati\")\n",
        "    ax.plot(X_plot, y_plot, color=\"blue\", label=\"Modello\")\n",
        "    ax.set_title(f\"{title}\n",
        "MSE train: {mse:.3f}\")\n",
        "    ax.set_xlabel(\"X\")\n",
        "    ax.set_ylabel(\"y\")\n",
        "\n",
        "# Output atteso: 3 grafici che mostrano l'effetto della complessita\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 — Learning curves per diagnosticare overfitting\n",
        "\n",
        "**Perché lo facciamo:** vogliamo vedere la distanza tra train e validation al crescere dei dati, cosi possiamo diagnosticare underfitting o overfitting.\n",
        "\n",
        "**Cosa mostra il grafico:** per ogni modello, due linee che indicano accuracy su train e validation al crescere dei campioni.\n",
        "\n",
        "**Perché conta:** il gap tra le due curve indica quanta varianza ha il modello.\n",
        "\n",
        "**Come interpretarlo:**\n",
        "- Linee basse e vicine: underfitting.\n",
        "- Linee alte ma distanti: overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Dimostrazione: Learning Curves per Diagnosticare Overfitting\n",
        "\n",
        "# Importiamo le librerie necessarie\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Creiamo un dataset di classificazione\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=5,\n",
        "    flip_y=0.1,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Micro-checkpoint: controlliamo le dimensioni e la distribuzione delle classi\n",
        "assert X.shape[0] == y.shape[0], \"X e y devono avere lo stesso numero di righe\"\n",
        "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
        "print(f\"Distribuzione classi: {np.bincount(y)}\")\n",
        "print(f\"NaN in X: {np.isnan(X).any()}, NaN in y: {np.isnan(y).any()}\")\n",
        "\n",
        "# Definiamo tre modelli con complessita crescente\n",
        "models = {\n",
        "    \"Logistic Regression (Possibile underfitting)\": LogisticRegression(max_iter=1000),\n",
        "    \"Decision Tree depth=5 (Bilanciato)\": DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    \"Decision Tree depth=None (Possibile overfitting)\": DecisionTreeClassifier(max_depth=None, random_state=42),\n",
        "}\n",
        "\n",
        "# Calcoliamo e visualizziamo le learning curves\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 4), sharey=True)\n",
        "\n",
        "for ax, (label, model) in zip(axes, models.items()):\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        estimator=model,\n",
        "        X=X,\n",
        "        y=y,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
        "        cv=5,\n",
        "        scoring=\"accuracy\",\n",
        "        n_jobs=None,\n",
        "    )\n",
        "\n",
        "    # Calcoliamo media e deviazione standard\n",
        "    train_mean = train_scores.mean(axis=1)\n",
        "    val_mean = val_scores.mean(axis=1)\n",
        "\n",
        "    # Visualizziamo le curve\n",
        "    ax.plot(train_sizes, train_mean, \"o-\", label=\"Train score\")\n",
        "    ax.plot(train_sizes, val_mean, \"o-\", label=\"Validation score\")\n",
        "    ax.set_title(label)\n",
        "    ax.set_xlabel(\"Dimensione del training\")\n",
        "    ax.set_ylabel(\"Accuracy\")\n",
        "    ax.legend()\n",
        "\n",
        "# Output atteso: 3 pannelli con curve train/validation\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 — Train vs test score al variare della complessita\n",
        "\n",
        "**Perché lo facciamo:** vogliamo osservare come un iperparametro di complessita (max_depth) cambia il gap tra train e test.\n",
        "\n",
        "**Cosa mostra il grafico:** due linee (train e test) al variare della profondita dell'albero.\n",
        "\n",
        "**Perché conta:** individuiamo la zona in cui il test smette di migliorare mentre il train continua a crescere.\n",
        "\n",
        "**Come interpretarlo:**\n",
        "- Curve vicine e alte: buona generalizzazione.\n",
        "- Train alto e test che cala: overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Dimostrazione: Train vs Test Score al Variare della Complessita\n",
        "\n",
        "# Importiamo le librerie necessarie\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Creiamo il dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=500,\n",
        "    n_features=15,\n",
        "    n_informative=8,\n",
        "    flip_y=0.1,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Split train/test (prima del training, per evitare leakage)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Micro-checkpoint: controlliamo dimensioni e distribuzione\n",
        "assert X_train.shape[0] + X_test.shape[0] == X.shape[0], \"Split incoerente\"\n",
        "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "print(f\"Distribuzione classi train: {np.bincount(y_train)}\")\n",
        "print(f\"Distribuzione classi test: {np.bincount(y_test)}\")\n",
        "\n",
        "# Testiamo diversi valori di max_depth\n",
        "max_depth_values = range(1, 21)\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for depth in max_depth_values:\n",
        "    # Creiamo e addestriamo il modello\n",
        "    model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Valutiamo su train e test\n",
        "    train_scores.append(model.score(X_train, y_train))\n",
        "    test_scores.append(model.score(X_test, y_test))\n",
        "\n",
        "# Visualizziamo il gap train-test\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(max_depth_values, train_scores, label=\"Train score\")\n",
        "plt.plot(max_depth_values, test_scores, label=\"Test score\")\n",
        "plt.xlabel(\"max_depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train vs Test score al variare della complessita\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEZIONE 4 — Metodi Spiegati\n",
        "\n",
        "Di seguito i metodi principali usati nella lezione. Per ciascuno indichiamo cosa fa, input/output, errori tipici e quando usarlo.\n",
        "\n",
        "## `make_classification`\n",
        "- Cosa fa: genera un dataset sintetico di classificazione.\n",
        "- Input: parametri scalari (n_samples, n_features, random_state).\n",
        "- Output: `X` ndarray (n_samples, n_features), `y` ndarray (n_samples,).\n",
        "- Errori tipici: classi troppo sbilanciate se `weights` non impostato.\n",
        "- Quando usarlo: demo e test rapidi; non usarlo per valutazioni reali.\n",
        "\n",
        "## `train_test_split`\n",
        "- Cosa fa: divide i dati in train e test.\n",
        "- Input: `X` (n_samples, n_features), `y` (n_samples,).\n",
        "- Output: `X_train`, `X_test`, `y_train`, `y_test`.\n",
        "- Errori tipici: dimenticare `stratify=y` in classificazione sbilanciata.\n",
        "- Quando usarlo: sempre per una prima validazione.\n",
        "\n",
        "## `learning_curve`\n",
        "- Cosa fa: calcola performance su train e validation al crescere dei dati.\n",
        "- Input: estimator, `X`, `y`, `cv`.\n",
        "- Output: `train_sizes`, `train_scores`, `val_scores`.\n",
        "- Errori tipici: interpretare male il gap train/validation.\n",
        "- Quando usarlo: diagnosi di overfitting/underfitting.\n",
        "\n",
        "## `PolynomialFeatures` + `LinearRegression`\n",
        "- Cosa fa: crea feature polinomiali e adatta una regressione lineare.\n",
        "- Input: `X` 2D, `y` 1D.\n",
        "- Output: modello che predice valori continui.\n",
        "- Errori tipici: usare `X` 1D senza reshape.\n",
        "- Quando usarlo: mostrare l'effetto della complessita in regressione.\n",
        "\n",
        "## `DecisionTreeClassifier`\n",
        "- Cosa fa: albero di decisione per classificazione.\n",
        "- Input: `X` 2D, `y` 1D.\n",
        "- Output: `model` addestrato con `predict` e `score`.\n",
        "- Errori tipici: albero troppo profondo senza regolarizzazione.\n",
        "- Quando usarlo: baseline interpretabile, non quando serve stabilita.\n",
        "\n",
        "## `cross_val_score`\n",
        "- Cosa fa: misura performance con cross-validation.\n",
        "- Input: estimator, `X`, `y`, `cv`.\n",
        "- Output: array di score per ciascun fold.\n",
        "- Errori tipici: usare metriche non appropriate al task.\n",
        "- Quando usarlo: per verificare stabilita del modello.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEZIONE 5 — Esercizi Risolti (Guidati)\n",
        "\n",
        "Gli esercizi sono completamente risolti con guida passo-passo.\n",
        "\n",
        "## Esercizio 12.1: Identificare Overfitting vs Underfitting\n",
        "\n",
        "**Obiettivo:** dato un modello e le sue performance, diagnosticare se c'e overfitting, underfitting o buona generalizzazione.\n",
        "\n",
        "**Procedura:**\n",
        "1. Creare un dataset.\n",
        "2. Allenare tre modelli con complessita diversa.\n",
        "3. Calcolare train/test score e gap.\n",
        "4. Diagnosticare ogni modello.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Perché lo facciamo:** vogliamo confrontare modelli con complessita diversa usando gli stessi dati, per capire come leggere il gap train-test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ESERCIZIO 12.1 — Identificare Overfitting vs Underfitting\n",
        "# ============================================================================\n",
        "\n",
        "# Importiamo le librerie necessarie\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# STEP 1: Creiamo il dataset\n",
        "print(\"STEP 1: Creazione Dataset\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=800,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=5,\n",
        "    flip_y=0.05,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Micro-checkpoint: controlliamo dimensioni e distribuzione classi\n",
        "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
        "print(f\"Distribuzione classi: {np.bincount(y)}\")\n",
        "\n",
        "# STEP 2: Split train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# STEP 3: Definiamo tre modelli con complessita diversa\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Decision Tree depth=3\": DecisionTreeClassifier(max_depth=3, random_state=42),\n",
        "    \"Decision Tree depth=None\": DecisionTreeClassifier(max_depth=None, random_state=42),\n",
        "}\n",
        "\n",
        "# STEP 4: Valutazione e diagnosi\n",
        "print(\"\n",
        "Confronto modelli\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Addestriamo il modello\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Calcoliamo score su train e test\n",
        "    train_score = model.score(X_train, y_train)\n",
        "    test_score = model.score(X_test, y_test)\n",
        "    gap = train_score - test_score\n",
        "\n",
        "    # Cross-validation per stabilita\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "\n",
        "    print(f\"{name}\")\n",
        "    print(f\"  Train score: {train_score:.3f}\")\n",
        "    print(f\"  Test score:  {test_score:.3f}\")\n",
        "    print(f\"  Gap:         {gap:.3f}\")\n",
        "    print(f\"  CV mean/std: {cv_scores.mean():.3f} / {cv_scores.std():.3f}\n",
        "\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Esercizio 12.2: Learning Curve Completa\n",
        "\n",
        "**Obiettivo:** costruire e interpretare una learning curve per diagnosticare problemi di generalizzazione.\n",
        "\n",
        "**Procedura:**\n",
        "1. Creare un dataset.\n",
        "2. Generare la learning curve con `learning_curve` di sklearn.\n",
        "3. Visualizzare train e validation score.\n",
        "4. Determinare se servono piu dati o un modello diverso.\n",
        "\n",
        "**Cosa mostra il grafico:** l'andamento delle performance su train e validation al crescere dei dati.\n",
        "\n",
        "**Come interpretarlo:**\n",
        "- Se le curve sono lontane, serve ridurre la complessita.\n",
        "- Se entrambe sono basse, serve un modello piu espressivo o nuove feature.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Perché lo facciamo:** una learning curve rende visibile il compromesso tra bias e varianza e aiuta a decidere la prossima azione.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ESERCIZIO 12.2 — Learning Curve Completa\n",
        "# ============================================================================\n",
        "\n",
        "# Importiamo le librerie necessarie\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# STEP 1: Creiamo il dataset\n",
        "print(\"STEP 1: Creazione Dataset\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=2000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=5,\n",
        "    flip_y=0.08,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Micro-checkpoint: confermiamo dimensioni e distribuzione\n",
        "print(f\"Dataset: {X.shape[0]} campioni, {X.shape[1]} feature\")\n",
        "print(f\"Distribuzione classi: {np.bincount(y)}\")\n",
        "\n",
        "# STEP 2: Definiamo il modello\n",
        "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "\n",
        "# STEP 3: Calcoliamo la learning curve\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    estimator=model,\n",
        "    X=X,\n",
        "    y=y,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 6),\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        ")\n",
        "\n",
        "# STEP 4: Calcoliamo le medie\n",
        "train_mean = train_scores.mean(axis=1)\n",
        "val_mean = val_scores.mean(axis=1)\n",
        "\n",
        "# STEP 5: Visualizziamo le curve\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(train_sizes, train_mean, \"o-\", label=\"Train score\")\n",
        "plt.plot(train_sizes, val_mean, \"o-\", label=\"Validation score\")\n",
        "plt.xlabel(\"Dimensione del training\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Learning Curve - Random Forest\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Esercizio 12.3: Trovare la Complessita Ottimale\n",
        "\n",
        "**Obiettivo:** usare la validation per trovare il valore ottimale di un iperparametro.\n",
        "\n",
        "**Procedura:**\n",
        "1. Testare diversi valori di `max_depth` per un Decision Tree.\n",
        "2. Per ogni valore, calcolare train e test score.\n",
        "3. Identificare il punto di overfitting.\n",
        "4. Scegliere il valore ottimale.\n",
        "\n",
        "**Cosa mostra il grafico:** le curve di train, test e cross-validation al variare di `max_depth`.\n",
        "\n",
        "**Come interpretarlo:**\n",
        "- Se il train continua a salire ma test e CV si fermano, siamo oltre il punto ottimale.\n",
        "- Il valore ottimale e vicino al massimo della curva di test o CV.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Perché lo facciamo:** osservare il punto in cui il train continua a salire ma il test si stabilizza o scende.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ESERCIZIO 12.3 — Trovare la Complessita Ottimale\n",
        "# ============================================================================\n",
        "\n",
        "# Importiamo le librerie necessarie\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# STEP 1: Creiamo il dataset\n",
        "print(\"STEP 1: Creazione Dataset\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=1500,\n",
        "    n_features=15,\n",
        "    n_informative=8,\n",
        "    n_redundant=4,\n",
        "    flip_y=0.08,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# STEP 2: Split train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Micro-checkpoint: controlliamo dimensioni\n",
        "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "\n",
        "# STEP 3: Testiamo diversi valori di max_depth\n",
        "max_depth_values = range(1, 21)\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "cv_means = []\n",
        "\n",
        "for depth in max_depth_values:\n",
        "    model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    train_scores.append(model.score(X_train, y_train))\n",
        "    test_scores.append(model.score(X_test, y_test))\n",
        "\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "    cv_means.append(cv_scores.mean())\n",
        "\n",
        "# STEP 4: Visualizziamo i risultati\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(max_depth_values, train_scores, label=\"Train score\")\n",
        "plt.plot(max_depth_values, test_scores, label=\"Test score\")\n",
        "plt.plot(max_depth_values, cv_means, label=\"CV mean\")\n",
        "plt.xlabel(\"max_depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Scelta della complessita ottimale\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEZIONE 6 — Glossario\n",
        "\n",
        "- **Generalizzazione**: capacita di un modello di funzionare bene su dati nuovi.\n",
        "- **Overfitting**: modello troppo complesso che memorizza il rumore.\n",
        "- **Underfitting**: modello troppo semplice che non cattura i pattern.\n",
        "- **Bias**: errore sistematico dovuto a un modello troppo semplice.\n",
        "- **Varianza**: sensibilita del modello a variazioni nei dati di training.\n",
        "- **Rumore irriducibile**: componente dell'errore non eliminabile.\n",
        "- **Learning curve**: grafico che mostra performance al crescere dei dati.\n",
        "- **Train score**: performance del modello sui dati di training.\n",
        "- **Validation score**: performance stimata su dati non visti (cross-validation).\n",
        "- **Gap train-test**: differenza tra performance su train e test.\n",
        "- **Iperparametro**: parametro scelto prima dell'addestramento (es. max_depth).\n",
        "- **Cross-validation**: valutazione ripetuta su piu split.\n",
        "- **Regolarizzazione**: tecniche che limitano la complessita del modello.\n",
        "- **Dataset sintetico**: dati generati artificialmente per esercizi.\n",
        "- **Decision tree**: modello a regole che cresce con la profondita.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEZIONE 7 — Errori Comuni e Debug Rapido\n",
        "\n",
        "| Sintomo | Causa probabile | Quick fix |\n",
        "|---|---|---|\n",
        "| Train molto alto, test molto basso | Modello troppo complesso | Riduci `max_depth`, aumenta dati, regolarizza |\n",
        "| Train e test entrambi bassi | Modello troppo semplice | Aumenta complessita o feature |\n",
        "| Learning curve piatta e bassa | Bias alto | Cambia modello o aggiungi feature |\n",
        "| Learning curve con grande gap | Varianza alta | Più dati o modello meno complesso |\n",
        "| Risultati non riproducibili | Seed mancante | Imposta `random_state` o `np.random.seed` |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEZIONE 8 — Conclusione Operativa\n",
        "\n",
        "## Concetti chiave\n",
        "\n",
        "- Bias e varianza spiegano il trade-off tra semplicita e complessita del modello.\n",
        "- Overfitting si riconosce da un grande gap tra train e validation/test.\n",
        "- Le learning curves aiutano a decidere se servono piu dati o un modello diverso.\n",
        "\n",
        "## Cosa applicare subito\n",
        "\n",
        "1. Misura sempre train e test score insieme.\n",
        "2. Usa la cross-validation per verificare stabilita.\n",
        "3. Se il gap e grande, riduci complessita o aumenta dati.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEZIONE 9 — End-of-lesson Checklist\n",
        "\n",
        "- So distinguere underfitting e overfitting con train/test score.\n",
        "- So leggere una learning curve e interpretarene il gap.\n",
        "- So collegare bias e varianza alla complessita del modello.\n",
        "- So proporre un rimedio pratico in base al problema.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEZIONE 10 — Didactic Changelog\n",
        "\n",
        "- Standardizzata la struttura con sezioni numerate e titolo coerente.\n",
        "- Aggiunti prerequisiti, indice e librerie utilizzate.\n",
        "- Inserita mappa decisionale per la diagnosi rapida.\n",
        "- Aggiunte motivazioni prima di ogni blocco di codice.\n",
        "- Inseriti micro-checkpoint con controlli su shape e classi.\n",
        "- Rafforzate le spiegazioni su bias, varianza e learning curve.\n",
        "- Aggiunta sezione Metodi Spiegati con input/output e errori tipici.\n",
        "- Aggiunti glossario e tabella errori comuni.\n",
        "- Inserita checklist finale operativa.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}