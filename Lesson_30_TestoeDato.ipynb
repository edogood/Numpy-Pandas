{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fcdb09f",
   "metadata": {},
   "source": [
    "# 1) Titolo e obiettivi\n",
    "\n",
    "Lezione 30: Testo e Dato - Trasformare testo in feature numeriche\n",
    "\n",
    "---\n",
    "\n",
    "## Mappa della lezione\n",
    "\n",
    "| Sezione | Contenuto | Tempo stimato |\n",
    "|---------|-----------|---------------|\n",
    "| 1 | Titolo, obiettivi, perché vettorizzare il testo | 5 min |\n",
    "| 2 | Teoria: preprocessing, BoW, n-grammi, sparsità | 15 min |\n",
    "| 3 | Schema mentale: workflow text → vector | 5 min |\n",
    "| 4 | Demo: preprocessing, CountVectorizer, parametri | 25 min |\n",
    "| 5 | Esercizi guidati + classificazione NB | 15 min |\n",
    "| 6 | Conclusione operativa | 10 min |\n",
    "| 7 | Checklist di fine lezione + glossario | 5 min |\n",
    "| 8 | Changelog didattico | 2 min |\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivi della lezione\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "| # | Obiettivo | Verifica |\n",
    "|---|-----------|----------|\n",
    "| 1 | **Pulire e tokenizzare** il testo | Sai rimuovere punteggiatura e lowercase? |\n",
    "| 2 | Costruire **Bag of Words** con CountVectorizer | Sai creare matrice sparsa da documenti? |\n",
    "| 3 | Usare **n-grammi** per catturare contesto | Sai settare ngram_range? |\n",
    "| 4 | Gestire **sparsità** e vocabolario | Sai usare max_features, min_df? |\n",
    "| 5 | **Trasformare nuovi testi** correttamente | Sai usare transform() senza refit? |\n",
    "\n",
    "---\n",
    "\n",
    "## L'idea centrale: dal testo ai numeri\n",
    "\n",
    "```\n",
    "TESTO GREZZO:                       DOPO PREPROCESSING:          BOW MATRIX:\n",
    "                                                                  the  cat  sat  dog  ran\n",
    "\"The cat sat.\"                      \"the cat sat\"                [  1    1    1    0    0  ]\n",
    "\"The dog ran!\"                      \"the dog ran\"                [  1    0    0    1    1  ]\n",
    "\n",
    "          │                               │                              │\n",
    "     Pulizia                        Tokenizzazione                 Vettorizzazione\n",
    "   (lowercase,                      (split su spazi)              (conteggio parole)\n",
    "    rimuovi !, .)\n",
    "```\n",
    "\n",
    "**Perché serve:** i modelli ML vogliono numeri, non stringhe!\n",
    "\n",
    "---\n",
    "\n",
    "## Il problema della sparsità\n",
    "\n",
    "```\n",
    "Vocabolario: 10.000 parole\n",
    "Documento medio: 50 parole\n",
    "\n",
    "Matrice BoW:\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│ 0  0  0  1  0  0  0  2  0  0  0  0  1  0  0  0  ... │  ← 99.5% zeri!\n",
    "│ 0  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0  ... │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "\n",
    "Sparsità = (n_zeri / n_totali) * 100\n",
    "```\n",
    "\n",
    "**Soluzione:** `max_features`, `min_df`, rappresentazioni sparse in scipy.\n",
    "\n",
    "---\n",
    "\n",
    "## N-grammi: catturare contesto\n",
    "\n",
    "| N-gram | Esempio | Pro | Contro |\n",
    "|--------|---------|-----|--------|\n",
    "| Unigram (1) | \"not\", \"good\" | Semplice | Perde \"not good\" |\n",
    "| Bigram (2) | \"not good\" | Cattura negazione | 2x vocabolario |\n",
    "| Trigram (3) | \"not very good\" | Più contesto | 3x vocabolario |\n",
    "\n",
    "```python\n",
    "# Solo unigrammi:\n",
    "CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# Uni + bigrammi:\n",
    "CountVectorizer(ngram_range=(1, 2))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow NLP base\n",
    "\n",
    "```\n",
    "┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐\n",
    "│  Testo   │ →  │  Pulizia │ →  │  Fit     │ →  │ Matrice  │\n",
    "│  grezzo  │    │ lowercase│    │ Vectorizer│   │  sparsa  │\n",
    "└──────────┘    │ punct    │    └──────────┘    └──────────┘\n",
    "                └──────────┘           │\n",
    "                                       ▼\n",
    "                              Nuovo testo? → transform() SOLO!\n",
    "```\n",
    "\n",
    "**Regola d'oro:** fit() su train, transform() su train e test.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisiti\n",
    "\n",
    "| Concetto | Dove lo trovi | Verifica |\n",
    "|----------|---------------|----------|\n",
    "| Stringhe Python | Base Python | Sai usare lower(), split()? |\n",
    "| Regex base | re module | Sai usare re.sub()? |\n",
    "| Matrici sparse | scipy | Sai che esistono formati CSR? |\n",
    "| Classificazione | Lezioni 5-6 | Sai cos'è MultinomialNB? |\n",
    "\n",
    "**Cosa useremo:** CountVectorizer, TfidfVectorizer (preview), MultinomialNB, regex per pulizia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c5de7",
   "metadata": {},
   "source": [
    "# 2) Teoria concettuale\n",
    "- Il testo va normalizzato (lowercase, rimozione punteggiatura/numeri) prima di essere vettorizzato.\n",
    "- Bag of Words: rappresentazione vettoriale basata su conteggi; input = lista di documenti, output = matrice sparsa (n_doc x vocab_size).\n",
    "- N-grammi: catturano contesto locale combinando parole consecutive (es. bigrammi). Pi? n-grammi aumentano dimensione e sparsimita'.\n",
    "- Sparsimita': la maggior parte delle celle e' zero; serve per capire memoria e performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e37e4c2",
   "metadata": {},
   "source": [
    "## CountVectorizer vs TF-IDF (preview)\n",
    "- CountVectorizer conta occorrenze; sensibile a parole frequenti ma non informative.\n",
    "- TF-IDF ripesa le parole in base alla rarita' nel corpus; utile per classificazione testo.\n",
    "- Entrambi richiedono vocabolario fisso: fit sul training, transform sul test/nuovi testi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7b1b5",
   "metadata": {},
   "source": [
    "# 3) Schema mentale / mappa decisionale\n",
    "1. Pulizia minima (lowercase, punteggiatura, numeri, spazi multipli).\n",
    "2. Tokenizzazione semplice o tramite CountVectorizer.\n",
    "3. Scelta n-grammi (1,1 per bag of words base; (1,2) per includere bigrammi).\n",
    "4. Valuta sparsimita' e dimensione vocabolario; opziona max_features/min_df per controllarla.\n",
    "5. Trasforma nuovi documenti solo con `transform`, non rifittare il vocabolario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f0d7c",
   "metadata": {},
   "source": [
    "# 4) Sezione dimostrativa\n",
    "- Demo 1: preprocessing e tokenizzazione manuale.\n",
    "- Demo 2: Bag of Words con CountVectorizer, sparsimita' e n-grammi.\n",
    "- Demo 3: parametri chiave (max_features, min_df) e trasformazione di nuovi testi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5ba65",
   "metadata": {},
   "source": [
    "## Demo 1 - Preprocessing e tokenizzazione\n",
    "Perche': capire cosa succede prima del vectorizer e controllare gli output attesi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384bc7d",
   "metadata": {},
   "source": [
    "## Demo 2 - Bag of Words e sparsimita'\n",
    "Perche': vedere vocabolario, matrice sparsa e impatto degli n-grammi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436c8e2",
   "metadata": {},
   "source": [
    "## Demo 3 - Parametri e nuovi documenti\n",
    "Perche': controllare la dimensione del vocabolario e applicare trasformazioni coerenti a nuovi testi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup NLP e corpus di esempio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Corpus di recensioni (ASCII)\n",
    "corpus = [\n",
    "    \"Ottimo prodotto, qualita eccellente!\",\n",
    "    \"Prodotto scadente, non lo consiglio\",\n",
    "    \"Qualita buona, prezzo giusto\",\n",
    "    \"Non funziona, prodotto difettoso\",\n",
    "    \"Consiglio questo prodotto, ottimo acquisto\",\n",
    "    \"Prezzo alto ma qualita ottima\",\n",
    "    \"Prodotto arrivato rotto, pessima esperienza\",\n",
    "    \"Acquisto consigliato, spedizione veloce\"\n",
    "]\n",
    "print(f\"Numero documenti: {len(corpus)}\")\n",
    "assert len(corpus) > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: preprocessing manuale\n",
    "# Scopo: mostrare lowercase + rimozione punteggiatura/numeri/spazi multipli.\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # punteggiatura -> spazio\n",
    "    text = re.sub(r'\\d+', ' ', text)       # numeri -> spazio\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "cleaned = [preprocess_text(doc) for doc in corpus]\n",
    "print(cleaned[:3])\n",
    "assert all(isinstance(t, str) for t in cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1 (continua): tokenizzazione semplice\n",
    "\n",
    "def tokenize(text: str) -> list:\n",
    "    return text.split()\n",
    "\n",
    "for i, doc in enumerate(cleaned[:3], 1):\n",
    "    tokens = tokenize(doc)\n",
    "    print(f\"Doc {i} tokens: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166bcb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Bag of Words con CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(corpus)\n",
    "print(f\"Shape BoW: {X_bow.shape}\")\n",
    "print(\"Vocabolario:\", list(vectorizer.get_feature_names_out()))\n",
    "assert X_bow.shape[0] == len(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63122db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2 (continua): analisi della sparsimita'\n",
    "n_total = X_bow.shape[0] * X_bow.shape[1]\n",
    "n_nonzero = X_bow.nnz\n",
    "sparsity = 1 - (n_nonzero / n_total)\n",
    "print(f\"Elementi totali: {n_total}, non-zero: {n_nonzero}, sparsity: {sparsity:.1%}\")\n",
    "assert 0 <= sparsity <= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cfe27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2 (continua): n-grammi (unigram + bigram)\n",
    "vec_bigram = CountVectorizer(ngram_range=(1,2))\n",
    "X_bigram = vec_bigram.fit_transform(corpus)\n",
    "vocab_uni = vectorizer.get_feature_names_out()\n",
    "vocab_bi = vec_bigram.get_feature_names_out()\n",
    "print(f\"Vocabolario unigram: {len(vocab_uni)} parole\")\n",
    "print(f\"Vocabolario uni+bigram: {len(vocab_bi)} features\")\n",
    "assert X_bigram.shape[0] == len(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ff0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3: parametri CountVectorizer\n",
    "print(\"Max features e min_df per controllare il vocabolario\")\n",
    "vec_limited = CountVectorizer(max_features=10)\n",
    "X_lim = vec_limited.fit_transform(corpus)\n",
    "print(f\"Vocabolario max_features=10: {list(vec_limited.get_feature_names_out())}\")\n",
    "\n",
    "vec_min_df = CountVectorizer(min_df=2)\n",
    "vec_min_df.fit(corpus)\n",
    "print(f\"Vocabolario min_df=2: {list(vec_min_df.get_feature_names_out())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3 (continua): trasformare nuovi documenti\n",
    "nuovi = [\"Prodotto eccellente e prezzo onesto\", \"Spedizione lenta e prodotto difettoso\"]\n",
    "X_new = vectorizer.transform(nuovi)\n",
    "print(f\"Shape nuovi documenti: {X_new.shape}\")\n",
    "print(\"Prima riga (densa)\", X_new[0].toarray())\n",
    "assert X_new.shape[1] == X_bow.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9014acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRASFORMAZIONE DI NUOVI DOCUMENTI\n",
    "# ============================================================\n",
    "# Importante: usare .transform() (non fit_transform) per nuovi dati\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRASFORMARE NUOVI DOCUMENTI\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Vectorizer già addestrato (su corpus)\n",
    "print(\"\\nVectorizer addestrato sul corpus originale\")\n",
    "print(f\"Vocabolario: {list(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# Nuovo documento da trasformare\n",
    "nuovo_doc = [\"Prodotto eccellente, prezzo ottimo\"]\n",
    "print(f\"\\nNuovo documento: \\\"{nuovo_doc[0]}\\\"\")\n",
    "\n",
    "# CORRETTO: usare transform (non fit_transform)\n",
    "X_nuovo = vectorizer.transform(nuovo_doc)\n",
    "\n",
    "print(f\"\\nVettore risultante:\")\n",
    "print(f\"  Shape: {X_nuovo.shape}\")\n",
    "print(f\"  Valori: {X_nuovo.toarray()[0]}\")\n",
    "\n",
    "# Quali parole sono state riconosciute?\n",
    "parole_riconosciute = []\n",
    "for i, count in enumerate(X_nuovo.toarray()[0]):\n",
    "    if count > 0:\n",
    "        parole_riconosciute.append(vectorizer.get_feature_names_out()[i])\n",
    "\n",
    "print(f\"\\nParole riconosciute dal vocabolario: {parole_riconosciute}\")\n",
    "\n",
    "# Parole nuove (out-of-vocabulary) vengono ignorate\n",
    "print(\"\\n⚠️  NOTA: 'eccellente' non era nel vocabolario → IGNORATA\")\n",
    "print(\"    Questo è un limite del BoW: parole nuove scompaiono\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da6256",
   "metadata": {},
   "source": [
    "# 5) Esercizi svolti (passo-passo)\n",
    "## Esercizio 30.1 - Pipeline di preprocessing\n",
    "Obiettivo: applicare pulizia, tokenizzazione e rimozione stopword italiane, restituendo liste di token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0537368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soluzione esercizio 30.1\n",
    "STOPWORDS_IT = {'il','lo','la','i','gli','le','un','una','di','a','da','in','con','su','per','tra','fra','e','o','ma','che','non','sono','ha','ho','questo','questa','questi','queste','come','quando','dove','perche','chi','cosa','piu','molto','anche','solo','ancora','se','ma'}\n",
    "\n",
    "def preprocess_and_tokenize(texts):\n",
    "    tokens_list = []\n",
    "    for t in texts:\n",
    "        clean = preprocess_text(t)\n",
    "        tokens = [tok for tok in tokenize(clean) if tok not in STOPWORDS_IT]\n",
    "        tokens_list.append(tokens)\n",
    "    return tokens_list\n",
    "\n",
    "processed = preprocess_and_tokenize(corpus)\n",
    "print(processed)\n",
    "assert len(processed) == len(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819070ea",
   "metadata": {},
   "source": [
    "## Esercizio 30.2 - BoW manuale\n",
    "Obiettivo: costruire manualmente matrice BoW e confrontarla con CountVectorizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a06e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soluzione esercizio 30.2\n",
    "\n",
    "def manual_bag_of_words(texts):\n",
    "    cleaned = [preprocess_text(t) for t in texts]\n",
    "    tokens = [tokenize(t) for t in cleaned]\n",
    "    vocab = sorted({tok for doc in tokens for tok in doc})\n",
    "    vocab_index = {w:i for i,w in enumerate(vocab)}\n",
    "    bow = np.zeros((len(tokens), len(vocab)), dtype=int)\n",
    "    for i, doc in enumerate(tokens):\n",
    "        for tok in doc:\n",
    "            bow[i, vocab_index[tok]] += 1\n",
    "    return bow, vocab\n",
    "\n",
    "bow_manual, vocab_manual = manual_bag_of_words(corpus)\n",
    "print(f\"Shape manuale: {bow_manual.shape}, vocab size: {len(vocab_manual)}\")\n",
    "assert bow_manual.shape[0] == len(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a602f",
   "metadata": {},
   "source": [
    "## Esercizio 30.3 - Sentiment con BoW\n",
    "Obiettivo: classificare recensioni positive/negative usando MultinomialNB su BoW.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soluzione esercizio 30.3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "recensioni = [\n",
    "    (\"Prodotto eccezionale, lo consiglio a tutti\", 1),\n",
    "    (\"Ottimo acquisto, qualita eccellente\", 1),\n",
    "    (\"Pessimo, arrivato rotto\", 0),\n",
    "    (\"Non funziona, soldi sprecati\", 0),\n",
    "    (\"Prezzo onesto e buona qualita\", 1),\n",
    "    (\"Esperienza terribile, non comprare\", 0),\n",
    "]\n",
    "texts, labels = zip(*recensioni)\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(texts)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42, stratify=labels)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "preds = nb.predict(X_test)\n",
    "acc = accuracy_score(y_test, preds)\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(classification_report(y_test, preds, digits=3))\n",
    "assert acc > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0be0a",
   "metadata": {},
   "source": [
    "# 6) Conclusione operativa\n",
    "\n",
    "## 5 take-home messages\n",
    "\n",
    "| # | Messaggio | Perché importante |\n",
    "|---|-----------|-------------------|\n",
    "| 1 | **Pulisci PRIMA di vettorizzare** | Lowercase + rimuovi punteggiatura |\n",
    "| 2 | **fit() su train, transform() su tutto** | Vocabolario consistente |\n",
    "| 3 | **Sparsità è normale** | 95%+ zeri è tipico, usa matrici sparse |\n",
    "| 4 | **N-grammi = più contesto, più dimensioni** | Trade-off da gestire |\n",
    "| 5 | **max_features/min_df per controllare** | Limita vocabolario troppo grande |\n",
    "\n",
    "---\n",
    "\n",
    "## Confronto sintetico: parametri CountVectorizer\n",
    "\n",
    "| Parametro | Default | Effetto | Quando cambiare |\n",
    "|-----------|---------|---------|-----------------|\n",
    "| `ngram_range` | (1,1) | Solo unigrammi | (1,2) per bigrammi |\n",
    "| `max_features` | None | Tutti i termini | 5000-10000 per limitare |\n",
    "| `min_df` | 1 | Anche termini rari | 2-5 per rimuovere rari |\n",
    "| `max_df` | 1.0 | Anche termini comuni | 0.9 per rimuovere troppo comuni |\n",
    "| `stop_words` | None | Nessuna rimozione | 'english' o lista custom |\n",
    "\n",
    "---\n",
    "\n",
    "## Perché questi concetti funzionano\n",
    "\n",
    "### 1) Bag of Words come conteggio\n",
    "\n",
    "```\n",
    "Documento: \"the cat sat on the mat\"\n",
    "\n",
    "Vocabolario: {cat: 0, mat: 1, on: 2, sat: 3, the: 4}\n",
    "\n",
    "Vettore BoW: [1, 1, 1, 1, 2]\n",
    "              ↑  ↑  ↑  ↑  ↑\n",
    "             cat mat on sat the (2 volte!)\n",
    "```\n",
    "\n",
    "### 2) Perché fit() e transform() separati\n",
    "\n",
    "```\n",
    "TRAINING:                          TEST:\n",
    "fit_transform(train_docs)          transform(test_docs)\n",
    "     ↓                                   ↓\n",
    "Impara vocabolario              Usa STESSO vocabolario\n",
    "+ trasforma                     Parole nuove → ignorate\n",
    "```\n",
    "\n",
    "Se rifitti su test, il vocabolario cambia → disastro!\n",
    "\n",
    "---\n",
    "\n",
    "## Reference card metodi\n",
    "\n",
    "| Metodo | Input | Output | Note |\n",
    "|--------|-------|--------|------|\n",
    "| `CountVectorizer()` | - | vectorizer object | Configurazione |\n",
    "| `.fit(docs)` | lista di stringhe | self | Impara vocabolario |\n",
    "| `.transform(docs)` | lista di stringhe | sparse matrix | Applica vocabolario |\n",
    "| `.fit_transform(docs)` | lista di stringhe | sparse matrix | fit + transform |\n",
    "| `.get_feature_names_out()` | - | array di termini | Vocabolario |\n",
    "| `.toarray()` | sparse matrix | dense array | Attenzione memoria! |\n",
    "\n",
    "---\n",
    "\n",
    "## Errori comuni e debug rapido\n",
    "\n",
    "| Errore | Perché sbagliato | Fix |\n",
    "|--------|-----------------|-----|\n",
    "| Refit su test | Vocabolario diverso | Solo transform() |\n",
    "| Dimenticare lowercase | \"The\" ≠ \"the\" | lowercase=True o preprocessing |\n",
    "| Vocabolario enorme | Lento, sparse | max_features, min_df |\n",
    "| Stopwords sbagliate | Lingua diversa | Specifica lingua o lista |\n",
    "| toarray() su grande | Memoria esplode | Tieni sparse |\n",
    "\n",
    "---\n",
    "\n",
    "## Template preprocessing + BoW\n",
    "\n",
    "```python\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1) Funzione pulizia\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # solo lettere e spazi\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 2) Applica pulizia\n",
    "docs_clean = [preprocess(doc) for doc in docs_raw]\n",
    "\n",
    "# 3) Vettorizza\n",
    "vec = CountVectorizer(ngram_range=(1, 2), max_features=5000, min_df=2)\n",
    "X_train = vec.fit_transform(train_docs_clean)\n",
    "X_test = vec.transform(test_docs_clean)\n",
    "\n",
    "# 4) Verifica\n",
    "print(f\"Vocabolario: {len(vec.get_feature_names_out())} termini\")\n",
    "print(f\"Sparsità: {100 * (1 - X_train.nnz / (X_train.shape[0] * X_train.shape[1])):.1f}%\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Prossimi passi\n",
    "\n",
    "| Lezione | Argomento | Collegamento |\n",
    "|---------|-----------|--------------|\n",
    "| 31 | TF-IDF | Pesatura per importanza |\n",
    "| 32 | Sentiment Analysis | Classificazione di polarità |\n",
    "| 33+ | NER, Document Intelligence | Entità e struttura |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e05363",
   "metadata": {},
   "source": [
    "# 7) Checklist di fine lezione\n",
    "- [ ] Ho pulito e tokenizzato il testo prima della vettorizzazione.\n",
    "- [ ] Ho controllato la dimensione del vocabolario e la sparsimita'.\n",
    "- [ ] Ho scelto n-grammi e parametri (max_features/min_df) adeguati.\n",
    "- [ ] Ho trasformato i nuovi testi con lo stesso vectorizer fit.\n",
    "- [ ] Ho testato una pipeline di classificazione (es. NB) con BoW.\n",
    "\n",
    "Glossario\n",
    "- Tokenizzazione: suddivisione del testo in parole.\n",
    "- Bag of Words: rappresentazione a conteggi di parole.\n",
    "- N-gramma: sequenza di n parole consecutive.\n",
    "- Sparsimita': percentuale di zeri in una matrice.\n",
    "- Vocabolario: insieme di termini mappati a colonne.\n",
    "- Stopword: parole molto frequenti e poco informative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Changelog didattico\n",
    "\n",
    "| Versione | Data | Modifiche |\n",
    "|----------|------|-----------|\n",
    "| 1.0 | 2024-02-01 | Creazione: preprocessing e BoW base |\n",
    "| 1.1 | 2024-02-08 | Aggiunto n-grammi e sparsità |\n",
    "| 2.0 | 2024-02-15 | Integrata classificazione NB |\n",
    "| 2.1 | 2024-02-20 | Refactor con checkpoint |\n",
    "| **2.3** | **2024-12-19** | **ESPANSIONE COMPLETA:** mappa lezione 8 sezioni, tabella obiettivi, ASCII text→vector workflow, sparsità diagram, n-grammi comparison, 5 take-home messages, tabella parametri CountVectorizer, fit/transform separation, template completo preprocessing + BoW, reference card metodi |\n",
    "\n",
    "---\n",
    "\n",
    "## Note per lo studente\n",
    "\n",
    "Questa lezione apre il **modulo NLP**:\n",
    "\n",
    "| Concetto | Lezione | Status |\n",
    "|----------|---------|--------|\n",
    "| Testo come dato | 30 (questa) | In corso |\n",
    "| TF-IDF | 31 | Prossima |\n",
    "| Sentiment Analysis | 32 | Dopo |\n",
    "| NER | 33 | Dopo |\n",
    "| Document Intelligence | 34 | Dopo |\n",
    "\n",
    "**Pipeline NLP base:**\n",
    "1. Pulizia testo (regex, lowercase)\n",
    "2. Tokenizzazione (split o vectorizer)\n",
    "3. Vettorizzazione (BoW, TF-IDF)\n",
    "4. Modello ML (NB, LogReg, ...)\n",
    "\n",
    "**Prossima tappa:** Lesson 31 - TF-IDF per pesare le parole importanti"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
