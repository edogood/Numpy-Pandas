{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Titolo e obiettivi\n",
    "Lezione 39: AI nel mondo reale - soglie, rischio e fallback umano.\n",
    "\n",
    "---\n",
    "\n",
    "## Mappa concettuale della lezione\n",
    "\n",
    "```\n",
    "AI IN PRODUZIONE - PIPELINE DECISIONALE\n",
    "=========================================\n",
    "\n",
    "    +------------------+\n",
    "    |  INPUT (caso)    |\n",
    "    |  ticket, request |\n",
    "    +--------+---------+\n",
    "             |\n",
    "             v\n",
    "    +------------------+\n",
    "    |  MODELLO ML      |\n",
    "    |  score/confidenza|\n",
    "    +--------+---------+\n",
    "             |\n",
    "             v\n",
    "    +------------------+\n",
    "    |  POLICY ENGINE   |\n",
    "    |  soglie, regole  |\n",
    "    +--------+---------+\n",
    "             |\n",
    "    +--------+---------+--------+\n",
    "    |                           |\n",
    "    v                           v\n",
    "+----------+            +------------+\n",
    "|   AUTO   |            |   HUMAN    |\n",
    "| decision |            |  fallback  |\n",
    "+----------+            +------------+\n",
    "\n",
    "\n",
    "SOGLIE E COSTI - TRADE-OFF\n",
    "===========================\n",
    "\n",
    "    SOGLIA BASSA                    SOGLIA ALTA\n",
    "    ============                    ============\n",
    "    Piu' automazione                Meno automazione\n",
    "    Piu' falsi positivi             Meno falsi positivi\n",
    "    Piu' rischio                    Piu' costo umano\n",
    "    Piu' copertura                  Meno copertura\n",
    "\n",
    "\n",
    "MONITORAGGIO CONTINUO\n",
    "=====================\n",
    "\n",
    "    Week 1    Week 2    Week 3    Week 4\n",
    "    ------    ------    ------    ------\n",
    "    [=====]   [=====]   [====]    [===]     <- Score medio\n",
    "    \n",
    "    DRIFT DETECTATO: score medio scende -> ALERTA!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivi didattici\n",
    "\n",
    "| # | Obiettivo | Livello |\n",
    "|---|-----------|---------|\n",
    "| 1 | Progettare policy di automazione basate su confidenza | Operativo |\n",
    "| 2 | Calcolare copertura e tasso di errore | Analitico |\n",
    "| 3 | Implementare fallback umano per casi incerti | Pratico |\n",
    "| 4 | Monitorare drift degli score nel tempo | Avanzato |\n",
    "| 5 | Mantenere audit log per compliance | Governance |\n",
    "| 6 | Bilanciare costo errore vs costo revisione umana | Strategico |\n",
    "\n",
    "---\n",
    "\n",
    "## Concetti chiave\n",
    "\n",
    "> **Copertura**: percentuale di casi gestiti automaticamente dal modello. Copertura alta = piu' efficienza ma piu' rischio.\n",
    "\n",
    "> **Fallback umano**: instradamento dei casi a bassa confidenza verso revisori umani per ridurre errori critici.\n",
    "\n",
    "> **Drift**: cambiamento nella distribuzione degli input o degli score nel tempo, segnale che il modello potrebbe degradare.\n",
    "\n",
    "---\n",
    "\n",
    "## Matrice costo-rischio\n",
    "\n",
    "```\n",
    "                    COSTO ERRORE BASSO    COSTO ERRORE ALTO\n",
    "                    ==================    =================\n",
    "CONFIDENZA ALTA     ✓ Automatizza         ✓ Automatizza\n",
    "                      (basso rischio)       (accettabile)\n",
    "\n",
    "CONFIDENZA BASSA    ✓ Automatizza         ✗ FALLBACK UMANO\n",
    "                      (errore costa poco)   (errore costa molto!)\n",
    "\n",
    "\n",
    "ESEMPIO PRATICO:\n",
    "----------------\n",
    "- Email spam: errore costa poco → soglia bassa OK\n",
    "- Frode bancaria: errore costa molto → soglia alta + review\n",
    "- Diagnosi medica: errore critico → sempre review umano\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cosa useremo\n",
    "- NumPy/Pandas per simulazioni\n",
    "- Funzioni di policy con soglie configurabili\n",
    "- Metriche di copertura e falsi positivi\n",
    "- Audit logging per tracciabilita'\n",
    "\n",
    "## Prerequisiti\n",
    "- Precision/Recall (Lezione 17)\n",
    "- Confusion matrix\n",
    "- Concetto di threshold optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Teoria concettuale\n",
    "- In produzione i modelli espongono score/valori di confidenza; serve scegliere soglie in base al costo degli errori.\n",
    "- Fallback umano: instradare i casi incerti a revisori per ridurre il rischio.\n",
    "- Monitoraggio: drift degli score e auditing delle decisioni.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Schema mentale / mappa decisionale\n",
    "1. Definisci classi a rischio e le soglie per automazione.\n",
    "2. Calcola copertura automatica (quanti casi gestiti dal modello).\n",
    "3. Monitora drift degli score; registra audit log.\n",
    "4. Aggiorna soglie/policy in base a costi ed evidenze.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sezione dimostrativa\n",
    "Demo: simulazione di score su ticket, policy soglia, copertura, drift e log di audit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup simulazione\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuliamo score di confidenza per 200 ticket con label vera (1=alto rischio)\n",
    "n = 200\n",
    "scores = np.clip(np.random.normal(0.6, 0.15, n), 0, 1)\n",
    "labels = (np.random.rand(n) > 0.7).astype(int)\n",
    "df = pd.DataFrame({'score': scores, 'label': labels})\n",
    "print(df.head())\n",
    "assert not df.isna().any().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy: se score >= soglia -> auto; altrimenti escalation umana\n",
    "threshold = 0.65\n",
    "\n",
    "def apply_policy(row):\n",
    "    decision = 'auto' if row['score'] >= threshold else 'human'\n",
    "    return decision\n",
    "\n",
    "df['decision'] = df.apply(apply_policy, axis=1)\n",
    "coverage = (df['decision'] == 'auto').mean()\n",
    "print(f\"Copertura automatica: {coverage:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stimiamo rischio: falsi positivi tra i casi auto (se label=0 ma auto)\n",
    "fps = ((df['decision']=='auto') & (df['label']==0)).mean()\n",
    "print(f\"Falsi positivi (sul totale casi): {fps:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Osservazioni\n",
    "- Aumentare la soglia riduce falsi positivi ma riduce copertura automatica.\n",
    "- Serve scegliere la soglia in base al costo errore vs costo umano.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Esercizi svolti (step-by-step)\n",
    "## Esercizio 39.1 - Drift semplice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 39.1: drift dello score medio\n",
    "score_new = np.clip(np.random.normal(0.45, 0.15, n), 0, 1)\n",
    "mean_old = df['score'].mean()\n",
    "mean_new = score_new.mean()\n",
    "print(f\"Mean score vecchio: {mean_old:.3f}, nuovo: {mean_new:.3f}\")\n",
    "if abs(mean_new - mean_old) > 0.05:\n",
    "    print(\"Possibile drift: rivalutare la soglia\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 39.2 - Audit log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 39.2: audit log\n",
    "from datetime import datetime\n",
    "\n",
    "df['audit'] = df.apply(lambda r: {\n",
    "    'ts': datetime.utcnow().isoformat(),\n",
    "    'score': r['score'],\n",
    "    'decision': r['decision']\n",
    "}, axis=1)\n",
    "print(df[['audit']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 39.3 - Policy class-specific\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 39.3: soglia diversa per classe ad alto costo\n",
    "costly_class_prob = np.random.rand(n)\n",
    "threshold_high_cost = 0.75\n",
    "\n",
    "decisions = []\n",
    "for s, cost_prob in zip(scores, costly_class_prob):\n",
    "    if cost_prob > 0.8 and s < threshold_high_cost:\n",
    "        decisions.append('human')\n",
    "    else:\n",
    "        decisions.append('auto' if s >= threshold else 'human')\n",
    "\n",
    "print(pd.Series(decisions).value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Conclusione operativa - Bignami AI in Produzione\n",
    "\n",
    "---\n",
    "\n",
    "## I 5 Take-Home Messages\n",
    "\n",
    "| # | Concetto | Perche' conta |\n",
    "|---|----------|---------------|\n",
    "| 1 | **Soglia = business decision, non tecnica** | Dipende da costo errore e costo review |\n",
    "| 2 | **Copertura e accuracy sono in trade-off** | Piu' automazione = piu' rischio errori |\n",
    "| 3 | **Classi diverse richiedono policy diverse** | Alto rischio → soglia piu' alta |\n",
    "| 4 | **Drift detection e' obbligatorio** | Modello degrada se dati cambiano |\n",
    "| 5 | **Audit log per compliance e debug** | Devi poter spiegare ogni decisione |\n",
    "\n",
    "---\n",
    "\n",
    "## Policy decision framework\n",
    "\n",
    "```\n",
    "STEP 1: CLASSIFICAZIONE RISCHIO\n",
    "================================\n",
    "\n",
    "    Input ──► Risk Class ──► Policy Selection\n",
    "    \n",
    "    - Risk LOW:   threshold = 0.5\n",
    "    - Risk MED:   threshold = 0.7\n",
    "    - Risk HIGH:  threshold = 0.9 + human review\n",
    "\n",
    "\n",
    "STEP 2: DECISION ENGINE\n",
    "=======================\n",
    "\n",
    "    IF score >= threshold:\n",
    "        decision = \"AUTO\"\n",
    "        action = model_prediction\n",
    "    ELSE:\n",
    "        decision = \"HUMAN\"\n",
    "        action = route_to_queue\n",
    "\n",
    "\n",
    "STEP 3: MONITORING\n",
    "==================\n",
    "\n",
    "    log = {\n",
    "        \"timestamp\": now(),\n",
    "        \"input_id\": case_id,\n",
    "        \"score\": model_score,\n",
    "        \"threshold\": used_threshold,\n",
    "        \"decision\": \"AUTO\" | \"HUMAN\",\n",
    "        \"outcome\": actual_result  # post-hoc\n",
    "    }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Metriche operative\n",
    "\n",
    "| Metrica | Formula | Interpretazione |\n",
    "|---------|---------|-----------------|\n",
    "| Copertura | auto / total | % casi automatizzati |\n",
    "| FP rate (auto) | FP_auto / auto | Errori su casi automatizzati |\n",
    "| Costo totale | FP×costo_errore + human×costo_review | Obiettivo: minimizzare |\n",
    "| Drift score | |mean(t) - mean(t-1)| / std | Alerta se > threshold |\n",
    "\n",
    "---\n",
    "\n",
    "## Template policy engine\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class PolicyEngine:\n",
    "    def __init__(self, default_threshold=0.7, high_risk_threshold=0.9):\n",
    "        self.default_threshold = default_threshold\n",
    "        self.high_risk_threshold = high_risk_threshold\n",
    "        self.audit_log = []\n",
    "    \n",
    "    def get_threshold(self, risk_class):\n",
    "        \"\"\"Threshold dinamico per classe di rischio\"\"\"\n",
    "        thresholds = {\n",
    "            'low': 0.5,\n",
    "            'medium': self.default_threshold,\n",
    "            'high': self.high_risk_threshold\n",
    "        }\n",
    "        return thresholds.get(risk_class, self.default_threshold)\n",
    "    \n",
    "    def decide(self, case_id, score, risk_class='medium'):\n",
    "        \"\"\"Decision engine con logging\"\"\"\n",
    "        threshold = self.get_threshold(risk_class)\n",
    "        decision = 'AUTO' if score >= threshold else 'HUMAN'\n",
    "        \n",
    "        # Audit logging\n",
    "        self.audit_log.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'case_id': case_id,\n",
    "            'score': score,\n",
    "            'risk_class': risk_class,\n",
    "            'threshold': threshold,\n",
    "            'decision': decision\n",
    "        })\n",
    "        return decision\n",
    "    \n",
    "    def get_coverage(self):\n",
    "        \"\"\"Calcola copertura automatica\"\"\"\n",
    "        if not self.audit_log:\n",
    "            return 0.0\n",
    "        df = pd.DataFrame(self.audit_log)\n",
    "        return (df['decision'] == 'AUTO').mean()\n",
    "    \n",
    "    def detect_drift(self, window=7, threshold=0.1):\n",
    "        \"\"\"Rileva drift degli score\"\"\"\n",
    "        if len(self.audit_log) < 2 * window:\n",
    "            return False, 0.0\n",
    "        df = pd.DataFrame(self.audit_log)\n",
    "        recent = df['score'].tail(window).mean()\n",
    "        previous = df['score'].iloc[-2*window:-window].mean()\n",
    "        drift = abs(recent - previous)\n",
    "        return drift > threshold, drift\n",
    "    \n",
    "    def export_audit(self):\n",
    "        \"\"\"Esporta log per compliance\"\"\"\n",
    "        return pd.DataFrame(self.audit_log)\n",
    "\n",
    "# Uso\n",
    "engine = PolicyEngine(default_threshold=0.7)\n",
    "for i, (score, risk) in enumerate(zip(scores, risk_classes)):\n",
    "    decision = engine.decide(f\"case_{i}\", score, risk)\n",
    "print(f\"Copertura: {engine.get_coverage():.1%}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Errori comuni e soluzioni\n",
    "\n",
    "| Errore | Conseguenza | Soluzione |\n",
    "|--------|-------------|-----------|\n",
    "| Soglia fissa per tutti i casi | Troppi errori su casi ad alto rischio | Policy class-based |\n",
    "| Nessun monitoraggio drift | Modello degrada silenziosamente | Alert su score distribution |\n",
    "| Mancanza di audit log | Non-compliance, no debugging | Log ogni decisione |\n",
    "| Ignorare costo review | Sottostima costo totale | Include human cost in optimization |\n",
    "\n",
    "---\n",
    "\n",
    "## Checklist deployment AI\n",
    "\n",
    "```\n",
    "PRE-DEPLOYMENT\n",
    "☐ Definite classi di rischio\n",
    "☐ Soglie calibrate su costi\n",
    "☐ Fallback umano configurato\n",
    "☐ Audit logging implementato\n",
    "\n",
    "POST-DEPLOYMENT\n",
    "☐ Monitoraggio drift attivo\n",
    "☐ Alert configurati\n",
    "☐ Review periodica decisioni\n",
    "☐ Feedback loop per retraining\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Checklist di fine lezione\n",
    "- [ ] Ho definito soglia(e) basate su costi.\n",
    "- [ ] Ho misurato copertura automatica e falsi positivi.\n",
    "- [ ] Ho considerato classi ad alto costo con policy dedicate.\n",
    "- [ ] Ho previsto monitoraggio drift degli score.\n",
    "- [ ] Ho registrato decisioni in un audit log.\n",
    "\n",
    "Glossario\n",
    "- Copertura: quota di casi gestiti automaticamente.\n",
    "- Falso positivo: caso auto ma label negativa.\n",
    "- Drift: cambiamento statistico degli score/dati nel tempo.\n",
    "- Audit log: registro delle decisioni per controllo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Changelog didattico\n",
    "\n",
    "| Versione | Data | Modifiche |\n",
    "|----------|------|-----------|\n",
    "| 1.0 | 2024-01-XX | Struttura iniziale 8 sezioni |\n",
    "| 2.0 | 2024-12-XX | Espansione completa AI in Produzione |\n",
    "| 2.1 | - | Pipeline decisionale ASCII completa |\n",
    "| 2.2 | - | Trade-off soglie visualizzato |\n",
    "| 2.3 | - | Matrice costo-rischio per decisioni |\n",
    "| 2.4 | - | Classe PolicyEngine completa |\n",
    "| 2.5 | - | Drift detection implementata |\n",
    "| 2.6 | - | Checklist pre/post deployment |\n",
    "\n",
    "---\n",
    "\n",
    "## Note di versione\n",
    "\n",
    "**v2.0 - Espansione didattica completa**\n",
    "- Focus su aspetti operativi del deployment AI\n",
    "- Policy engine come pattern riutilizzabile\n",
    "- Emphasis su audit e compliance\n",
    "- Drift detection come best practice\n",
    "- Preparazione per MLOps e monitoring avanzato\n",
    "\n",
    "**Dipendenze didattiche**\n",
    "- Richiede: Lezione 17 (metriche), threshold optimization\n",
    "- Prepara: Concetti MLOps, CI/CD per modelli, A/B testing\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
