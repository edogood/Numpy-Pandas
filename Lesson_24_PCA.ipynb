{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7b49ed",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Lezione 24 â€” PCA: Principal Component Analysis\n",
    "\n",
    "## Obiettivi di Apprendimento\n",
    "\n",
    "| # | Obiettivo | Livello |\n",
    "|---|-----------|---------|\n",
    "| 1 | Capire il concetto di riduzione dimensionalitÃ  | ðŸŸ¢ Base |\n",
    "| 2 | Comprendere varianza spiegata e componenti principali | ðŸŸ¢ Base |\n",
    "| 3 | Usare PCA per visualizzazione dati ad alta dimensionalitÃ  | ðŸŸ¡ Intermedio |\n",
    "| 4 | Scegliere il numero ottimale di componenti | ðŸŸ¡ Intermedio |\n",
    "| 5 | Interpretare i loadings delle componenti | ðŸ”´ Avanzato |\n",
    "| 6 | Applicare PCA come preprocessing per ML | ðŸ”´ Avanzato |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Indice\n",
    "\n",
    "1. **Teoria** â€” DimensionalitÃ , varianza, autovettori\n",
    "2. **Schema Mentale** â€” Workflow PCA\n",
    "3. **Demo Pratiche** â€” 5 demo progressive\n",
    "4. **Esercizi** â€” 3 esercizi con soluzioni\n",
    "5. **Conclusione** â€” Cosa portarsi a casa\n",
    "6. **Bignami** â€” Reference card\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Setup\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_digits, make_blobs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061eae91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“– 1. Teoria\n",
    "\n",
    "### 1.1 Il Problema della DimensionalitÃ \n",
    "\n",
    "Immagina di avere un dataset con **100 feature**. Problemi:\n",
    "\n",
    "| Problema | Conseguenza |\n",
    "|----------|-------------|\n",
    "| **Curse of Dimensionality** | I punti diventano \"equidistanti\" |\n",
    "| **Overfitting** | Troppi parametri, pochi campioni |\n",
    "| **Visualizzazione impossibile** | Non puoi plottare 100 dimensioni |\n",
    "| **Computazione lenta** | PiÃ¹ feature = piÃ¹ calcoli |\n",
    "\n",
    "**PCA risolve tutto questo** riducendo le dimensioni mantenendo l'informazione importante!\n",
    "\n",
    "```\n",
    "100 feature â”€â”€â”€â”€[PCA]â”€â”€â”€â”€> 2-10 componenti\n",
    "                              â†“\n",
    "                        Stessa informazione\n",
    "                        (o quasi!)\n",
    "```\n",
    "\n",
    "### ðŸ“Š Intuizione Geometrica\n",
    "\n",
    "```\n",
    "PRIMA (2D originale):         DOPO (1D con PCA):\n",
    "         â—                         \n",
    "        â—â—                    PC1\n",
    "       â—â—â—    â† dati         â”€â”€â—â—â—â—â—â—â—â—â—â—â—â”€â”€\n",
    "      â—â—â—â—      correlati         â†‘\n",
    "     â—â—â—â—â—                    proiezione sulla\n",
    "                              direzione di max varianza\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be81c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.2 Cosa Fa PCA: I 4 Passi\n",
    "\n",
    "PCA trova le **direzioni di massima varianza** nei dati:\n",
    "\n",
    "```\n",
    "ALGORITMO PCA:\n",
    "\n",
    "1. CENTRA i dati (sottrai la media)\n",
    "   X_centered = X - mean(X)\n",
    "\n",
    "2. CALCOLA la matrice di covarianza\n",
    "   Cov = X_centered.T @ X_centered / (n-1)\n",
    "\n",
    "3. TROVA autovalori e autovettori\n",
    "   eigenvalues, eigenvectors = eig(Cov)\n",
    "   \n",
    "4. PROIETTA i dati sugli autovettori principali\n",
    "   X_pca = X_centered @ eigenvectors[:, :k]\n",
    "```\n",
    "\n",
    "### ðŸ”‘ Concetti Chiave\n",
    "\n",
    "| Termine | Significato |\n",
    "|---------|-------------|\n",
    "| **Componente Principale (PC)** | Direzione di massima varianza |\n",
    "| **Autovettore** | La direzione della PC |\n",
    "| **Autovalore** | Quanta varianza spiega quella PC |\n",
    "| **Varianza Spiegata** | % di informazione catturata |\n",
    "| **Loadings** | Peso di ogni feature originale nella PC |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13de423",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.3 Varianza Spiegata\n",
    "\n",
    "La domanda chiave: **quante componenti servono?**\n",
    "\n",
    "```\n",
    "Varianza spiegata cumulativa:\n",
    "\n",
    "PC1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  60%\n",
    "PC2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  25%  â†’ Cum: 85%\n",
    "PC3: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  10%  â†’ Cum: 95%\n",
    "PC4: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   5%  â†’ Cum: 100%\n",
    "\n",
    "Con solo PC1 + PC2 (2 componenti) mantieni l'85% dell'informazione!\n",
    "```\n",
    "\n",
    "### ðŸ“ Regole Pratiche per Scegliere K\n",
    "\n",
    "| Metodo | Regola |\n",
    "|--------|--------|\n",
    "| **Soglia varianza** | Mantieni componenti fino a 90-95% varianza cumulativa |\n",
    "| **Elbow method** | Cerca il \"gomito\" nel plot degli autovalori |\n",
    "| **Kaiser criterion** | Mantieni solo PC con autovalore > 1 (su dati standardizzati) |\n",
    "| **Scree plot** | Dove la curva diventa \"piatta\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9c8ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.4 Interpretare i Loadings\n",
    "\n",
    "I **loadings** ti dicono come ogni feature originale contribuisce alla PC:\n",
    "\n",
    "```\n",
    "Esempio: PC1 per dati di clienti\n",
    "\n",
    "Feature          Loading\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "etÃ               +0.45    â† contribuisce molto (positivo)\n",
    "reddito          +0.52    â† contribuisce molto (positivo)\n",
    "spesa_mensile    +0.48    â† contribuisce molto (positivo)\n",
    "n_figli          -0.10    â† contribuisce poco\n",
    "\n",
    "Interpretazione: PC1 cattura il \"potere d'acquisto\"\n",
    "(etÃ  + reddito + spesa vanno insieme)\n",
    "```\n",
    "\n",
    "### âš ï¸ Importante: Scaling!\n",
    "\n",
    "**PCA Ã¨ sensibile alla scala!** Feature con valori grandi dominano.\n",
    "\n",
    "```\n",
    "SBAGLIATO:                    CORRETTO:\n",
    "etÃ : 20-80                    StandardScaler prima di PCA!\n",
    "reddito: 20000-200000  â† domina tutto\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5afed5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.5 Quando Usare PCA\n",
    "\n",
    "| âœ… Usa PCA quando... | âŒ Evita PCA quando... |\n",
    "|----------------------|------------------------|\n",
    "| Hai troppe feature (>20) | Le feature sono giÃ  poche |\n",
    "| Vuoi visualizzare dati HD | Ogni feature Ã¨ importante singolarmente |\n",
    "| C'Ã¨ multicollinearitÃ  | I dati non sono correlati |\n",
    "| Vuoi ridurre overfitting | Hai bisogno di interpretabilitÃ  diretta |\n",
    "| Come preprocessing per ML | I dati sono categorici |\n",
    "\n",
    "### ðŸ”„ PCA nel Pipeline ML\n",
    "\n",
    "```\n",
    "Dati â”€â†’ StandardScaler â”€â†’ PCA â”€â†’ Modello ML\n",
    "         (necessario!)    â†“\n",
    "                      n_components=0.95\n",
    "                      (mantieni 95% varianza)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371870c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§  2. Schema Mentale\n",
    "\n",
    "### Workflow PCA\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                          PCA WORKFLOW                               â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  1. PREPARAZIONE                                                    â”‚\n",
    "â”‚     â””â”€â”€ StandardScaler() â€” OBBLIGATORIO per PCA!                    â”‚\n",
    "â”‚     â””â”€â”€ Verifica: dati numerici, no NaN                             â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  2. PCA ESPLORATIVO (tutte le componenti)                           â”‚\n",
    "â”‚     â””â”€â”€ pca = PCA()  # senza n_components                           â”‚\n",
    "â”‚     â””â”€â”€ pca.fit(X_scaled)                                           â”‚\n",
    "â”‚     â””â”€â”€ Analizza: explained_variance_ratio_                         â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  3. SCEGLI n_components                                             â”‚\n",
    "â”‚     â””â”€â”€ Plot varianza cumulativa                                    â”‚\n",
    "â”‚     â””â”€â”€ Soglia: 90-95% varianza                                     â”‚\n",
    "â”‚     â””â”€â”€ Oppure: PCA(n_components=0.95) automatico                   â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  4. FIT FINALE                                                      â”‚\n",
    "â”‚     â””â”€â”€ pca = PCA(n_components=k)                                   â”‚\n",
    "â”‚     â””â”€â”€ X_pca = pca.fit_transform(X_scaled)                         â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  5. INTERPRETAZIONE                                                 â”‚\n",
    "â”‚     â””â”€â”€ pca.components_ â†’ loadings                                  â”‚\n",
    "â”‚     â””â”€â”€ Quali feature contribuiscono a ogni PC?                     â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  6. USO                                                             â”‚\n",
    "â”‚     â””â”€â”€ Visualizzazione (2D/3D)                                     â”‚\n",
    "â”‚     â””â”€â”€ Input per clustering o classificazione                      â”‚\n",
    "â”‚     â””â”€â”€ Riduzione rumore                                            â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### âœ… Checklist Pre-PCA\n",
    "\n",
    "```\n",
    "â–¡ Dati scalati con StandardScaler?\n",
    "â–¡ Nessun valore mancante?\n",
    "â–¡ Solo feature numeriche?\n",
    "â–¡ Hai un obiettivo chiaro? (visualizzazione, preprocessing, noise reduction)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6f519",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”¬ 3. Demo Pratiche\n",
    "\n",
    "### Demo 1 â€” Primo PCA: Dataset Iris\n",
    "\n",
    "Visualizziamo il classico dataset Iris (4 feature) in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb402ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 1 â€” Primo PCA: Dataset Iris\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEMO 1 â€” PCA sul Dataset Iris\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Carica Iris\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "print(f\"ðŸ“Š Dataset Iris:\")\n",
    "print(f\"   - Campioni: {X_iris.shape[0]}\")\n",
    "print(f\"   - Feature: {X_iris.shape[1]}\")\n",
    "print(f\"   - Feature names: {feature_names}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 1: Scaling (OBBLIGATORIO!)\n",
    "# ============================================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 2: PCA esplorativo\n",
    "# ============================================\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Varianza spiegata per componente:\")\n",
    "for i, var in enumerate(pca_full.explained_variance_ratio_):\n",
    "    cum_var = pca_full.explained_variance_ratio_[:i+1].sum()\n",
    "    bar = \"â–ˆ\" * int(var * 50)\n",
    "    print(f\"   PC{i+1}: {var*100:5.1f}% {bar} (cum: {cum_var*100:.1f}%)\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 3: PCA con 2 componenti per visualizzazione\n",
    "# ============================================\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Con 2 componenti manteniamo: {pca.explained_variance_ratio_.sum()*100:.1f}% della varianza\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 4: Visualizzazione\n",
    "# ============================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot PCA\n",
    "colors = ['red', 'green', 'blue']\n",
    "target_names = iris.target_names\n",
    "\n",
    "for i, (color, name) in enumerate(zip(colors, target_names)):\n",
    "    mask = y_iris == i\n",
    "    axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], c=color, label=name, \n",
    "                    s=50, alpha=0.7, edgecolors='black')\n",
    "\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=11)\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=11)\n",
    "axes[0].set_title('Iris Dataset in 2D (PCA)', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scree plot\n",
    "axes[1].bar(range(1, 5), pca_full.explained_variance_ratio_, alpha=0.7, \n",
    "            label='Varianza individuale', color='steelblue')\n",
    "axes[1].plot(range(1, 5), np.cumsum(pca_full.explained_variance_ratio_), \n",
    "             'ro-', label='Varianza cumulativa')\n",
    "axes[1].axhline(y=0.95, color='red', linestyle='--', alpha=0.5, label='Soglia 95%')\n",
    "axes[1].set_xlabel('Componente Principale', fontsize=11)\n",
    "axes[1].set_ylabel('Varianza Spiegata', fontsize=11)\n",
    "axes[1].set_title('Scree Plot', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].set_xticks(range(1, 5))\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸŽ¯ OSSERVAZIONI:\n",
    "   1. Con solo 2 PC manteniamo il 95%+ della varianza\n",
    "   2. Le 3 specie sono chiaramente separabili in 2D\n",
    "   3. PC1 cattura la maggior parte della varianza (~73%)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8456e27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Demo 2 â€” Interpretare i Loadings\n",
    "\n",
    "Vediamo cosa significano le componenti principali in termini di feature originali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 2 â€” Interpretare i Loadings\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEMO 2 â€” Interpretare i Loadings\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# I loadings sono in pca.components_\n",
    "# Ogni riga Ã¨ una PC, ogni colonna Ã¨ una feature originale\n",
    "\n",
    "loadings = pca.components_\n",
    "df_loadings = pd.DataFrame(\n",
    "    loadings,\n",
    "    columns=feature_names,\n",
    "    index=['PC1', 'PC2']\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Loadings (peso di ogni feature nelle PC):\")\n",
    "print(df_loadings.round(3))\n",
    "\n",
    "# Visualizzazione\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap dei loadings\n",
    "im = axes[0].imshow(loadings, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[0].set_xticks(range(len(feature_names)))\n",
    "axes[0].set_xticklabels([f.replace(' (cm)', '') for f in feature_names], rotation=45, ha='right')\n",
    "axes[0].set_yticks(range(2))\n",
    "axes[0].set_yticklabels(['PC1', 'PC2'])\n",
    "axes[0].set_title('Heatmap dei Loadings', fontsize=12)\n",
    "plt.colorbar(im, ax=axes[0], label='Loading')\n",
    "\n",
    "# Aggiungi valori\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        axes[0].text(j, i, f'{loadings[i, j]:.2f}', ha='center', va='center', \n",
    "                     color='white' if abs(loadings[i, j]) > 0.5 else 'black')\n",
    "\n",
    "# Biplot\n",
    "ax = axes[1]\n",
    "# Scatter dei punti\n",
    "for i, (color, name) in enumerate(zip(colors, target_names)):\n",
    "    mask = y_iris == i\n",
    "    ax.scatter(X_pca[mask, 0], X_pca[mask, 1], c=color, label=name, \n",
    "               s=30, alpha=0.5)\n",
    "\n",
    "# Aggiungi le frecce per i loadings\n",
    "scale = 3  # Scala per visibilitÃ \n",
    "for i, feature in enumerate(feature_names):\n",
    "    ax.arrow(0, 0, loadings[0, i]*scale, loadings[1, i]*scale,\n",
    "             head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "    ax.text(loadings[0, i]*scale*1.15, loadings[1, i]*scale*1.15, \n",
    "            feature.replace(' (cm)', ''), fontsize=9, ha='center')\n",
    "\n",
    "ax.set_xlabel('PC1', fontsize=11)\n",
    "ax.set_ylabel('PC2', fontsize=11)\n",
    "ax.set_title('Biplot: Dati + Loadings', fontsize=12)\n",
    "ax.legend(loc='lower left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(x=0, color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ“Œ INTERPRETAZIONE:\n",
    "\n",
    "PC1 (73% varianza):\n",
    "   - Petal length (+0.58) e petal width (+0.57) dominano\n",
    "   - Separa fiori con petali grandi da petali piccoli\n",
    "   â†’ \"Dimensione del petalo\"\n",
    "\n",
    "PC2 (23% varianza):\n",
    "   - Sepal width (+0.60) domina\n",
    "   - Distingue fiori con sepali larghi vs stretti\n",
    "   â†’ \"Larghezza del sepalo\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29acb2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Demo 3 â€” Scegliere n_components Automaticamente\n",
    "\n",
    "sklearn permette di specificare la varianza target invece del numero di componenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d629b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 3 â€” Scegliere n_components Automaticamente\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEMO 3 â€” Selezione Automatica di n_components\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dataset piÃ¹ grande: digits (64 feature = 8x8 pixel)\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(f\"ðŸ“Š Dataset Digits:\")\n",
    "print(f\"   - Campioni: {X_digits.shape[0]}\")\n",
    "print(f\"   - Feature: {X_digits.shape[1]} (immagini 8x8)\")\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_digits_scaled = scaler.fit_transform(X_digits)\n",
    "\n",
    "# ============================================\n",
    "# Metodo 1: Specificare la varianza target\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METODO 1: Varianza Target\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "soglie = [0.80, 0.90, 0.95, 0.99]\n",
    "\n",
    "for soglia in soglie:\n",
    "    pca = PCA(n_components=soglia)\n",
    "    pca.fit(X_digits_scaled)\n",
    "    print(f\"   Varianza {soglia*100:.0f}% â†’ {pca.n_components_} componenti\")\n",
    "\n",
    "# ============================================\n",
    "# Metodo 2: Scree Plot + Gomito\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METODO 2: Scree Plot\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_digits_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].plot(range(1, 65), pca_full.explained_variance_ratio_, 'b-o', markersize=3)\n",
    "axes[0].set_xlabel('Componente', fontsize=11)\n",
    "axes[0].set_ylabel('Varianza Spiegata', fontsize=11)\n",
    "axes[0].set_title('Scree Plot - Varianza per Componente', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim(0, 65)\n",
    "\n",
    "# Varianza cumulativa\n",
    "cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "axes[1].plot(range(1, 65), cumsum, 'g-o', markersize=3)\n",
    "axes[1].axhline(y=0.90, color='orange', linestyle='--', label='90%')\n",
    "axes[1].axhline(y=0.95, color='red', linestyle='--', label='95%')\n",
    "axes[1].set_xlabel('Componente', fontsize=11)\n",
    "axes[1].set_ylabel('Varianza Cumulativa', fontsize=11)\n",
    "axes[1].set_title('Varianza Cumulativa', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(0, 65)\n",
    "\n",
    "# Trova dove raggiungiamo 90% e 95%\n",
    "n_90 = np.argmax(cumsum >= 0.90) + 1\n",
    "n_95 = np.argmax(cumsum >= 0.95) + 1\n",
    "axes[1].axvline(x=n_90, color='orange', linestyle=':', alpha=0.7)\n",
    "axes[1].axvline(x=n_95, color='red', linestyle=':', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\"\"\n",
    "ðŸ“Š RISULTATI:\n",
    "   - Per 90% varianza: {n_90} componenti (da 64!)\n",
    "   - Per 95% varianza: {n_95} componenti\n",
    "   \n",
    "   Riduzione: 64 â†’ {n_90} = {100*(1-n_90/64):.0f}% meno dimensioni!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703fc78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Demo 4 â€” Visualizzazione di Dati ad Alta DimensionalitÃ \n",
    "\n",
    "Visualizziamo le cifre scritte a mano (64D) in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b33230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 4 â€” Visualizzazione Dati Alta DimensionalitÃ \n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEMO 4 â€” Visualizzazione Digits in 2D e 3D\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# PCA a 2 componenti\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_digits_2d = pca_2d.fit_transform(X_digits_scaled)\n",
    "\n",
    "# PCA a 3 componenti\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_digits_3d = pca_3d.fit_transform(X_digits_scaled)\n",
    "\n",
    "print(f\"ðŸ“Š Varianza spiegata:\")\n",
    "print(f\"   2D: {pca_2d.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "print(f\"   3D: {pca_3d.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Alcune immagini originali\n",
    "ax1 = fig.add_subplot(131)\n",
    "for i in range(10):\n",
    "    ax1.imshow(digits.images[i], cmap='gray', extent=[i*9, i*9+8, 0, 8])\n",
    "ax1.set_xlim(0, 90)\n",
    "ax1.set_ylim(-1, 9)\n",
    "ax1.set_title('Esempi di Cifre (8x8 pixel)', fontsize=12)\n",
    "ax1.axis('off')\n",
    "\n",
    "# 2D scatter\n",
    "ax2 = fig.add_subplot(132)\n",
    "scatter = ax2.scatter(X_digits_2d[:, 0], X_digits_2d[:, 1], \n",
    "                       c=y_digits, cmap='tab10', s=10, alpha=0.6)\n",
    "ax2.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax2.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax2.set_title('Digits in 2D (PCA)', fontsize=12)\n",
    "plt.colorbar(scatter, ax=ax2, label='Cifra')\n",
    "\n",
    "# 3D scatter\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "scatter3d = ax3.scatter(X_digits_3d[:, 0], X_digits_3d[:, 1], X_digits_3d[:, 2],\n",
    "                         c=y_digits, cmap='tab10', s=10, alpha=0.6)\n",
    "ax3.set_xlabel('PC1')\n",
    "ax3.set_ylabel('PC2')\n",
    "ax3.set_zlabel('PC3')\n",
    "ax3.set_title('Digits in 3D (PCA)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Zoom su alcune cifre\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for digit in range(10):\n",
    "    ax = axes[digit // 5, digit % 5]\n",
    "    mask = y_digits == digit\n",
    "    ax.scatter(X_digits_2d[~mask, 0], X_digits_2d[~mask, 1], c='lightgray', s=5, alpha=0.3)\n",
    "    ax.scatter(X_digits_2d[mask, 0], X_digits_2d[mask, 1], c='red', s=10, alpha=0.7)\n",
    "    ax.set_title(f'Cifra {digit}', fontsize=11)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle('Distribuzione di ogni cifra nello spazio PCA 2D', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸŽ¯ OSSERVAZIONI:\n",
    "   1. Cifre simili (es. 3,5,8) sono vicine nello spazio PCA\n",
    "   2. Cifre diverse (es. 0,1) sono ben separate\n",
    "   3. Con solo 2 PC (~30% varianza) vediamo giÃ  la struttura!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188906e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Demo 5 â€” PCA per Riduzione Rumore\n",
    "\n",
    "PCA puÃ² essere usato per rimuovere il rumore ricostruendo i dati con meno componenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 5 â€” PCA per Riduzione Rumore\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEMO 5 â€” PCA per Riduzione Rumore\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prendi alcune cifre e aggiungi rumore\n",
    "np.random.seed(42)\n",
    "indices = [0, 100, 200, 300, 400]  # 5 cifre diverse\n",
    "X_sample = digits.images[indices].copy()\n",
    "\n",
    "# Aggiungi rumore gaussiano\n",
    "noise_level = 5\n",
    "X_noisy = X_sample + np.random.normal(0, noise_level, X_sample.shape)\n",
    "X_noisy = np.clip(X_noisy, 0, 16)  # Clip ai valori validi\n",
    "\n",
    "# Flatten per PCA\n",
    "X_noisy_flat = X_noisy.reshape(5, 64)\n",
    "\n",
    "# Ricostruzione con diversi numeri di componenti\n",
    "n_components_list = [5, 10, 20, 40]\n",
    "\n",
    "fig, axes = plt.subplots(len(indices), len(n_components_list) + 2, figsize=(16, 10))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    # Originale\n",
    "    axes[i, 0].imshow(X_sample[i], cmap='gray')\n",
    "    if i == 0:\n",
    "        axes[i, 0].set_title('Originale', fontsize=10)\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Con rumore\n",
    "    axes[i, 1].imshow(X_noisy[i], cmap='gray')\n",
    "    if i == 0:\n",
    "        axes[i, 1].set_title(f'+ Rumore\\n(Ïƒ={noise_level})', fontsize=10)\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Ricostruzioni\n",
    "    for j, n_comp in enumerate(n_components_list):\n",
    "        # Fit PCA su tutto il dataset digits per avere una buona base\n",
    "        pca_denoise = PCA(n_components=n_comp)\n",
    "        pca_denoise.fit(X_digits_scaled)\n",
    "        \n",
    "        # Trasforma e ricostruisci la singola immagine rumorosa\n",
    "        x_noisy_scaled = (X_noisy_flat[i] - scaler.mean_) / scaler.scale_\n",
    "        x_pca = pca_denoise.transform(x_noisy_scaled.reshape(1, -1))\n",
    "        x_reconstructed = pca_denoise.inverse_transform(x_pca)\n",
    "        x_reconstructed = x_reconstructed * scaler.scale_ + scaler.mean_\n",
    "        \n",
    "        axes[i, j+2].imshow(x_reconstructed.reshape(8, 8), cmap='gray')\n",
    "        if i == 0:\n",
    "            var_explained = pca_denoise.explained_variance_ratio_.sum() * 100\n",
    "            axes[i, j+2].set_title(f'n={n_comp}\\n({var_explained:.0f}%)', fontsize=10)\n",
    "        axes[i, j+2].axis('off')\n",
    "\n",
    "# Etichette righe\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[i, 0].set_ylabel(f'Cifra {y_digits[idx]}', fontsize=10)\n",
    "\n",
    "plt.suptitle('PCA per Riduzione Rumore: Originale â†’ Rumoroso â†’ Ricostruzione', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ“Š OSSERVAZIONI:\n",
    "\n",
    "1. Con poche componenti (5-10): \n",
    "   - Rimuove molto rumore\n",
    "   - Ma perde anche dettagli\n",
    "\n",
    "2. Con piÃ¹ componenti (20-40):\n",
    "   - Mantiene piÃ¹ dettagli\n",
    "   - Ma rimuove meno rumore\n",
    "\n",
    "3. Trade-off:\n",
    "   - Poche PC â†’ piÃ¹ smoothing, meno dettaglio\n",
    "   - Molte PC â†’ piÃ¹ dettaglio, meno denoising\n",
    "\n",
    "ðŸŽ¯ PCA filtra il rumore perchÃ© il rumore Ã¨ \"random\" \n",
    "   e non viene catturato dalle componenti principali!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44147d87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ 4. Esercizi\n",
    "\n",
    "### ðŸ“ Esercizio 24.1 â€” PCA su Dati di Vini\n",
    "\n",
    "**Consegna:** Usa PCA per visualizzare e analizzare il dataset Wine.\n",
    "\n",
    "**Richieste:**\n",
    "1. Carica il dataset Wine (sklearn.datasets.load_wine)\n",
    "2. Scala i dati e applica PCA\n",
    "3. Determina quante componenti servono per l'80% e 95% della varianza\n",
    "4. Visualizza i dati in 2D colorati per tipo di vino\n",
    "5. Interpreta le prime 2 PC analizzando i loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633263de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 24.1 â€” SOLUZIONE\n",
    "# ============================================\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ESERCIZIO 24.1 â€” PCA su Dataset Wine\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 1: Carica e prepara i dati\n",
    "# ============================================\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "feature_names_wine = wine.feature_names\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Wine:\")\n",
    "print(f\"   - Campioni: {X_wine.shape[0]}\")\n",
    "print(f\"   - Feature: {X_wine.shape[1]}\")\n",
    "print(f\"   - Classi: {wine.target_names}\")\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_wine_scaled = scaler.fit_transform(X_wine)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 2: PCA esplorativo\n",
    "# ============================================\n",
    "pca_wine = PCA()\n",
    "pca_wine.fit(X_wine_scaled)\n",
    "\n",
    "cumsum_var = np.cumsum(pca_wine.explained_variance_ratio_)\n",
    "n_80 = np.argmax(cumsum_var >= 0.80) + 1\n",
    "n_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Componenti necessarie:\")\n",
    "print(f\"   - Per 80% varianza: {n_80} componenti\")\n",
    "print(f\"   - Per 95% varianza: {n_95} componenti\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 3: Visualizzazione\n",
    "# ============================================\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_wine_2d = pca_2d.fit_transform(X_wine_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1, 14), pca_wine.explained_variance_ratio_, alpha=0.7, color='steelblue')\n",
    "axes[0].plot(range(1, 14), cumsum_var, 'ro-', label='Cumulativa')\n",
    "axes[0].axhline(y=0.80, color='orange', linestyle='--', alpha=0.7, label='80%')\n",
    "axes[0].axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95%')\n",
    "axes[0].set_xlabel('Componente', fontsize=11)\n",
    "axes[0].set_ylabel('Varianza Spiegata', fontsize=11)\n",
    "axes[0].set_title('Scree Plot', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2D scatter\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, name in enumerate(wine.target_names):\n",
    "    mask = y_wine == i\n",
    "    axes[1].scatter(X_wine_2d[mask, 0], X_wine_2d[mask, 1], \n",
    "                    c=colors[i], label=name, s=50, alpha=0.7, edgecolors='black')\n",
    "axes[1].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "axes[1].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "axes[1].set_title('Wine Dataset in 2D', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Loadings heatmap\n",
    "loadings = pca_2d.components_\n",
    "im = axes[2].barh(range(len(feature_names_wine)), loadings[0], alpha=0.7, label='PC1')\n",
    "axes[2].barh(range(len(feature_names_wine)), loadings[1], alpha=0.7, label='PC2', left=loadings[0])\n",
    "axes[2].set_yticks(range(len(feature_names_wine)))\n",
    "axes[2].set_yticklabels(feature_names_wine, fontsize=8)\n",
    "axes[2].set_xlabel('Loading', fontsize=11)\n",
    "axes[2].set_title('Loadings PC1 e PC2', fontsize=12)\n",
    "axes[2].legend()\n",
    "axes[2].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# PASSO 4: Interpretazione\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PASSO 4: Interpretazione Loadings\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Top 3 feature per PC1 e PC2\n",
    "df_loadings = pd.DataFrame({\n",
    "    'Feature': feature_names_wine,\n",
    "    'PC1': loadings[0],\n",
    "    'PC2': loadings[1]\n",
    "})\n",
    "\n",
    "print(\"\\nðŸ“Š Top 3 feature per PC1 (ordine assoluto):\")\n",
    "top_pc1 = df_loadings.reindex(df_loadings['PC1'].abs().sort_values(ascending=False).index)\n",
    "print(top_pc1.head(3).to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ“Š Top 3 feature per PC2 (ordine assoluto):\")\n",
    "top_pc2 = df_loadings.reindex(df_loadings['PC2'].abs().sort_values(ascending=False).index)\n",
    "print(top_pc2.head(3).to_string(index=False))\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ“Œ INTERPRETAZIONE:\n",
    "   - PC1: Cattura principalmente la \"intensitÃ \" del vino \n",
    "     (flavanoids, proline, color_intensity)\n",
    "   - PC2: Distingue per contenuto \"minerale/acido\"\n",
    "     (alcalinity_of_ash, nonflavanoid_phenols)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb30d6e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ‹ï¸ Esercizio 24.2 â€” PCA come Preprocessing per Classificazione\n",
    "\n",
    "**Obiettivo:** Usare PCA per ridurre le dimensioni prima di un classificatore.\n",
    "\n",
    "**Consegna:**\n",
    "1. Usa il dataset **Digits** (load_digits)\n",
    "2. Applica PCA conservando il **90%** della varianza\n",
    "3. Addestra un **LogisticRegression** sui dati originali e sui dati ridotti\n",
    "4. Confronta:\n",
    "   - Accuracy su test set\n",
    "   - Tempo di training\n",
    "   - Numero di feature usate\n",
    "\n",
    "**Domanda:** La riduzione di dimensionalitÃ  migliora o peggiora le prestazioni?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfbe818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 24.2 â€” SOLUZIONE\n",
    "# ============================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ESERCIZIO 24.2 â€” PCA come Preprocessing per Classificazione\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 1: Prepara i dati\n",
    "# ============================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset:\")\n",
    "print(f\"   - Training: {X_train_scaled.shape}\")\n",
    "print(f\"   - Test: {X_test_scaled.shape}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 2: Training SENZA PCA\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SCENARIO 1: Senza PCA (64 feature)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start = time.time()\n",
    "lr_no_pca = LogisticRegression(max_iter=5000, random_state=42)\n",
    "lr_no_pca.fit(X_train_scaled, y_train)\n",
    "time_no_pca = time.time() - start\n",
    "\n",
    "acc_no_pca = lr_no_pca.score(X_test_scaled, y_test)\n",
    "print(f\"   â±ï¸  Tempo training: {time_no_pca:.4f}s\")\n",
    "print(f\"   ðŸ“Š Accuracy: {acc_no_pca*100:.2f}%\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 3: Training CON PCA (90% varianza)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SCENARIO 2: Con PCA (90% varianza)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "pca_90 = PCA(n_components=0.90)\n",
    "X_train_pca = pca_90.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca_90.transform(X_test_scaled)\n",
    "\n",
    "n_components = pca_90.n_components_\n",
    "print(f\"   ðŸ“ Componenti selezionate: {n_components}\")\n",
    "\n",
    "start = time.time()\n",
    "lr_pca = LogisticRegression(max_iter=5000, random_state=42)\n",
    "lr_pca.fit(X_train_pca, y_train)\n",
    "time_pca = time.time() - start\n",
    "\n",
    "acc_pca = lr_pca.score(X_test_pca, y_test)\n",
    "print(f\"   â±ï¸  Tempo training: {time_pca:.4f}s\")\n",
    "print(f\"   ðŸ“Š Accuracy: {acc_pca*100:.2f}%\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 4: Confronto\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFRONTO FINALE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Metrica': ['Feature', 'Tempo (s)', 'Accuracy (%)'],\n",
    "    'Senza PCA': [64, f\"{time_no_pca:.4f}\", f\"{acc_no_pca*100:.2f}\"],\n",
    "    'Con PCA': [n_components, f\"{time_pca:.4f}\", f\"{acc_pca*100:.2f}\"]\n",
    "})\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "speedup = time_no_pca / time_pca if time_pca > 0 else 1\n",
    "feature_reduction = (1 - n_components/64) * 100\n",
    "acc_diff = (acc_pca - acc_no_pca) * 100\n",
    "\n",
    "print(f\"\"\"\n",
    "ðŸ“Œ CONCLUSIONI:\n",
    "   - Riduzione feature: {feature_reduction:.1f}% (da 64 a {n_components})\n",
    "   - Speedup training: {speedup:.2f}x\n",
    "   - Differenza accuracy: {acc_diff:+.2f}%\n",
    "   \n",
    "ðŸ’¡ PCA riduce drasticamente le dimensioni mantenendo \n",
    "   prestazioni quasi identiche â†’ ottimo per dataset grandi!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc54f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ‹ï¸ Esercizio 24.3 â€” Confronto Spazio Originale vs PCA\n",
    "\n",
    "**Obiettivo:** Visualizzare l'effetto della riduzione dimensionale sulla separabilitÃ  delle classi.\n",
    "\n",
    "**Consegna:**\n",
    "1. Usa il dataset **Iris**\n",
    "2. Crea una visualizzazione con:\n",
    "   - **Pannello 1**: Scatter plot di 2 feature ORIGINALI (sepal length vs sepal width)\n",
    "   - **Pannello 2**: Scatter plot PC1 vs PC2\n",
    "3. Calcola e confronta la **distanza media tra i centroidi** delle classi nei due spazi\n",
    "4. Quale rappresentazione separa meglio le classi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3063998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 24.3 â€” SOLUZIONE\n",
    "# ============================================\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ESERCIZIO 24.3 â€” Confronto Spazio Originale vs PCA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 1: Prepara i dati\n",
    "# ============================================\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Spazio originale: solo 2 feature (sepal)\n",
    "X_orig = X_iris[:, :2]  # sepal length, sepal width\n",
    "\n",
    "# Spazio PCA\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_iris_scaled)\n",
    "\n",
    "print(f\"ðŸ“Š Varianza spiegata da PC1+PC2: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 2: Visualizzazione\n",
    "# ============================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "markers = ['o', 's', '^']\n",
    "\n",
    "# Pannello 1: Spazio originale\n",
    "for i, name in enumerate(target_names):\n",
    "    mask = y_iris == i\n",
    "    axes[0].scatter(X_orig[mask, 0], X_orig[mask, 1], \n",
    "                    c=colors[i], marker=markers[i], s=80,\n",
    "                    label=name, alpha=0.7, edgecolors='black')\n",
    "axes[0].set_xlabel('Sepal Length', fontsize=12)\n",
    "axes[0].set_ylabel('Sepal Width', fontsize=12)\n",
    "axes[0].set_title('Spazio Originale (2 feature)', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Pannello 2: Spazio PCA\n",
    "for i, name in enumerate(target_names):\n",
    "    mask = y_iris == i\n",
    "    axes[1].scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                    c=colors[i], marker=markers[i], s=80,\n",
    "                    label=name, alpha=0.7, edgecolors='black')\n",
    "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12)\n",
    "axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12)\n",
    "axes[1].set_title('Spazio PCA (2 componenti)', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# PASSO 3: Calcolo distanze tra centroidi\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALISI SEPARABILITÃ€ â€” Distanza tra Centroidi\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def calcola_distanza_media_centroidi(X, y):\n",
    "    \"\"\"Calcola la distanza media tra tutti i centroidi delle classi.\"\"\"\n",
    "    centroids = []\n",
    "    for c in np.unique(y):\n",
    "        centroids.append(X[y == c].mean(axis=0))\n",
    "    centroids = np.array(centroids)\n",
    "    \n",
    "    # Matrice delle distanze tra centroidi\n",
    "    dist_matrix = cdist(centroids, centroids)\n",
    "    \n",
    "    # Media delle distanze (esclusa diagonale)\n",
    "    n = len(centroids)\n",
    "    total_dist = 0\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            total_dist += dist_matrix[i, j]\n",
    "            count += 1\n",
    "    return total_dist / count, centroids\n",
    "\n",
    "dist_orig, centroids_orig = calcola_distanza_media_centroidi(X_orig, y_iris)\n",
    "dist_pca, centroids_pca = calcola_distanza_media_centroidi(X_pca, y_iris)\n",
    "\n",
    "print(f\"\\nðŸ“ Distanza media tra centroidi:\")\n",
    "print(f\"   - Spazio originale: {dist_orig:.3f}\")\n",
    "print(f\"   - Spazio PCA: {dist_pca:.3f}\")\n",
    "\n",
    "improvement = (dist_pca - dist_orig) / dist_orig * 100\n",
    "\n",
    "print(f\"\"\"\n",
    "ðŸ“Œ CONCLUSIONI:\n",
    "   - PCA aumenta la distanza tra centroidi del {improvement:.1f}%\n",
    "   - Questo significa che le classi sono PIÃ™ SEPARATE in PCA\n",
    "   - PCA massimizza la varianza â†’ massimizza la separazione!\n",
    "   \n",
    "ðŸ’¡ Anche se PCA Ã¨ unsupervised (non usa le label), \n",
    "   spesso migliora la separabilitÃ  perchÃ© le classi \n",
    "   tendono a differire lungo le direzioni di massima varianza.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a7946",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸŽ¯ 5. Conclusione â€” Cosa Portarsi a Casa\n",
    "\n",
    "## âœ… Concetti Chiave Appresi\n",
    "\n",
    "| Concetto | Descrizione |\n",
    "|----------|-------------|\n",
    "| **PCA** | Trova le direzioni di massima varianza nei dati |\n",
    "| **Componenti Principali** | Nuove feature ordinate per importanza |\n",
    "| **Varianza Spiegata** | Quanto \"informazione\" cattura ogni PC |\n",
    "| **Loadings** | Pesi delle feature originali in ogni PC |\n",
    "\n",
    "## ðŸš¨ Errori Comuni da Evitare\n",
    "\n",
    "1. **Non scalare i dati** â†’ Feature con range diversi dominano PCA\n",
    "2. **Tenere troppe/poche componenti** â†’ Usa scree plot + soglia 80-95%\n",
    "3. **Ignorare i loadings** â†’ Perdi interpretabilitÃ  del modello\n",
    "4. **Applicare PCA a dati categorici** â†’ PCA Ã¨ per dati numerici continui\n",
    "\n",
    "## ðŸ’¡ Quando Usare PCA\n",
    "\n",
    "```\n",
    "âœ… USA PCA:\n",
    "   - Molte feature correlate\n",
    "   - Visualizzazione di dati high-dimensional\n",
    "   - Preprocessing per velocizzare training\n",
    "   - Rimuovere rumore\n",
    "\n",
    "âŒ NON USARE PCA:\n",
    "   - Poche feature indipendenti\n",
    "   - Serve interpretabilitÃ  diretta\n",
    "   - Dati categorici o sparsi\n",
    "```\n",
    "\n",
    "## ðŸ”— Ponte verso la Prossima Lezione\n",
    "\n",
    "Nella **Lezione 25** combineremo **PCA + Clustering**:\n",
    "- PCA per ridurre le dimensioni\n",
    "- K-Means o DBSCAN sui dati ridotti\n",
    "- Visualizzazione dei cluster in 2D\n",
    "\n",
    "Questo workflow Ã¨ molto comune nel mondo reale!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891fa629",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“š 6. Bignami â€” Scheda di Riferimento Rapido\n",
    "\n",
    "## ðŸ“– Definizioni Essenziali\n",
    "\n",
    "| Termine | Definizione |\n",
    "|---------|-------------|\n",
    "| **Componente Principale (PC)** | Direzione di massima varianza, combinazione lineare delle feature originali |\n",
    "| **Varianza Spiegata** | Frazione della varianza totale catturata da una PC (valori 0-1) |\n",
    "| **Loadings** | Coefficienti che indicano il contributo di ogni feature a una PC |\n",
    "| **Scree Plot** | Grafico per visualizzare la varianza spiegata per componente |\n",
    "| **Curse of Dimensionality** | Problemi che emergono con molte feature: sparsitÃ , overfitting, calcolo |\n",
    "\n",
    "## ðŸ“ Regole per Scegliere n_components\n",
    "\n",
    "| Metodo | Regola |\n",
    "|--------|--------|\n",
    "| **Soglia Varianza** | Mantieni 80-95% della varianza cumulativa |\n",
    "| **Kaiser** | Mantieni PC con eigenvalue > 1 (dopo StandardScaler) |\n",
    "| **Gomito** | Trova il \"gomito\" nello scree plot |\n",
    "\n",
    "## ðŸ”§ Template di Codice\n",
    "\n",
    "### PCA Base\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Scala (OBBLIGATORIO!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. PCA con n fisso\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 3. PCA con varianza target\n",
    "pca = PCA(n_components=0.95)  # 95% varianza\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "```\n",
    "\n",
    "### Scree Plot\n",
    "```python\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "plt.bar(range(1, len(cumsum)+1), pca_full.explained_variance_ratio_)\n",
    "plt.plot(range(1, len(cumsum)+1), cumsum, 'ro-')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--')\n",
    "```\n",
    "\n",
    "### Loadings Heatmap\n",
    "```python\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "    index=feature_names\n",
    ")\n",
    "sns.heatmap(loadings, annot=True, cmap='RdBu_r', center=0)\n",
    "```\n",
    "\n",
    "## âœ… Checklist Pre-PCA\n",
    "\n",
    "- [ ] Dati numerici continui?\n",
    "- [ ] Feature scalate con StandardScaler?\n",
    "- [ ] Feature correlate (senso applicare PCA)?\n",
    "- [ ] Definita soglia varianza target?\n",
    "- [ ] Piano per interpretare i loadings?\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Fine Lezione 24 â€” PCA Masterclass!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
