{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002ae5a4",
   "metadata": {},
   "source": [
    "#  Sezione 1 – Titolo e Obiettivi della Lezione\n",
    "\n",
    "## Lezione 5 — Introduzione a Scikit-Learn: Tassonomia dei Modelli e Logistic Regression\n",
    "\n",
    "###  Obiettivi di apprendimento\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "1. **Comprendere le quattro famiglie di modelli** in Scikit-Learn (lineari, tree, distanza, bayesiani)\n",
    "2. **Capire la matematica dei modelli lineari** (combinazione lineare, sigmoide, logit)\n",
    "3. **Applicare la pipeline standard ML**: split → scaling → fit → predict → score\n",
    "4. **Implementare una Logistic Regression** completa per classificazione binaria\n",
    "5. **Interpretare i coefficienti** del modello (segno, magnitudo, importanza)\n",
    "6. **Evitare il data leakage** applicando le operazioni nell'ordine corretto\n",
    "\n",
    "---\n",
    "\n",
    "###  Prerequisiti\n",
    "\n",
    "| Prerequisito | Lezione | Concetti da padroneggiare |\n",
    "|--------------|---------|---------------------------|\n",
    "| Pandas avanzato | Lesson 04 | GroupBy, Transform, Feature Engineering |\n",
    "| NumPy base | Lesson 01 | Array, operazioni vettoriali |\n",
    "| Matematica base | — | Esponenziale, logaritmo, funzioni |\n",
    "\n",
    "---\n",
    "\n",
    "###  Perché questa lezione è il punto di svolta?\n",
    "\n",
    "Questa lezione segna il passaggio da **analisi dati** a **Machine Learning**:\n",
    "\n",
    "| Prima (Data Analysis) | Dopo (Machine Learning) |\n",
    "|----------------------|-------------------------|\n",
    "| Descrivere i dati | **Predire** nuovi dati |\n",
    "| Statistiche storiche | **Modelli** che generalizzano |\n",
    "| Interpretazione manuale | **Algoritmi** che apprendono |\n",
    "\n",
    "La **Logistic Regression** è il modello perfetto per iniziare:\n",
    "- È semplice da capire matematicamente\n",
    "- I coefficienti sono interpretabili\n",
    "- È la baseline per modelli più complessi\n",
    "- Usata in produzione in molte aziende\n",
    "\n",
    "---\n",
    "\n",
    "###  Struttura della lezione (8 sezioni)\n",
    "\n",
    "1. **Titolo e Obiettivi** ← Sei qui\n",
    "2. **Teoria concettuale profonda** (famiglie modelli, matematica)\n",
    "3. **Schema mentale / Mappa decisionale**\n",
    "4. **Sezione dimostrativa** con micro-checkpoint\n",
    "5. **Reference e Esercizi** + Errori comuni / Debug\n",
    "6. **Conclusione operativa**\n",
    "7. **Checklist di fine lezione**\n",
    "8. **Changelog didattico**\n",
    "\n",
    "---\n",
    "\n",
    "###  Librerie utilizzate\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "```\n",
    "\n",
    "**Nota**: questa è la prima lezione che usa `sklearn` (Scikit-Learn)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f78dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2) Teoria concettuale profonda\n",
    "\n",
    "## 1.1 Glossario Preliminare\n",
    "\n",
    "Prima di leggere qualsiasi formula, fissiamo un glossario chiaro:\n",
    "\n",
    "| Termine | Definizione |\n",
    "|---------|-------------|\n",
    "| **Feature** | Variabile di input che descrive un'osservazione (es. eta, reddito) |\n",
    "| **Target** | Variabile da predire (output del modello) |\n",
    "| **Modello lineare** | Combina le feature con pesi e bias per produrre un output |\n",
    "| **Logit** | Trasformazione $\\log \\frac{p}{1-p}$ che converte probabilita in quantita illimitata |\n",
    "| **Sigmoid** | Funzione $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ che schiaccia valori tra 0 e 1 |\n",
    "| **Odds** | Rapporto tra probabilita di successo e insuccesso: $\\frac{p}{1-p}$ |\n",
    "| **Regularizzazione** | Penalita sui pesi per ridurre overfitting |\n",
    "\n",
    "## 1.2 Le Quattro Famiglie di Modelli\n",
    "\n",
    "Scikit-Learn offre molti modelli, ma pensarli per **famiglie concettuali** aiuta a scegliere:\n",
    "\n",
    "### Famiglia 1: Modelli Lineari\n",
    "- **Esempi**: `LinearRegression`, `LogisticRegression`, `Ridge`, `Lasso`\n",
    "- **Idea**: stimano una combinazione lineare delle feature\n",
    "- **Formula**: $\\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b$\n",
    "- **Pro**: interpretabili, veloci, ottime baseline\n",
    "- **Contro**: assumono relazioni lineari\n",
    "\n",
    "### Famiglia 2: Tree-Based (Alberi)\n",
    "- **Esempi**: `DecisionTreeClassifier`, `RandomForestClassifier`, `GradientBoostingClassifier`\n",
    "- **Idea**: suddivisione dello spazio tramite soglie\n",
    "- **Pro**: catturano non-linearita, robusti\n",
    "- **Contro**: rischio overfitting, meno interpretabili\n",
    "\n",
    "### Famiglia 3: Distanza / Vicinanza (KNN)\n",
    "- **Esempi**: `KNeighborsClassifier`, `KNeighborsRegressor`\n",
    "- **Idea**: classificano in base ai vicini piu simili\n",
    "- **Pro**: nessuna assunzione forte, intuitivi\n",
    "- **Contro**: lenti in predizione, sensibili allo scaling\n",
    "\n",
    "### Famiglia 4: Probabilistici / Bayesiani\n",
    "- **Esempi**: `GaussianNB`, `MultinomialNB`\n",
    "- **Idea**: applicano il teorema di Bayes\n",
    "- **Pro**: velocissimi, ottimi su dati testuali\n",
    "- **Contro**: assumono indipendenza tra feature\n",
    "\n",
    "## 1.3 Matematica della Logistic Regression\n",
    "\n",
    "### Step 1: Combinazione Lineare (Logit)\n",
    "$$z = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "### Step 2: Funzione Sigmoide\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "La sigmoide trasforma z (che va da $-\\infty$ a $+\\infty$) in una probabilita (tra 0 e 1).\n",
    "\n",
    "### Probabilita Stimata\n",
    "$$\\hat{p} = P(y=1 \\mid x) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)$$\n",
    "\n",
    "### Interpretazione dei Coefficienti\n",
    "- $w_j > 0$: aumentando la feature j, aumenta la probabilita della classe 1\n",
    "- $w_j < 0$: aumentando la feature j, diminuisce la probabilita\n",
    "- $|w_j|$ grande: la feature ha forte impatto\n",
    "\n",
    "## 1.4 Perche Standardizzare?\n",
    "\n",
    "**StandardScaler** applica:\n",
    "$$x' = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "**Benefici:**\n",
    "- Stabilita numerica\n",
    "- Convergenza piu rapida\n",
    "- Coefficienti comparabili tra feature\n",
    "- **Necessario** per modelli lineari, KNN, SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a6352",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3) Schema mentale / mappa decisionale\n",
    "\n",
    "## Pipeline Standard ML\n",
    "\n",
    "```\n",
    "[Dati Grezzi]\n",
    "      |\n",
    "      v\n",
    "[Feature Engineering]  ← Crea/trasforma variabili\n",
    "      |\n",
    "      v\n",
    "[Train/Test Split]     ← Dividi per validazione\n",
    "      |\n",
    "      v\n",
    "[Preprocessing]        ← Scaling, encoding\n",
    "      |\n",
    "      v\n",
    "[Fit del Modello]      ← Apprendimento\n",
    "      |\n",
    "      v\n",
    "[Predict]              ← Previsioni su test\n",
    "      |\n",
    "      v\n",
    "[Score/Valutazione]    ← Metriche\n",
    "      |\n",
    "      v\n",
    "[Iterazione]           ← Migliora se necessario\n",
    "```\n",
    "\n",
    "## Come Scegliere un Modello?\n",
    "\n",
    "```\n",
    "TIPO DI PROBLEMA?\n",
    "       |\n",
    "   ┌───┴───┐\n",
    "   v       v\n",
    "CLASSIF. REGRESS.\n",
    "   |       |\n",
    "   v       v\n",
    "Quante classi?  Target continuo?\n",
    "   |              |\n",
    "   v              v\n",
    "BINARIA → LogisticRegression, RandomForest\n",
    "MULTI   → Stessi + SoftMax\n",
    "CONTINUO → LinearRegression, RandomForest\n",
    "```\n",
    "\n",
    "## Euristiche di Scelta Rapida\n",
    "\n",
    "| Situazione | Modello Consigliato |\n",
    "|------------|---------------------|\n",
    "| Dati quasi lineari | Logistic/Linear Regression |\n",
    "| Non-linearita sospette | RandomForest, GradientBoosting |\n",
    "| Dataset enorme | GradientBoosting, SGDClassifier |\n",
    "| Testo (NLP classico) | Naive Bayes + TF-IDF |\n",
    "| Dataset piccolo | KNN |\n",
    "| Serve interpretabilita | Logistic Regression, Decision Tree |\n",
    "\n",
    "## Checklist Pre-Modellazione\n",
    "\n",
    "1. Ho definito chiaramente il target?\n",
    "2. Ho separato train e test PRIMA di qualsiasi preprocessing?\n",
    "3. Ho standardizzato le feature (per modelli lineari)?\n",
    "4. Ho verificato il bilanciamento delle classi?\n",
    "5. Ho scelto le metriche appropriate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040ed34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4) Sezione dimostrativa\n",
    "\n",
    "## 3.1 Setup e Creazione Dataset Sintetico\n",
    "\n",
    "**Perche questo passaggio:** Usiamo `make_classification` per creare un dataset controllato. Questo ci permette di capire il flusso senza preoccuparci della qualita dei dati reali.\n",
    "\n",
    "**Parametri chiave:**\n",
    "- `n_samples`: numero di osservazioni\n",
    "- `n_features`: numero di feature\n",
    "- `n_informative`: feature effettivamente utili\n",
    "- `class_sep`: separabilita delle classi (piu alto = piu facile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d8bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (600, 6)\n",
      "Positive class ratio: 0.50\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SETUP: Import librerie e creazione dataset sintetico\n",
    "# =============================================================================\n",
    "# Usiamo make_classification per creare un dataset controllato.\n",
    "# Questo ci permette di:\n",
    "# - Capire il flusso ML senza preoccuparci della qualità dei dati\n",
    "# - Conoscere esattamente quante feature sono informative\n",
    "# - Controllare la difficoltà del problema (class_sep)\n",
    "#\n",
    "# In produzione userai dati reali, ma per imparare è meglio iniziare così!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"=== SETUP: Creazione Dataset Sintetico ===\\n\")\n",
    "\n",
    "# Creiamo un dataset sintetico per classificazione binaria\n",
    "# Parametri spiegati:\n",
    "# - n_samples=600: 600 osservazioni (righe)\n",
    "# - n_features=6: 6 feature (colonne di input)\n",
    "# - n_informative=4: 4 di queste feature sono realmente utili\n",
    "# - n_redundant=0: nessuna feature ridondante\n",
    "# - class_sep=1.5: separabilità delle classi (più alto = più facile)\n",
    "# - random_state=42: per riproducibilità\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=600,\n",
    "    n_features=6,\n",
    "    n_informative=4,\n",
    "    n_redundant=0,\n",
    "    class_sep=1.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MICRO-CHECKPOINT: Verifica struttura del dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "print(f\"Struttura del dataset:\")\n",
    "print(f\"  X (features): {X.shape} → {X.shape[0]} campioni, {X.shape[1]} feature\")\n",
    "print(f\"  y (target):   {y.shape} → {y.shape[0]} etichette\")\n",
    "\n",
    "print(f\"\\nStatistiche del target:\")\n",
    "print(f\"  Classi uniche: {np.unique(y)}\")\n",
    "print(f\"  Bilanciamento: {y.mean():.3f} (0.5 = perfettamente bilanciato)\")\n",
    "print(f\"  Classe 0: {(y == 0).sum()} campioni\")\n",
    "print(f\"  Classe 1: {(y == 1).sum()} campioni\")\n",
    "\n",
    "# Verifica forme\n",
    "assert X.shape == (600, 6), f\"ERRORE: Shape X = {X.shape}, atteso (600, 6)\"\n",
    "assert y.shape == (600,), f\"ERRORE: Shape y = {y.shape}, atteso (600,)\"\n",
    "assert set(y) == {0, 1}, \"ERRORE: y deve contenere solo 0 e 1\"\n",
    "\n",
    "print(\"\\n--- Micro-checkpoint: dataset creato correttamente ✓ ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f94e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.2 Train/Test Split\n",
    "\n",
    "### Perché dividere i dati?\n",
    "\n",
    "Il **test set** serve per valutare il modello su dati **MAI visti** durante l'addestramento.  \n",
    "Se valutiamo sugli stessi dati usati per il training, otteniamo una stima **ottimistica** (overfitting).\n",
    "\n",
    "### Analogia\n",
    "\n",
    "> È come studiare con le soluzioni dell'esame. Prendi 10, ma non hai imparato nulla.  \n",
    "> Il test set è l'esame vero, senza soluzioni.\n",
    "\n",
    "### Parametri chiave di `train_test_split()`\n",
    "\n",
    "| Parametro | Valore | Significato |\n",
    "|-----------|--------|-------------|\n",
    "| `test_size` | 0.25 | 25% dei dati per il test (75% per training) |\n",
    "| `stratify` | y | Mantiene le proporzioni delle classi in entrambi i set |\n",
    "| `random_state` | 42 | Garantisce riproducibilità |\n",
    "\n",
    "### Perché `stratify=y`?\n",
    "\n",
    "Se il dataset ha 60% classe 0 e 40% classe 1:\n",
    "- **Senza stratify**: il test set potrebbe avere 80% classe 0 (sbilanciato!)\n",
    "- **Con stratify**: il test set avrà 60% classe 0 e 40% classe 1 (proporzioni preservate)\n",
    "\n",
    "### Proporzioni tipiche\n",
    "\n",
    "| Train | Test | Quando usarla |\n",
    "|-------|------|---------------|\n",
    "| 75% | 25% | Default ragionevole |\n",
    "| 80% | 20% | Dataset medio-grande |\n",
    "| 90% | 10% | Dataset molto grande |\n",
    "| 60% | 40% | Dataset piccolo (più dati per valutazione) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4fb90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 6) (150, 6)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRAIN/TEST SPLIT: Divisione dei dati\n",
    "# =============================================================================\n",
    "# Dividiamo i dati in due set:\n",
    "# - Training set (75%): usato per addestrare il modello\n",
    "# - Test set (25%): usato per valutare il modello su dati \"nuovi\"\n",
    "#\n",
    "# IMPORTANTE: il test set simula dati futuri mai visti.\n",
    "# Se valutiamo sul training, otteniamo stime ottimistiche (overfitting)!\n",
    "\n",
    "print(\"=== TRAIN/TEST SPLIT ===\\n\")\n",
    "\n",
    "# Divisione stratificata: mantiene le proporzioni delle classi\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,      # 25% per test\n",
    "    stratify=y,          # Mantieni proporzioni classi\n",
    "    random_state=42      # Riproducibilità\n",
    ")\n",
    "\n",
    "# Mostra le dimensioni\n",
    "print(f\"Dataset originale: {len(y)} campioni\")\n",
    "print(f\"Training set:      {len(y_train)} campioni ({len(y_train)/len(y)*100:.0f}%)\")\n",
    "print(f\"Test set:          {len(y_test)} campioni ({len(y_test)/len(y)*100:.0f}%)\")\n",
    "\n",
    "# Verifica stratificazione\n",
    "prop_train = y_train.mean()\n",
    "prop_test = y_test.mean()\n",
    "\n",
    "print(f\"\\n=== Verifica Stratificazione ===\")\n",
    "print(f\"Proporzione classe 1 nel dataset originale: {y.mean():.3f}\")\n",
    "print(f\"Proporzione classe 1 nel training set:     {prop_train:.3f}\")\n",
    "print(f\"Proporzione classe 1 nel test set:         {prop_test:.3f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MICRO-CHECKPOINT: Verifica correttezza dello split\n",
    "# -----------------------------------------------------------------------------\n",
    "# Le proporzioni devono essere quasi uguali\n",
    "differenza = abs(prop_train - prop_test)\n",
    "assert differenza < 0.05, \\\n",
    "    f\"ERRORE: stratificazione non corretta! Differenza: {differenza:.3f}\"\n",
    "\n",
    "# Verifica che non ci siano sovrapposizioni (ogni campione è in train OR test)\n",
    "assert len(y_train) + len(y_test) == len(y), \\\n",
    "    \"ERRORE: la somma di train e test non corrisponde al totale!\"\n",
    "\n",
    "print(f\"\\n--- Micro-checkpoint: split stratificato corretto ✓ ---\")\n",
    "print(f\"    Differenza proporzioni: {differenza:.4f} < 0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c9674",
   "metadata": {},
   "source": [
    "### 4.3 Standardizzazione delle Feature (StandardScaler)\n",
    "\n",
    "#### Perché Scalare è Fondamentale?\n",
    "\n",
    "La **standardizzazione** (o scaling) trasforma le feature per avere **media 0** e **deviazione standard 1**. Questo passaggio è **critico** per molti algoritmi di machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "#### L'Analogia del \"Campo di Gioco Equo\"\n",
    "\n",
    "Immagina di organizzare una gara in cui:\n",
    "- Un corridore misura la distanza in **metri** (0-100)\n",
    "- Un altro misura in **centimetri** (0-10000)\n",
    "- Un terzo misura in **chilometri** (0-0.1)\n",
    "\n",
    "Chi vince? Senza una scala comune, il confronto è **impossibile**!\n",
    "\n",
    "---\n",
    "\n",
    "#### Formula della Standardizzazione (Z-score)\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Dove:\n",
    "- $x$ = valore originale\n",
    "- $\\mu$ = media della feature\n",
    "- $\\sigma$ = deviazione standard della feature\n",
    "- $z$ = valore standardizzato\n",
    "\n",
    "**Interpretazione**: il valore standardizzato indica **quante deviazioni standard** il dato dista dalla media.\n",
    "\n",
    "| z-score | Significato |\n",
    "|---------|-------------|\n",
    "| 0 | Il valore è esattamente nella media |\n",
    "| +1 | Una deviazione standard sopra la media |\n",
    "| -2 | Due deviazioni standard sotto la media |\n",
    "\n",
    "---\n",
    "\n",
    "#### Quali Algoritmi RICHIEDONO lo Scaling?\n",
    "\n",
    "| Algoritmo | Scaling Necessario? | Perché |\n",
    "|-----------|---------------------|--------|\n",
    "| **Logistic Regression** | ✅ Sì | Convergenza del gradient descent |\n",
    "| **SVM** | ✅ Sì | Calcolo distanze nello spazio |\n",
    "| **K-Nearest Neighbors** | ✅ Sì | Distanze euclidee |\n",
    "| **Neural Networks** | ✅ Sì | Attivazioni e gradienti stabili |\n",
    "| **Decision Trees** | ❌ No | Split basati su soglie, invarianti alla scala |\n",
    "| **Random Forest** | ❌ No | Ensemble di alberi |\n",
    "| **XGBoost/LightGBM** | ❌ No* | *Può aiutare per convergenza |\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚠️ REGOLA D'ORO: Fit su Train, Transform su Tutto\n",
    "\n",
    "```\n",
    "scaler.fit(X_train)       # Calcola media e std SOLO dal training\n",
    "scaler.transform(X_train) # Applica la trasformazione al training\n",
    "scaler.transform(X_test)  # Applica la STESSA trasformazione al test\n",
    "```\n",
    "\n",
    "**ERRORE GRAVISSIMO (Data Leakage)**:\n",
    "```python\n",
    "# MAI FARE QUESTO!\n",
    "scaler.fit(X)  # Usi info del test per calcolare media/std\n",
    "```\n",
    "\n",
    "Se fai fit sull'intero dataset, le statistiche del test \"contaminano\" il training, portando a **stime ottimistiche** delle performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc16d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.35214359e-17 -4.49455288e-16  3.50337043e-17  2.91125149e-17\n",
      "  5.21188031e-16 -1.48029737e-18]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STANDARDIZZAZIONE: Scaling delle Feature\n",
    "# =============================================================================\n",
    "# StandardScaler applica la trasformazione z-score:\n",
    "#   z = (x - media) / deviazione_standard\n",
    "#\n",
    "# Dopo la trasformazione: media ≈ 0, std ≈ 1\n",
    "\n",
    "print(\"=== STANDARDIZZAZIONE CON StandardScaler ===\\n\")\n",
    "\n",
    "# Creiamo lo scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# FIT: calcola media e std SOLO sul training set\n",
    "# Questo è CRUCIALE per evitare data leakage!\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Mostra i parametri appresi\n",
    "print(\"Parametri appresi dallo scaler (sul training set):\")\n",
    "print(f\"  Medie delle feature:     {scaler.mean_[:3]}... (prime 3)\")\n",
    "print(f\"  Std delle feature:       {scaler.scale_[:3]}... (prime 3)\")\n",
    "\n",
    "# TRANSFORM: applica la trasformazione\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # USA media/std del TRAINING!\n",
    "\n",
    "# Verifica la trasformazione\n",
    "print(f\"\\n=== Prima della standardizzazione (Training) ===\")\n",
    "print(f\"  Media feature 0:  {X_train[:, 0].mean():.4f}\")\n",
    "print(f\"  Std feature 0:    {X_train[:, 0].std():.4f}\")\n",
    "\n",
    "print(f\"\\n=== Dopo la standardizzazione (Training) ===\")\n",
    "print(f\"  Media feature 0:  {X_train_scaled[:, 0].mean():.6f} (≈ 0)\")\n",
    "print(f\"  Std feature 0:    {X_train_scaled[:, 0].std():.6f} (≈ 1)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MICRO-CHECKPOINT: Verifica standardizzazione corretta\n",
    "# -----------------------------------------------------------------------------\n",
    "mean_after = np.abs(X_train_scaled.mean(axis=0))  # Medie per colonna\n",
    "std_after = X_train_scaled.std(axis=0)            # Std per colonna\n",
    "\n",
    "# Le medie devono essere vicine a 0\n",
    "assert mean_after.max() < 1e-10, \\\n",
    "    f\"ERRORE: media non nulla! Max: {mean_after.max():.2e}\"\n",
    "\n",
    "# Le std devono essere vicine a 1\n",
    "assert np.allclose(std_after, 1, atol=0.01), \\\n",
    "    f\"ERRORE: std non unitaria!\"\n",
    "\n",
    "print(f\"\\n--- Micro-checkpoint: standardizzazione corretta ✓ ---\")\n",
    "print(f\"    Max media assoluta: {mean_after.max():.2e}\")\n",
    "print(f\"    Range std: [{std_after.min():.4f}, {std_after.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f405a",
   "metadata": {},
   "source": [
    "### 4.4 Addestramento del Modello: Logistic Regression\n",
    "\n",
    "#### L'API Universale di Scikit-Learn: fit()\n",
    "\n",
    "Ogni modello in scikit-learn segue lo stesso pattern:\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train)  # Addestra il modello\n",
    "```\n",
    "\n",
    "**Cosa succede durante il fit?**\n",
    "1. L'algoritmo \"studia\" i dati di training\n",
    "2. Ottimizza i parametri interni del modello\n",
    "3. Cerca di minimizzare l'errore di predizione\n",
    "\n",
    "---\n",
    "\n",
    "#### Logistic Regression: Il \"Primo Modello\" per la Classificazione\n",
    "\n",
    "Nonostante il nome, la Logistic Regression è un classificatore (non un regressore!).\n",
    "\n",
    "**Come funziona** (intuizione):\n",
    "1. Calcola una combinazione lineare delle feature: $z = w_1 x_1 + w_2 x_2 + ... + b$\n",
    "2. Applica la funzione **sigmoid** per ottenere una probabilità:\n",
    "\n",
    "$$P(y=1|x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "3. Se $P > 0.5$, predice classe 1, altrimenti classe 0\n",
    "\n",
    "---\n",
    "\n",
    "#### Parametri Importanti di LogisticRegression\n",
    "\n",
    "| Parametro | Default | Descrizione |\n",
    "|-----------|---------|-------------|\n",
    "| `C` | 1.0 | Inverso della regolarizzazione (↑C = meno regolarizzazione) |\n",
    "| `penalty` | 'l2' | Tipo di regolarizzazione (L1, L2, elasticnet, none) |\n",
    "| `solver` | 'lbfgs' | Algoritmo di ottimizzazione |\n",
    "| `max_iter` | 100 | Numero massimo di iterazioni |\n",
    "| `random_state` | None | Seed per riproducibilità |\n",
    "\n",
    "---\n",
    "\n",
    "#### Regolarizzazione: Perché `C` è Importante?\n",
    "\n",
    "La regolarizzazione **penalizza i coefficienti troppo grandi**, prevenendo l'overfitting.\n",
    "\n",
    "| Valore di C | Effetto |\n",
    "|-------------|---------|\n",
    "| C molto piccolo (0.001) | Forte regolarizzazione, modello più semplice |\n",
    "| C = 1 (default) | Bilanciamento |\n",
    "| C molto grande (1000) | Poca regolarizzazione, modello più complesso |\n",
    "\n",
    "**Intuizione**: C controlla quanto il modello può \"fidarsi\" dei dati. Con C alto, si fida molto (rischio overfitting). Con C basso, è più conservativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8283d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "bda666bd-a8c6-490b-ab67-e75d201e499f",
       "rows": [
        [
         "feature_1",
         "1.1586538968827325"
        ],
        [
         "feature_4",
         "0.7823028257283188"
        ],
        [
         "feature_5",
         "0.598549870239413"
        ],
        [
         "feature_2",
         "0.09697855791010884"
        ],
        [
         "feature_3",
         "0.06530627582463851"
        ],
        [
         "feature_0",
         "-0.8244110452495775"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 6
       }
      },
      "text/plain": [
       "feature_1    1.158654\n",
       "feature_4    0.782303\n",
       "feature_5    0.598550\n",
       "feature_2    0.096979\n",
       "feature_3    0.065306\n",
       "feature_0   -0.824411\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ADDESTRAMENTO: fit() della Logistic Regression\n",
    "# =============================================================================\n",
    "# Il metodo fit() \"addestra\" il modello:\n",
    "# - Trova i pesi ottimali (coefficienti)\n",
    "# - Minimizza la loss function (log-loss per classificazione)\n",
    "# - Itera fino a convergenza o max_iter\n",
    "\n",
    "print(\"=== ADDESTRAMENTO LOGISTIC REGRESSION ===\\n\")\n",
    "\n",
    "# Creazione e addestramento del modello\n",
    "model = LogisticRegression(\n",
    "    C=1.0,              # Forza della regolarizzazione (inverso)\n",
    "    penalty='l2',       # Regolarizzazione L2 (ridge)\n",
    "    solver='lbfgs',     # Algoritmo di ottimizzazione\n",
    "    max_iter=1000,      # Iterazioni massime (aumentato per sicurezza)\n",
    "    random_state=42     # Riproducibilità\n",
    ")\n",
    "\n",
    "# Addestramento sui dati SCALATI\n",
    "# NOTA: usiamo X_train_scaled, NON X_train!\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Modello addestrato con successo!\")\n",
    "print(f\"Numero di iterazioni effettuate: {model.n_iter_[0]}\")\n",
    "\n",
    "# Esploriamo i parametri appresi dal modello\n",
    "print(f\"\\n=== Parametri del Modello ===\")\n",
    "print(f\"Numero di coefficienti: {len(model.coef_[0])}\")\n",
    "print(f\"Primi 5 coefficienti: {model.coef_[0][:5]}\")\n",
    "print(f\"Intercetta (bias): {model.intercept_[0]:.4f}\")\n",
    "\n",
    "# Coefficienti più importanti (in valore assoluto)\n",
    "coef_abs = np.abs(model.coef_[0])\n",
    "top_idx = np.argsort(coef_abs)[-3:][::-1]  # Top 3\n",
    "\n",
    "print(f\"\\n=== Feature più Influenti (coefficienti maggiori) ===\")\n",
    "for rank, idx in enumerate(top_idx, 1):\n",
    "    print(f\"  {rank}. Feature {idx}: coef = {model.coef_[0][idx]:.4f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MICRO-CHECKPOINT: Verifica addestramento completato\n",
    "# -----------------------------------------------------------------------------\n",
    "# Il modello deve avere coefficienti definiti\n",
    "assert hasattr(model, 'coef_'), \"ERRORE: modello non addestrato!\"\n",
    "assert model.coef_.shape == (1, X_train_scaled.shape[1]), \\\n",
    "    \"ERRORE: forma coefficienti non corretta!\"\n",
    "\n",
    "# Deve aver raggiunto la convergenza (meno di max_iter)\n",
    "converged = model.n_iter_[0] < 1000\n",
    "status = \"CONVERGENTE ✓\" if converged else \"MAX ITER RAGGIUNTO ⚠️\"\n",
    "\n",
    "print(f\"\\n--- Micro-checkpoint: addestramento completato ✓ ---\")\n",
    "print(f\"    Status convergenza: {status}\")\n",
    "print(f\"    Coefficienti: shape {model.coef_.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46f5aa",
   "metadata": {},
   "source": [
    "### 4.5 Predizione e Valutazione\n",
    "\n",
    "#### L'API Universale di Scikit-Learn: predict()\n",
    "\n",
    "Dopo l'addestramento, possiamo fare predizioni:\n",
    "\n",
    "```python\n",
    "y_pred = model.predict(X_test)  # Predice le classi\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### predict() vs predict_proba()\n",
    "\n",
    "| Metodo | Output | Uso |\n",
    "|--------|--------|-----|\n",
    "| `predict(X)` | Etichette di classe (0 o 1) | Classificazione diretta |\n",
    "| `predict_proba(X)` | Probabilità per ogni classe | Quando serve la \"confidenza\" |\n",
    "\n",
    "**Esempio predict_proba**:\n",
    "```python\n",
    "proba = model.predict_proba(X_test)\n",
    "# proba[i, 0] = P(classe 0)\n",
    "# proba[i, 1] = P(classe 1)\n",
    "```\n",
    "\n",
    "La soglia di default è 0.5: se P(classe 1) > 0.5, predice 1.\n",
    "\n",
    "---\n",
    "\n",
    "#### Accuracy: La Metrica Base\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{Predizioni Corrette}}{\\text{Totale Predizioni}} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "| Termine | Significato |\n",
    "|---------|-------------|\n",
    "| TP (True Positive) | Predetto 1, era 1 ✓ |\n",
    "| TN (True Negative) | Predetto 0, era 0 ✓ |\n",
    "| FP (False Positive) | Predetto 1, era 0 ✗ |\n",
    "| FN (False Negative) | Predetto 0, era 1 ✗ |\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚠️ Quando l'Accuracy NON Basta\n",
    "\n",
    "L'accuracy può essere **fuorviante** con dataset sbilanciati:\n",
    "\n",
    "| Scenario | Classe 0 | Classe 1 | Accuracy \"stupida\" |\n",
    "|----------|----------|----------|-------------------|\n",
    "| Bilanciato | 50% | 50% | 50% (random) |\n",
    "| Sbilanciato | 95% | 5% | 95% (predici sempre 0!) |\n",
    "\n",
    "**Nel caso sbilanciato**: un modello che predice SEMPRE classe 0 ha 95% di accuracy, ma è inutile!\n",
    "\n",
    "Per dataset sbilanciati useremo metriche più sofisticate (precision, recall, F1) nelle lezioni successive.\n",
    "\n",
    "---\n",
    "\n",
    "#### Perché Valutiamo sul TEST Set?\n",
    "\n",
    "- **Training accuracy**: quanto bene il modello \"ricorda\" i dati visti\n",
    "- **Test accuracy**: quanto bene il modello **generalizza** a dati nuovi\n",
    "\n",
    "Se training accuracy >> test accuracy → **Overfitting** (il modello ha memorizzato, non imparato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83218f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.727\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PREDIZIONE E VALUTAZIONE\n",
    "# =============================================================================\n",
    "# Usiamo il modello addestrato per fare predizioni sul test set\n",
    "# e valutare le performance con accuracy_score\n",
    "\n",
    "print(\"=== PREDIZIONE E VALUTAZIONE ===\\n\")\n",
    "\n",
    "# Predizione sui dati di TEST (scalati!)\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "# Predizione anche sul training per confronto\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "\n",
    "# Calcolo accuracy\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"=== Accuracy del Modello ===\")\n",
    "print(f\"Training Accuracy: {acc_train:.4f} ({acc_train*100:.1f}%)\")\n",
    "print(f\"Test Accuracy:     {acc_test:.4f} ({acc_test*100:.1f}%)\")\n",
    "\n",
    "# Gap tra train e test (indicatore di overfitting)\n",
    "gap = acc_train - acc_test\n",
    "print(f\"\\nGap Train-Test: {gap:.4f}\")\n",
    "\n",
    "if gap > 0.05:\n",
    "    print(\"⚠️  ATTENZIONE: possibile overfitting (gap > 5%)\")\n",
    "elif gap < 0:\n",
    "    print(\"❓ Strano: test accuracy > train accuracy\")\n",
    "else:\n",
    "    print(\"✓  Gap accettabile: il modello generalizza bene\")\n",
    "\n",
    "# Esaminiamo anche le probabilità predette\n",
    "y_proba_test = model.predict_proba(X_test_scaled)\n",
    "\n",
    "print(f\"\\n=== Probabilità Predette (primi 5 campioni) ===\")\n",
    "print(f\"{'Campione':^10} {'P(classe 0)':^12} {'P(classe 1)':^12} {'Predetto':^10} {'Reale':^8}\")\n",
    "print(\"-\" * 55)\n",
    "for i in range(5):\n",
    "    print(f\"{i:^10} {y_proba_test[i, 0]:^12.3f} {y_proba_test[i, 1]:^12.3f} \"\n",
    "          f\"{y_pred_test[i]:^10} {y_test[i]:^8}\")\n",
    "\n",
    "# Analisi degli errori\n",
    "errori = y_pred_test != y_test\n",
    "n_errori = errori.sum()\n",
    "print(f\"\\n=== Analisi Errori ===\")\n",
    "print(f\"Errori totali: {n_errori}/{len(y_test)} ({n_errori/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MICRO-CHECKPOINT: Verifica predizioni valide\n",
    "# -----------------------------------------------------------------------------\n",
    "# Le predizioni devono essere solo 0 o 1\n",
    "assert set(y_pred_test).issubset({0, 1}), \\\n",
    "    \"ERRORE: predizioni non valide!\"\n",
    "\n",
    "# L'accuracy deve essere ragionevole (meglio del caso random)\n",
    "assert acc_test > 0.5, \\\n",
    "    f\"ERRORE: accuracy ({acc_test:.2f}) peggiore del caso random!\"\n",
    "\n",
    "# Le probabilità devono sommare a 1\n",
    "proba_sum = y_proba_test.sum(axis=1)\n",
    "assert np.allclose(proba_sum, 1), \\\n",
    "    \"ERRORE: le probabilità non sommano a 1!\"\n",
    "\n",
    "print(f\"\\n--- Micro-checkpoint: predizioni valide ✓ ---\")\n",
    "print(f\"    Accuracy test: {acc_test:.4f} > 0.5 (baseline)\")\n",
    "print(f\"    Predizioni: valori in {{0, 1}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb33e95",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5) Esercizi Risolti (Step by Step)\n",
    "\n",
    "In questa sezione consolidiamo quanto appreso con una **reference card** completa di tutti i metodi utilizzati, organizzati per fase del workflow ML.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 Creazione Dataset Sintetico\n",
    "\n",
    "| Metodo | Sintassi | Cosa Fa | Output |\n",
    "|--------|----------|---------|--------|\n",
    "| `make_classification()` | `make_classification(n_samples=200, n_features=10)` | Genera dataset classificazione | `X, y` arrays |\n",
    "| Parametro `n_informative` | `n_informative=5` | Feature realmente predittive | int |\n",
    "| Parametro `n_redundant` | `n_redundant=2` | Feature combinazioni lineari | int |\n",
    "| Parametro `n_clusters_per_class` | `n_clusters_per_class=1` | Complessità delle classi | int |\n",
    "| Parametro `random_state` | `random_state=42` | Riproducibilità | int |\n",
    "\n",
    "**Pattern d'uso:**\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(\n",
    "    n_samples=200, n_features=10, n_informative=5,\n",
    "    n_redundant=2, n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2 Train/Test Split\n",
    "\n",
    "| Metodo | Sintassi | Cosa Fa | Output |\n",
    "|--------|----------|---------|--------|\n",
    "| `train_test_split()` | `train_test_split(X, y, test_size=0.25)` | Divide dati in train e test | 4 arrays |\n",
    "| Parametro `test_size` | `test_size=0.25` | Proporzione per test | float [0,1] |\n",
    "| Parametro `stratify` | `stratify=y` | Mantiene proporzioni classi | array-like |\n",
    "| Parametro `random_state` | `random_state=42` | Riproducibilità | int |\n",
    "| Parametro `shuffle` | `shuffle=True` (default) | Mischia i dati prima dello split | bool |\n",
    "\n",
    "**Pattern d'uso:**\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3 Preprocessing - StandardScaler\n",
    "\n",
    "| Metodo | Sintassi | Cosa Fa | Output |\n",
    "|--------|----------|---------|--------|\n",
    "| `StandardScaler()` | `scaler = StandardScaler()` | Crea oggetto scaler | estimator |\n",
    "| `.fit()` | `scaler.fit(X_train)` | Calcola media/std | self |\n",
    "| `.transform()` | `scaler.transform(X)` | Applica trasformazione | array scaled |\n",
    "| `.fit_transform()` | `scaler.fit_transform(X_train)` | Fit + Transform insieme | array scaled |\n",
    "| `.mean_` | `scaler.mean_` | Medie calcolate | array |\n",
    "| `.scale_` | `scaler.scale_` | Deviazioni standard | array |\n",
    "| `.inverse_transform()` | `scaler.inverse_transform(X_scaled)` | Riporta a scala originale | array |\n",
    "\n",
    "**Pattern d'uso (CORRETTO):**\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit + transform su train\n",
    "X_test_scaled = scaler.transform(X_test)        # SOLO transform su test!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5.4 Modello - LogisticRegression\n",
    "\n",
    "| Metodo | Sintassi | Cosa Fa | Output |\n",
    "|--------|----------|---------|--------|\n",
    "| `LogisticRegression()` | `model = LogisticRegression()` | Crea modello | estimator |\n",
    "| `.fit()` | `model.fit(X_train, y_train)` | Addestra il modello | self |\n",
    "| `.predict()` | `model.predict(X_test)` | Predice le classi | array (0/1) |\n",
    "| `.predict_proba()` | `model.predict_proba(X_test)` | Predice probabilità | array (n, 2) |\n",
    "| `.coef_` | `model.coef_` | Coefficienti (pesi) | array (1, n_features) |\n",
    "| `.intercept_` | `model.intercept_` | Intercetta (bias) | array (1,) |\n",
    "| `.n_iter_` | `model.n_iter_` | Iterazioni effettuate | array |\n",
    "| `.classes_` | `model.classes_` | Etichette delle classi | array |\n",
    "\n",
    "**Pattern d'uso:**\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5.5 Valutazione - Metriche\n",
    "\n",
    "| Metodo | Sintassi | Cosa Fa | Output |\n",
    "|--------|----------|---------|--------|\n",
    "| `accuracy_score()` | `accuracy_score(y_true, y_pred)` | Proporzione corrette | float [0,1] |\n",
    "| Parametro `normalize` | `normalize=False` | Conta invece di proporzione | int |\n",
    "\n",
    "**Pattern d'uso:**\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240f41a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.6 Glossario Essenziale\n",
    "\n",
    "Questa tabella definisce tutti i termini tecnici utilizzati nella lezione. Consultala quando incontri un termine non chiaro.\n",
    "\n",
    "| Termine | Definizione | Esempio/Contesto |\n",
    "|---------|-------------|------------------|\n",
    "| **Feature** | Variabile di input (colonna di X) usata per la previsione | Età, reddito, altezza |\n",
    "| **Target** | Variabile da predire (y) | 0/1 per classificazione binaria |\n",
    "| **Training Set** | Dati usati per addestrare il modello | 75% del dataset |\n",
    "| **Test Set** | Dati usati per valutare il modello (mai visti in training) | 25% del dataset |\n",
    "| **Fit** | Processo di addestramento: il modello \"impara\" dai dati | `model.fit(X, y)` |\n",
    "| **Predict** | Generazione di previsioni dal modello addestrato | `model.predict(X_new)` |\n",
    "| **Standardizzazione** | Trasformazione per avere media=0 e std=1 (z-score) | `StandardScaler()` |\n",
    "| **Data Leakage** | Uso improprio di informazioni del test nel training | Fit scaler su tutto X |\n",
    "| **Accuracy** | Proporzione di previsioni corrette su totale | 0.85 = 85% corrette |\n",
    "| **Coefficiente** | Peso assegnato a ogni feature dal modello lineare | `model.coef_` |\n",
    "| **Intercetta (Bias)** | Termine costante aggiunto alla combinazione lineare | `model.intercept_` |\n",
    "| **Sigmoide** | Funzione che comprime valori in [0,1]: $\\sigma(z) = 1/(1+e^{-z})$ | Output della logistic |\n",
    "| **Logit** | Logaritmo degli odds, input della sigmoide | Valore lineare $z$ |\n",
    "| **Stratificazione** | Mantenere le proporzioni delle classi nello split | `stratify=y` |\n",
    "| **Overfitting** | Modello troppo adattato ai dati di training, generalizza male | Train acc >> Test acc |\n",
    "| **Underfitting** | Modello troppo semplice, non cattura i pattern | Train acc e Test acc bassi |\n",
    "| **Regolarizzazione** | Penalità sui coefficienti per prevenire overfitting | Parametro `C` |\n",
    "| **Convergenza** | L'algoritmo ha trovato una soluzione stabile | `n_iter_ < max_iter` |\n",
    "| **Iperparametro** | Parametro impostato dall'utente, non appreso dal modello | C, max_iter, penalty |\n",
    "\n",
    "---\n",
    "\n",
    "### Distinzione Importante: Parametri vs Iperparametri\n",
    "\n",
    "| Categoria | Chi li Imposta | Quando | Esempi |\n",
    "|-----------|----------------|--------|--------|\n",
    "| **Parametri** | L'algoritmo durante il fit | Training | coef_, intercept_ |\n",
    "| **Iperparametri** | L'utente prima del fit | Configurazione | C, max_iter, penalty |\n",
    "\n",
    "I **parametri** vengono appresi dai dati. Gli **iperparametri** controllano come avviene l'apprendimento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e89b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.7 Errori Comuni e Debug Rapido\n",
    "\n",
    "Questa sezione raccoglie gli errori più frequenti quando si implementa una pipeline di classificazione. Per ogni errore: sintomo, causa, soluzione.\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Errore 1: Data Leakage - Fit dello Scaler su Tutto il Dataset\n",
    "\n",
    "**Sintomo**: Test accuracy troppo alta, performance reali deludenti\n",
    "\n",
    "**Causa**: Le statistiche del test \"contaminano\" il training\n",
    "\n",
    "```python\n",
    "# SBAGLIATO - data leakage!\n",
    "scaler.fit(X)                    # Usa info del test per calcolare media/std\n",
    "X_scaled = scaler.transform(X)\n",
    "X_train, X_test = train_test_split(X_scaled)\n",
    "\n",
    "# CORRETTO - split PRIMA dello scaling\n",
    "X_train, X_test = train_test_split(X)\n",
    "scaler.fit(X_train)                        # Fit SOLO su train\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)   # Transform con parametri del train\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Errore 2: Dimenticare di Scalare il Test Set\n",
    "\n",
    "**Sintomo**: `ValueError` o predizioni completamente sbagliate\n",
    "\n",
    "**Causa**: Il modello è stato addestrato su dati scalati, ma predice su dati non scalati\n",
    "\n",
    "```python\n",
    "# SBAGLIATO - scale mismatch!\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test)  # X_test NON scalato!\n",
    "\n",
    "# CORRETTO\n",
    "y_pred = model.predict(X_test_scaled)  # Usa dati scalati\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Errore 3: Confondere predict() e predict_proba()\n",
    "\n",
    "**Sintomo**: Risultati inaspettati, errori di tipo\n",
    "\n",
    "**Causa**: Confusione tra classi e probabilità\n",
    "\n",
    "```python\n",
    "# .predict() restituisce CLASSI (0 o 1)\n",
    "y_pred = model.predict(X_test)\n",
    "# Output: array([0, 1, 1, 0, 1, ...])\n",
    "\n",
    "# .predict_proba() restituisce PROBABILITÀ (matrice n×2)\n",
    "y_proba = model.predict_proba(X_test)\n",
    "# Output: array([[0.3, 0.7], [0.8, 0.2], ...])\n",
    "# Colonna 0: P(classe 0), Colonna 1: P(classe 1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Errore 4: Non Impostare random_state\n",
    "\n",
    "**Sintomo**: Risultati diversi ad ogni esecuzione\n",
    "\n",
    "**Causa**: Operazioni casuali senza seed\n",
    "\n",
    "```python\n",
    "# SBAGLIATO - risultati non riproducibili\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# CORRETTO - sempre riproducibile\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "model = LogisticRegression(random_state=42)  # Anche nel modello!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Errore 5: Ordine Errato delle Operazioni\n",
    "\n",
    "**Sintomo**: Data leakage o errori\n",
    "\n",
    "**Causa**: Scale prima dello split\n",
    "\n",
    "```python\n",
    "# SBAGLIATO - ordine errato\n",
    "X_scaled = scaler.fit_transform(X)         # 1. Scala TUTTO\n",
    "X_train, X_test = train_test_split(X_scaled)  # 2. Poi split\n",
    "\n",
    "# CORRETTO - ordine giusto\n",
    "X_train, X_test = train_test_split(X)      # 1. PRIMA split\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # 2. Poi scala train\n",
    "X_test_scaled = scaler.transform(X_test)   # 3. Poi scala test\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Errore 6: ConvergenceWarning\n",
    "\n",
    "**Sintomo**: Warning: `lbfgs failed to converge`\n",
    "\n",
    "**Causa**: L'algoritmo non ha trovato una soluzione in max_iter iterazioni\n",
    "\n",
    "```python\n",
    "# SOLUZIONE 1: aumenta max_iter\n",
    "model = LogisticRegression(max_iter=1000)  # Default è 100\n",
    "\n",
    "# SOLUZIONE 2: scala i dati (spesso risolve)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# SOLUZIONE 3: prova un solver diverso\n",
    "model = LogisticRegression(solver='saga', max_iter=500)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Errore 7: Fidarsi dell'Accuracy su Classi Sbilanciate\n",
    "\n",
    "**Sintomo**: Accuracy alta ma modello inutile\n",
    "\n",
    "**Causa**: Il modello predice sempre la classe maggioritaria\n",
    "\n",
    "```python\n",
    "# Se 95% dei dati è classe 0:\n",
    "# Un modello che predice SEMPRE 0 ha accuracy = 0.95!\n",
    "\n",
    "# SOLUZIONE: usa metriche aggiuntive\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Tabella Riassuntiva Debug\n",
    "\n",
    "| Errore | Sintomo | Fix Rapido |\n",
    "|--------|---------|------------|\n",
    "| Data leakage | Test acc troppo alta | Split PRIMA di fit |\n",
    "| Test non scalato | Predizioni sbagliate | Usa X_test_scaled |\n",
    "| predict vs proba | Tipo dati errato | Scegli il metodo giusto |\n",
    "| No random_state | Risultati variabili | Aggiungi random_state=42 |\n",
    "| Ordine sbagliato | Leakage/errori | Split → Scale → Fit |\n",
    "| ConvergenceWarning | Warning | max_iter=1000 |\n",
    "| Accuracy ingannevole | Modello inutile | Usa F1, precision, recall |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6d0e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6) Conclusione Operativa\n",
    "\n",
    "## Cosa Hai Imparato in Questa Lezione\n",
    "\n",
    "Questa lezione segna il **passaggio dall'analisi dati al Machine Learning**. Hai acquisito:\n",
    "\n",
    "### Competenze Teoriche\n",
    "1. **Tassonomia dei modelli ML**: Le quattro famiglie (lineari, tree-based, distanza, bayesiani) e quando usarle\n",
    "2. **Logistic Regression**: Come funziona matematicamente (combinazione lineare + sigmoide)\n",
    "3. **Concetto di generalizzazione**: Perché separiamo train e test\n",
    "4. **Standardizzazione**: Perché è necessaria per modelli lineari\n",
    "\n",
    "### Competenze Pratiche\n",
    "1. **Generazione dataset**: `make_classification()` per esperimenti controllati\n",
    "2. **Train/Test Split**: `train_test_split()` con stratificazione\n",
    "3. **Preprocessing**: `StandardScaler()` con il pattern fit-transform corretto\n",
    "4. **Training**: `model.fit(X, y)` - l'API universale di scikit-learn\n",
    "5. **Prediction**: `model.predict()` e `model.predict_proba()`\n",
    "6. **Evaluation**: `accuracy_score()` come prima metrica\n",
    "\n",
    "---\n",
    "\n",
    "## Pattern Ricorrente: La Pipeline di Classificazione\n",
    "\n",
    "Questo è il pattern che userai in OGNI progetto di classificazione:\n",
    "\n",
    "```python\n",
    "# ==========================================================\n",
    "# PIPELINE STANDARD DI CLASSIFICAZIONE - MEMORIZZA QUESTO!\n",
    "# ==========================================================\n",
    "\n",
    "# 1. IMPORT\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 2. SPLIT (prima di qualsiasi preprocessing!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. SCALING (fit su train, transform su entrambi)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. TRAINING\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 5. PREDICTION & EVALUATION\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Mappa Concettuale della Lezione\n",
    "\n",
    "```\n",
    "            MACHINE LEARNING\n",
    "                   │\n",
    "    ┌──────────────┴──────────────┐\n",
    "    │                             │\n",
    "SUPERVISIONE                  NON SUPERVISIONE\n",
    "(usa target y)                (no target)\n",
    "    │\n",
    "    ├── CLASSIFICAZIONE ◄── Questa lezione!\n",
    "    │      └── Logistic Regression\n",
    "    │\n",
    "    └── REGRESSIONE\n",
    "           └── Linear Regression (prossime lezioni)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Collegamento con le Prossime Lezioni\n",
    "\n",
    "| Lezione | Argomento | Cosa Aggiungerà |\n",
    "|---------|-----------|-----------------|\n",
    "| **06** | Modelli Lineari Avanzati | Regressione, Ridge, Lasso |\n",
    "| **07** | Validazione | Cross-validation, GridSearchCV |\n",
    "| **08** | Overfitting | Diagnosi e rimedi |\n",
    "| **09** | Tree-Based Models | Decision Tree, intuizione |\n",
    "| **10** | Gradient Boosting | XGBoost, LightGBM |\n",
    "| **11** | Metriche Avanzate | Precision, Recall, F1, ROC |\n",
    "\n",
    "---\n",
    "\n",
    "## Take-Home Messages\n",
    "\n",
    "> 🎯 **Messaggio 1**: Il Machine Learning è un processo sistematico (split → scale → fit → predict → score), non magia.\n",
    "\n",
    "> 🎯 **Messaggio 2**: Il data leakage è l'errore più comune e insidioso. Split PRIMA di tutto!\n",
    "\n",
    "> 🎯 **Messaggio 3**: L'accuracy non basta per dataset sbilanciati. Imparerai altre metriche.\n",
    "\n",
    "> 🎯 **Messaggio 4**: random_state=42 è tuo amico per la riproducibilità."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd411f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7) Checklist di Fine Lezione\n",
    "\n",
    "Prima di procedere alla prossima lezione, verifica di padroneggiare ogni competenza. Usa questa checklist come **autovalutazione**.\n",
    "\n",
    "---\n",
    "\n",
    "## Competenze Teoriche\n",
    "\n",
    "- [ ] **So elencare le 4 famiglie di modelli ML** (lineari, tree-based, distanza, bayesiani)\n",
    "- [ ] **Capisco quando usare un modello lineare vs tree-based** (dati lineari vs relazioni complesse)\n",
    "- [ ] **So spiegare cos'è la funzione sigmoide** e perché trasforma valori in probabilità\n",
    "- [ ] **Capisco la differenza tra parametri e iperparametri**\n",
    "- [ ] **So spiegare cos'è il data leakage** e perché è pericoloso\n",
    "\n",
    "---\n",
    "\n",
    "## Competenze Pratiche - train_test_split\n",
    "\n",
    "- [ ] **So usare `train_test_split()`** con tutti i parametri necessari\n",
    "- [ ] **Capisco il parametro `stratify=y`** e quando usarlo (classificazione)\n",
    "- [ ] **So impostare `random_state`** per riproducibilità\n",
    "\n",
    "---\n",
    "\n",
    "## Competenze Pratiche - StandardScaler\n",
    "\n",
    "- [ ] **Capisco perché lo scaling è necessario** per modelli lineari\n",
    "- [ ] **So usare `StandardScaler` correttamente**: fit su train, transform su entrambi\n",
    "- [ ] **Conosco la formula z-score**: $z = (x - \\mu) / \\sigma$\n",
    "- [ ] **So che i tree-based models NON richiedono scaling**\n",
    "\n",
    "---\n",
    "\n",
    "## Competenze Pratiche - LogisticRegression\n",
    "\n",
    "- [ ] **So creare e addestrare una `LogisticRegression`**\n",
    "- [ ] **Capisco la differenza tra `.predict()` e `.predict_proba()`**\n",
    "- [ ] **So interpretare i coefficienti** (segno = direzione, magnitudo = importanza)\n",
    "- [ ] **So cosa significa il parametro `C`** (regolarizzazione)\n",
    "\n",
    "---\n",
    "\n",
    "## Competenze Pratiche - Valutazione\n",
    "\n",
    "- [ ] **So calcolare l'accuracy** con `accuracy_score()`\n",
    "- [ ] **Capisco i limiti dell'accuracy** su dataset sbilanciati\n",
    "- [ ] **So interpretare il gap train-test** come indicatore di overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## Competenze di Debug\n",
    "\n",
    "- [ ] **So riconoscere e correggere il data leakage**\n",
    "- [ ] **So gestire il ConvergenceWarning** (aumentare max_iter)\n",
    "- [ ] **Conosco l'ordine corretto delle operazioni**: split → scale → fit → predict\n",
    "\n",
    "---\n",
    "\n",
    "## Verifica Pratica\n",
    "\n",
    "Sei pronto per la prossima lezione se riesci a scrivere **da zero** (senza guardare) questa pipeline:\n",
    "\n",
    "```python\n",
    "# Scrivi questo codice a memoria per verificare la tua comprensione\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "**Se hai dubbi su qualche punto, rileggi la sezione corrispondente prima di proseguire.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b5a49",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8) Changelog Didattico\n",
    "\n",
    "Registro delle modifiche apportate al notebook per garantire tracciabilità e trasparenza del processo di miglioramento continuo.\n",
    "\n",
    "---\n",
    "\n",
    "| Versione | Data | Autore | Modifiche |\n",
    "|----------|------|--------|-----------|\n",
    "| 1.0 | Originale | - | Versione iniziale: teoria estesa sulle 4 famiglie di modelli, poco codice pratico |\n",
    "| 2.0 | 2025-01 | Ristrutturazione | Riorganizzazione completa secondo template a 8 sezioni |\n",
    "| | | | **Sezione 1**: Header espanso con obiettivi, prerequisiti, mappa concettuale |\n",
    "| | | | **Sezione 2**: Teoria consolidata con formule LaTeX e analogie |\n",
    "| | | | **Sezione 3**: Schema mentale con decision tree per scelta modello |\n",
    "| | | | **Sezione 4**: Notebook dimostrativo completo con micro-checkpoint |\n",
    "| | | | - Aggiunto \"Perché questo passaggio\" prima di ogni cella di codice |\n",
    "| | | | - Aggiunte asserzioni e sanity check in ogni step |\n",
    "| | | | - Aggiunto esempio predict_proba con tabella probabilità |\n",
    "| | | | - Verifiche stratificazione e convergenza |\n",
    "| | | | **Sezione 5**: Reference card completa con pattern d'uso |\n",
    "| | | | - Tabelle per ogni fase del workflow |\n",
    "| | | | - Glossario 19 termini con esempi |\n",
    "| | | | - 7 errori comuni con sintomi, cause, soluzioni |\n",
    "| | | | **Sezione 6**: Conclusione operativa con take-home messages |\n",
    "| | | | **Sezione 7**: Checklist autovalutazione (15+ items) |\n",
    "| | | | **Sezione 8**: Changelog completo |\n",
    "\n",
    "---\n",
    "\n",
    "## Statistiche del Notebook\n",
    "\n",
    "| Metrica | Valore |\n",
    "|---------|--------|\n",
    "| Sezioni totali | 8 |\n",
    "| Celle Markdown didattiche | 15+ |\n",
    "| Celle Python con codice | 5 |\n",
    "| Micro-checkpoint | 5 |\n",
    "| Termini nel glossario | 19 |\n",
    "| Errori documentati | 7 |\n",
    "| Items nella checklist | 20 |\n",
    "| Formule matematiche | 3 |\n",
    "| Tabelle esplicative | 15+ |\n",
    "\n",
    "---\n",
    "\n",
    "## Note per Future Revisioni\n",
    "\n",
    "- Considerare l'aggiunta di visualizzazioni (confusion matrix, decision boundary)\n",
    "- Espandere la sezione su precision/recall quando si affronta il tema delle metriche\n",
    "- Collegare a esercizi pratici su dataset reali\n",
    "- Aggiungere riferimenti a documentazione ufficiale scikit-learn\n",
    "\n",
    "---\n",
    "\n",
    "**Fine della Lezione 05 - Tassonomia Modelli Lineari**\n",
    "\n",
    "*Prossima lezione: Modelli Lineari Avanzati (Regressione, Ridge, Lasso)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
