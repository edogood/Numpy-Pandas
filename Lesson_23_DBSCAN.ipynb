{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0353990d",
   "metadata": {},
   "source": [
    "# Lezione 23 - DBSCAN: clustering basato su densita\n",
    "\n",
    "## Sezione 1 - Titolo e obiettivi\n",
    "\n",
    "Obiettivo: usare DBSCAN per trovare cluster di forma arbitraria, scegliere eps e min_samples, e gestire outlier (noise).\n",
    "\n",
    "### Cosa impari\n",
    "- Differenze tra clustering basato su densita e su distanza.\n",
    "- Ruolo di eps e min_samples e loro scelta con k-distance graph.\n",
    "- Core/border/noise e come leggerli nei risultati.\n",
    "- Confronto DBSCAN vs K-Means vs Gerarchico.\n",
    "\n",
    "### Perche serve\n",
    "Quando le forme non sono sferiche o ci sono outlier, K-Means e Gerarchico possono fallire; DBSCAN trova cluster per densita e marca il rumore invece di forzare assegnazioni.\n",
    "\n",
    "### Prerequisiti minimi\n",
    "- Conoscere K-Means e clustering gerarchico (lezioni precedenti).\n",
    "- Saper usare StandardScaler e silhouette score.\n",
    "- Nozioni di distanza euclidea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f836ea",
   "metadata": {},
   "source": [
    "## Sezione 2 - Teoria profonda\n",
    "\n",
    "### 1.1 Perche DBSCAN\n",
    "- Non richiede K a priori: scopre automaticamente quanti cluster ci sono.\n",
    "- Trova cluster di forma arbitraria (anche a mezzaluna).\n",
    "- Identifica outlier (label = -1) invece di forzare assegnazioni.\n",
    "- Limite: soffre quando le densita tra cluster sono molto diverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a5088",
   "metadata": {},
   "source": [
    "### 1.2 Parametri chiave: eps e min_samples\n",
    "\n",
    "| Parametro | Significato | Effetto pratico |\n",
    "|-----------|-------------|-----------------|\n",
    "| eps | Raggio del vicinato | eps basso -> piu noise, eps alto -> cluster fusi |\n",
    "| min_samples | Min punti nel vicinato per essere denso | Valore basso sensibile al rumore, alto richiede zone molto dense |\n",
    "\n",
    "Regola empirica: `min_samples >= D + 1` (D = numero di feature). In 2D parti da 4-5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb7cc3",
   "metadata": {},
   "source": [
    "### 1.3 Tipi di punti e regola di costruzione del cluster\n",
    "\n",
    "- Core point: ha almeno min_samples punti entro eps.\n",
    "- Border point: non e core, ma e entro eps da un core.\n",
    "- Noise point: non e core e non e vicino a nessun core (label = -1).\n",
    "\n",
    "Un cluster e l'insieme dei core connessi (entro eps) piu i border collegati a quei core.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc2263",
   "metadata": {},
   "source": [
    "### 1.4 Algoritmo in breve\n",
    "\n",
    "1) Visita un punto non etichettato.\n",
    "2) Trova i vicini entro eps.\n",
    "3) Se i vicini sono meno di min_samples -> noise provvisorio.\n",
    "4) Altrimenti diventa core, crea un cluster, espandi aggiungendo vicini (visitandoli) finche non ci sono nuovi core connessi.\n",
    "5) Alla fine i noise provvisori non collegati restano label -1.\n",
    "\n",
    "Complessita: con strutture di vicinato (KD-tree/Ball-tree) circa O(n log n); senza, O(n^2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5440ad96",
   "metadata": {},
   "source": [
    "### 1.5 Scegliere eps: k-distance graph\n",
    "\n",
    "Procedura:\n",
    "1. Scegli min_samples (almeno D+1).\n",
    "2. Calcola per ogni punto la distanza al k-esimo vicino (k = min_samples).\n",
    "3. Ordina queste distanze e plottale.\n",
    "4. Il gomito del grafico e un buon eps: sotto il gomito cluster compatti, sopra il gomito unisce cluster.\n",
    "\n",
    "Se eps e troppo piccolo: tanti noise e cluster spezzati. Se eps e troppo grande: pochi cluster e rumore assorbito.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656a681",
   "metadata": {},
   "source": [
    "## Sezione 3 - Schema mentale e decision map\n",
    "\n",
    "Workflow DBSCAN (dopo lo scaling):\n",
    "\n",
    "```\n",
    "StandardScaler -> stima min_samples (>= D+1)\n",
    "              -> k-distance graph -> scegli eps al gomito\n",
    "              -> DBSCAN(eps, min_samples)\n",
    "              -> conta cluster (escludendo label -1)\n",
    "              -> valuta: % noise, silhouette (senza noise), forme plausibili\n",
    "              -> tuning: aumenta eps se troppo noise; riduci eps se cluster fusi\n",
    "```\n",
    "\n",
    "Checklist rapida\n",
    "- [ ] Dati scalati.\n",
    "- [ ] min_samples motivato (>= D+1).\n",
    "- [ ] eps stimato da k-distance graph.\n",
    "- [ ] Gestione noise: calcolata percentuale e interpretata.\n",
    "- [ ] Silhouette calcolata escludendo label -1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5402ef3",
   "metadata": {},
   "source": [
    "## Sezione 4 - Notebook dimostrativo\n",
    "\n",
    "### Perche questo passo (Demo 1 - Moons)\n",
    "Mostriamo come DBSCAN separa forme non convessa dove K-Means fallisce. Ci aspettiamo che K-Means tagli le mezzalune, mentre DBSCAN riconosce i due archi e isola gli outlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781566d0",
   "metadata": {},
   "source": [
    "### Checkpoint operativi per le demo\n",
    "- Import completi in ogni demo per eseguire cellule singole.\n",
    "- Assert su shape e assenza di NaN dopo lo scaling.\n",
    "- Silhouette calcolata solo sui punti non noise (label != -1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1 - DBSCAN su moons vs K-Means (forme non convesse)\n",
    "# Intento: mostrare che DBSCAN gestisce forme a mezzaluna mentre K-Means le taglia male.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "np.random.seed(42)\n",
    "X, y_true = make_moons(n_samples=300, noise=0.05)\n",
    "assert X.ndim == 2 and not np.isnan(X).any(), \"Dati malformati\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "assert X_scaled.shape == X.shape\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=30, alpha=0.7)\n",
    "axes[0].set_title('Ground truth')\n",
    "axes[0].set_xlabel('x'); axes[0].set_ylabel('y'); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "labels_km = kmeans.fit_predict(X_scaled)\n",
    "ari_km = adjusted_rand_score(y_true, labels_km)\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=labels_km, cmap='viridis', s=30, alpha=0.7)\n",
    "axes[1].set_title(f'K-Means (ARI={ari_km:.3f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
    "labels_db = dbscan.fit_predict(X_scaled)\n",
    "ari_db = adjusted_rand_score(y_true, labels_db)\n",
    "noise_mask = labels_db == -1\n",
    "colors = labels_db.astype(float)\n",
    "colors[noise_mask] = -0.5\n",
    "axes[2].scatter(X[:, 0], X[:, 1], c=colors, cmap='viridis', s=30, alpha=0.7)\n",
    "if noise_mask.sum() > 0:\n",
    "    axes[2].scatter(X[noise_mask, 0], X[noise_mask, 1], c='red', marker='x', s=50, label='Noise')\n",
    "    axes[2].legend()\n",
    "axes[2].set_title(f'DBSCAN (ARI={ari_db:.3f})')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "n_clusters = len(set(labels_db)) - (1 if -1 in labels_db else 0)\n",
    "print(f\"Cluster trovati da DBSCAN: {n_clusters}, noise: {noise_mask.sum()} punti\")\n",
    "print(\"Nota: silhouette va calcolata escludendo label -1 se serve una metrica di qualita.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208276d9",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 2 - k-distance graph)\n",
    "Usiamo il k-distance graph per stimare eps, includendo outlier. Obiettivo: vedere un gomito chiaro e usare quel valore per DBSCAN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2 - k-distance graph per stimare eps\n",
    "# Intento: usare il gomito del k-distance graph per scegliere eps e applicare DBSCAN.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "np.random.seed(42)\n",
    "X_blobs, _ = make_blobs(n_samples=250, centers=3, cluster_std=0.8, random_state=42)\n",
    "outliers = np.random.uniform(low=-10, high=10, size=(15, 2))\n",
    "X_full = np.vstack([X_blobs, outliers])\n",
    "scaler = StandardScaler(); X_scaled = scaler.fit_transform(X_full)\n",
    "assert not np.isnan(X_scaled).any()\n",
    "\n",
    "min_samples = 5\n",
    "nn = NearestNeighbors(n_neighbors=min_samples)\n",
    "nn.fit(X_scaled)\n",
    "distances, _ = nn.kneighbors(X_scaled)\n",
    "k_dist = np.sort(distances[:, -1])\n",
    "\n",
    "# Knee via seconda derivata\n",
    "second_der = np.diff(np.diff(k_dist))\n",
    "knee_idx = int(np.argmax(second_der) + 2)\n",
    "eps_opt = float(k_dist[knee_idx])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(range(len(k_dist)), k_dist, 'b-', linewidth=2)\n",
    "axes[0].axhline(y=eps_opt, color='red', linestyle='--', label=f'eps ~ {eps_opt:.2f}')\n",
    "axes[0].scatter([knee_idx], [eps_opt], color='red', s=80)\n",
    "axes[0].set_title('k-distance graph (gomito)'); axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "model = DBSCAN(eps=eps_opt, min_samples=min_samples)\n",
    "labels = model.fit_predict(X_scaled)\n",
    "noise_mask = labels == -1\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_clusters + 1))\n",
    "for i in range(n_clusters):\n",
    "    mask = labels == i\n",
    "    axes[1].scatter(X_full[mask, 0], X_full[mask, 1], c=[colors[i]], s=40, alpha=0.7, label=f'Cluster {i}')\n",
    "axes[1].scatter(X_full[noise_mask, 0], X_full[noise_mask, 1], c='red', marker='x', s=60, label='Noise')\n",
    "axes[1].set_title(f'DBSCAN con eps stimato Cluster={n_clusters}, Noise={noise_mask.sum()}'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102abd5",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 3 - Core, border, noise)\n",
    "Visualizziamo come DBSCAN etichetta core, border e noise su un dataset piccolo. Obiettivo: leggere le categorie e la percentuale di rumore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3 - Core, border, noise\n",
    "# Intento: evidenziare i tre tipi di punti prodotti da DBSCAN.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "np.random.seed(123)\n",
    "X_small = np.array([\n",
    "    [1,1],[1.2,0.8],[0.8,1.2],[1.1,1.1],[0.9,0.9],[1.3,1.0],[1.0,1.3],\n",
    "    [4,4],[4.2,3.8],[3.8,4.2],[4.1,4.1],[3.9,3.9],\n",
    "    [8,1],[8.1,1.2],[7.9,0.8],\n",
    "    [6,6]  # outlier isolato\n",
    "])\n",
    "scaler = StandardScaler(); X_scaled = scaler.fit_transform(X_small)\n",
    "\n",
    "model = DBSCAN(eps=0.6, min_samples=3)\n",
    "labels = model.fit_predict(X_scaled)\n",
    "core_mask = np.zeros_like(labels, dtype=bool)\n",
    "core_mask[model.core_sample_indices_] = True\n",
    "noise_mask = labels == -1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.scatter(X_small[core_mask,0], X_small[core_mask,1], c='blue', s=120, edgecolors='black', label='Core')\n",
    "ax.scatter(X_small[~core_mask & ~noise_mask,0], X_small[~core_mask & ~noise_mask,1], c='green', s=80, edgecolors='black', label='Border')\n",
    "ax.scatter(X_small[noise_mask,0], X_small[noise_mask,1], c='red', marker='x', s=120, label='Noise')\n",
    "ax.set_title('Core vs Border vs Noise'); ax.grid(True, alpha=0.3); ax.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(f\"Cluster trovati: {len(set(labels)) - (1 if -1 in labels else 0)}; Noise: {noise_mask.sum()} punti\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20479cb",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 4 - Effetto eps e min_samples)\n",
    "Variare eps e min_samples per capire sensibilita e stabilita dei cluster e del noise. Obiettivo: mostrare come tuning influenza risultati e silhouette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba79e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 4 - Effetto di eps e min_samples\n",
    "# Intento: mostrare come variazioni dei parametri cambiano cluster e noise.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "np.random.seed(42)\n",
    "X_circ, _ = make_circles(n_samples=400, factor=0.5, noise=0.05)\n",
    "X_scaled = StandardScaler().fit_transform(X_circ)\n",
    "assert not np.isnan(X_scaled).any()\n",
    "\n",
    "parametri = [\n",
    "    {'eps':0.15, 'min_samples':3},\n",
    "    {'eps':0.25, 'min_samples':4},\n",
    "    {'eps':0.35, 'min_samples':6}\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "for ax, pars in zip(axes, parametri):\n",
    "    db = DBSCAN(eps=pars['eps'], min_samples=pars['min_samples'])\n",
    "    labels = db.fit_predict(X_scaled)\n",
    "    noise_mask = labels == -1\n",
    "    mask = labels != -1\n",
    "    sil = silhouette_score(X_scaled[mask], labels[mask]) if mask.sum()>1 and len(set(labels[mask]))>1 else -1\n",
    "    ax.scatter(X_circ[mask,0], X_circ[mask,1], c=labels[mask], cmap='viridis', s=20, alpha=0.7)\n",
    "    ax.scatter(X_circ[noise_mask,0], X_circ[noise_mask,1], c='red', marker='x', s=30, label='Noise')\n",
    "    ax.set_title(f\"eps={pars['eps']}, min_samples={pars['min_samples']} Silhouette={sil:.3f}, Noise={noise_mask.sum()}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"Tuning: aumenta eps per ridurre noise; diminuisci eps o aumenta min_samples se cluster si fondono troppo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b673e205",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 5 - Confronto finale)\n",
    "Confrontiamo DBSCAN con K-Means e Agglomerative su forme complesse per scegliere il metodo piu adatto in base a forma, noise e silhouette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073645fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 5 - Confronto DBSCAN vs K-Means vs Agglomerative\n",
    "# Intento: scegliere l'algoritmo in base a forma e noise.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "\n",
    "np.random.seed(42)\n",
    "X1, y1 = make_moons(n_samples=300, noise=0.07)\n",
    "X2, y2 = make_circles(n_samples=200, noise=0.05, factor=0.5)\n",
    "X = np.vstack([X1, X2]); y_true = np.concatenate([y1, y2 + 2])\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "labels_km = kmeans.fit_predict(X_scaled)\n",
    "ari_km = adjusted_rand_score(y_true, labels_km)\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
    "labels_ag = agg.fit_predict(X_scaled)\n",
    "ari_ag = adjusted_rand_score(y_true, labels_ag)\n",
    "\n",
    "db = DBSCAN(eps=0.25, min_samples=5)\n",
    "labels_db = db.fit_predict(X_scaled)\n",
    "ari_db = adjusted_rand_score(y_true, labels_db)\n",
    "noise_mask = labels_db == -1\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,4))\n",
    "axes[0].scatter(X[:,0], X[:,1], c=labels_km, cmap='viridis', s=15, alpha=0.7)\n",
    "axes[0].set_title(f'K-Means (ARI={ari_km:.3f})'); axes[0].grid(True, alpha=0.3)\n",
    "axes[1].scatter(X[:,0], X[:,1], c=labels_ag, cmap='viridis', s=15, alpha=0.7)\n",
    "axes[1].set_title(f'Agglomerative (ARI={ari_ag:.3f})'); axes[1].grid(True, alpha=0.3)\n",
    "colors = labels_db.astype(float); colors[noise_mask] = -0.5\n",
    "axes[2].scatter(X[:,0], X[:,1], c=colors, cmap='viridis', s=15, alpha=0.7)\n",
    "axes[2].set_title(f'DBSCAN (ARI={ari_db:.3f}, noise={noise_mask.sum()})'); axes[2].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"Confronto: DBSCAN gestisce forme e noise; K-Means assume sfericita; Agglomerativo dipende dal linkage scelto.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407de1e1",
   "metadata": {},
   "source": [
    "## Sezione 5 - Esercizi guidati (step by step)\n",
    "\n",
    "### Perche questo esercizio (23.1)\n",
    "Usare DBSCAN per anomaly detection su transazioni, scegliendo eps con k-distance graph e interpretando le anomalie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 23.1 - Anomaly detection con DBSCAN\n",
    "# Intento: individuare transazioni sospette usando k-distance graph e DBSCAN.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "importo = [25,30,28,35,500,22,27,1000,33,29,31,26,750,28,24]\n",
    "durata = [5,6,4,7,2,5,6,1,8,5,6,4,3,5,6]\n",
    "\n",
    "df = pd.DataFrame({'transazione':[f'T{i}' for i in range(1,16)], 'importo':importo, 'durata_min':durata})\n",
    "X = df[['importo','durata_min']].values\n",
    "scaler = StandardScaler(); Xs = scaler.fit_transform(X)\n",
    "assert not np.isnan(Xs).any()\n",
    "\n",
    "min_samples = 3\n",
    "nn = NearestNeighbors(n_neighbors=min_samples)\n",
    "nn.fit(Xs)\n",
    "dist, _ = nn.kneighbors(Xs)\n",
    "k_dist = np.sort(dist[:, -1])\n",
    "second_der = np.diff(np.diff(k_dist))\n",
    "knee = int(np.argmax(second_der) + 2) if len(second_der)>0 else len(k_dist)//2\n",
    "eps_opt = float(k_dist[knee])\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(13,5))\n",
    "axes[0].plot(range(len(k_dist)), k_dist, 'b-o'); axes[0].axhline(y=eps_opt, color='red', linestyle='--', label=f'eps~{eps_opt:.2f}')\n",
    "axes[0].set_title('k-distance graph'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "db = DBSCAN(eps=max(eps_opt,0.6), min_samples=min_samples)\n",
    "labels = db.fit_predict(Xs)\n",
    "df['cluster'] = labels; df['anomalia'] = labels == -1\n",
    "colors = ['red' if a else 'green' for a in df['anomalia']]\n",
    "axes[1].scatter(df['importo'], df['durata_min'], c=colors, s=120, edgecolors='black')\n",
    "for _, row in df.iterrows():\n",
    "    axes[1].annotate(row['transazione'], (row['importo']+10, row['durata_min']+0.1), fontsize=8)\n",
    "axes[1].set_title(f\"DBSCAN eps~{max(eps_opt,0.6):.2f}, noise={df['anomalia'].sum()}\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(df[['transazione','importo','durata_min','cluster','anomalia']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd7b52f",
   "metadata": {},
   "source": [
    "### Perche questo esercizio (23.2)\n",
    "Applicare DBSCAN a coordinate geografiche per individuare zone dense e POI isolati senza imporre K.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba7e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 23.2 - Clustering geografico di POI\n",
    "# Intento: raggruppare punti di interesse vicini e trovare isolati.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "lat = [45.46,45.47,45.46,45.48,45.70,45.71,45.69,45.20,45.46,45.47]\n",
    "lon = [9.18,9.19,9.17,9.18,9.30,9.31,9.29,9.50,9.20,9.18]\n",
    "nomi = ['Duomo','Scala','Castello','Brera','Monza1','Monza2','Monza3','Pavia','Navigli','Porta Romana']\n",
    "\n",
    "df_poi = pd.DataFrame({'nome':nomi,'lat':lat,'lon':lon})\n",
    "X_geo = df_poi[['lat','lon']].values\n",
    "assert X_geo.shape[1]==2\n",
    "\n",
    "min_samples = 2\n",
    "eps_geo = 0.05  # circa 5km in coordinate decimali\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=min_samples)\n",
    "nn.fit(X_geo); dist,_ = nn.kneighbors(X_geo); k_dist=np.sort(dist[:,-1])\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "axes[0].plot(range(len(k_dist)), k_dist, 'b-o'); axes[0].axhline(y=eps_geo, color='red', linestyle='--', label=f'eps={eps_geo}')\n",
    "axes[0].set_title('k-distance graph (geo)'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "db = DBSCAN(eps=eps_geo, min_samples=min_samples)\n",
    "labels = db.fit_predict(X_geo)\n",
    "df_poi['cluster'] = labels\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "colors = plt.cm.Set1(np.linspace(0,1,n_clusters+1))\n",
    "for cid in range(n_clusters):\n",
    "    mask = labels==cid\n",
    "    axes[1].scatter(df_poi.loc[mask,'lon'], df_poi.loc[mask,'lat'], c=[colors[cid]], s=160, edgecolors='black', alpha=0.8, label=f'Zona {cid}')\n",
    "noise_mask = labels==-1\n",
    "if noise_mask.sum()>0:\n",
    "    axes[1].scatter(df_poi.loc[noise_mask,'lon'], df_poi.loc[noise_mask,'lat'], c='gray', s=180, marker='X', edgecolors='black', label='Isolati')\n",
    "for _, row in df_poi.iterrows():\n",
    "    axes[1].annotate(row['nome'], (row['lon']+0.01, row['lat']+0.01), fontsize=9)\n",
    "axes[1].set_title(f'Clustering POI (cluster={n_clusters}, isolati={noise_mask.sum()})'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(df_poi[['nome','lat','lon','cluster']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950de6cd",
   "metadata": {},
   "source": [
    "### Perche questo esercizio (23.3)\n",
    "Implementare un tuning automatico di eps e confrontarlo con scelte manuali, valutando silhouette e noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5860072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 23.3 - Tuning automatico di eps\n",
    "# Intento: stimare eps con gomito e confrontarlo con scelte manuali.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "np.random.seed(42)\n",
    "X, y_true = make_moons(n_samples=300, noise=0.08)\n",
    "Xs = StandardScaler().fit_transform(X)\n",
    "min_samples = 5\n",
    "\n",
    "def trova_eps_ottimale(X, min_samples, plot=False):\n",
    "    nn = NearestNeighbors(n_neighbors=min_samples)\n",
    "    nn.fit(X)\n",
    "    dist,_ = nn.kneighbors(X)\n",
    "    k_dist = np.sort(dist[:,-1])\n",
    "    if len(k_dist)>3:\n",
    "        second_der = np.diff(np.diff(k_dist))\n",
    "        knee = int(np.argmax(second_der) + 2)\n",
    "    else:\n",
    "        knee = len(k_dist)//2\n",
    "    eps = float(k_dist[knee])\n",
    "    if plot:\n",
    "        plt.figure(figsize=(7,4))\n",
    "        plt.plot(range(len(k_dist)), k_dist, 'b-')\n",
    "        plt.axhline(y=eps, color='red', linestyle='--', label=f'eps={eps:.3f}')\n",
    "        plt.scatter([knee],[eps], color='red'); plt.legend(); plt.title('k-distance graph'); plt.grid(True, alpha=0.3); plt.show()\n",
    "    return eps\n",
    "\n",
    "eps_auto = trova_eps_ottimale(Xs, min_samples, plot=True)\n",
    "print(f\"eps ottimale stimato: {eps_auto:.3f}\")\n",
    "\n",
    "eps_values = [0.1, 0.3, 0.5, eps_auto]\n",
    "labels_text = ['0.1', '0.3', '0.5', 'auto']\n",
    "fig, axes = plt.subplots(2,2, figsize=(12,10)); axes = axes.flatten()\n",
    "results = {}\n",
    "for ax, eps, lab in zip(axes, eps_values, labels_text):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(Xs)\n",
    "    mask = labels != -1\n",
    "    sil = silhouette_score(Xs[mask], labels[mask]) if mask.sum()>1 and len(set(labels[mask]))>1 else -1\n",
    "    ari = adjusted_rand_score(y_true, labels)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = (labels==-1).sum()\n",
    "    colors = labels.astype(float); colors[labels==-1] = -0.5\n",
    "    ax.scatter(X[:,0], X[:,1], c=colors, cmap='viridis', s=20, alpha=0.7)\n",
    "    if n_noise>0:\n",
    "        ax.scatter(X[labels==-1,0], X[labels==-1,1], c='red', marker='x', s=30, label='Noise')\n",
    "        ax.legend()\n",
    "    ax.set_title(f\"eps={lab} K={n_clusters}, noise={n_noise}, sil={sil:.3f}, ari={ari:.3f}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    results[lab] = {'eps':eps, 'sil':sil, 'ari':ari, 'noise':n_noise, 'clusters':n_clusters}\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"eps  | clusters | noise | silhouette | ARI\")\n",
    "for lab, r in results.items():\n",
    "    print(f\"{lab:<4} {r['clusters']:>8} {r['noise']:>7} {r['sil']:>11.3f} {r['ari']:>8.3f}\")\n",
    "print(\"Conclusione: il gomito fornisce un buon eps, ma puo richiedere ritocco manuale.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d39baf0",
   "metadata": {},
   "source": [
    "## Sezione 6 - Conclusione operativa\n",
    "\n",
    "### Cosa portarsi a casa\n",
    "- DBSCAN lavora per densita: non richiede K e identifica noise (-1) invece di forzare assegnazioni.\n",
    "- eps si sceglie con il k-distance graph; min_samples tipicamente >= dimensioni + 1.\n",
    "- Calcola silhouette escludendo i noise; valuta anche % noise e forme ottenute.\n",
    "- Se i cluster hanno densita molto diverse, DBSCAN puo fallire: considera alternative o segmenta per zona.\n",
    "\n",
    "### Methods explained (uso, input/output, errori tipici)\n",
    "- `StandardScaler`: standardizza feature (input array n_samples x n_features, output stessa shape); errori: NaN o colonne costanti; sempre prima di distanze.\n",
    "- `NearestNeighbors`: trova vicini per k-distance graph; input X 2D, n_neighbors int; output distanze array; errori: n_neighbors troppo grande rispetto ai campioni.\n",
    "- `DBSCAN`: clustering per densita; input X 2D, eps float, min_samples int; output labels 1D con -1 per noise; errori: eps troppo basso (tutto noise) o troppo alto (cluster fusi).\n",
    "- `silhouette_score`: qualita di separazione; input X 2D, labels 1D; output float; errore: include noise -> filtrare label -1.\n",
    "- `adjusted_rand_score`: confronto con verita terreno; input y_true, y_pred; output [-1,1]; errore: ignorare noise se compari a ground truth pulita.\n",
    "\n",
    "### Common errors and quick debug\n",
    "- Dati non scalati: eps sbilanciato; soluzione: StandardScaler.\n",
    "- Uso di silhouette con noise incluso: valori ingannevoli; soluzione: filtra label -1.\n",
    "- eps scelto a occhio senza k-distance: cluster instabili; soluzione: usa grafico e annota gomito.\n",
    "- min_samples troppo basso: troppi punti core e poco noise; soluzione: aumenta a >= D+1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ebc6ae",
   "metadata": {},
   "source": [
    "## Sezione 7 - End-of-lesson checklist e glossario\n",
    "\n",
    "### Checklist finale\n",
    "- [ ] Dati scalati con `StandardScaler`.\n",
    "- [ ] min_samples scelto (>= dimensioni + 1) e motivato.\n",
    "- [ ] k-distance graph calcolato e eps scelto al gomito.\n",
    "- [ ] DBSCAN eseguito e percentuale di noise valutata.\n",
    "- [ ] Silhouette calcolata escludendo noise.\n",
    "- [ ] Cluster interpretati e, se necessario, confrontati con K-Means/Gerarchico.\n",
    "\n",
    "### Glossario (termini usati)\n",
    "- eps: raggio del vicinato.\n",
    "- min_samples: punti minimi per densita.\n",
    "- Core point: punto con abbastanza vicini entro eps.\n",
    "- Border point: vicino a un core ma non core.\n",
    "- Noise: punti etichettati -1 (fuori da cluster).\n",
    "- k-distance graph: grafico delle distanze al k-esimo vicino per stimare eps.\n",
    "- Silhouette: metrica di separazione tra cluster.\n",
    "- ARI (Adjusted Rand Index): confronto tra clustering e verita a terra.\n",
    "- StandardScaler: standardizzazione feature.\n",
    "- NearestNeighbors: ricerca dei vicini piu prossimi.\n",
    "- DBSCAN: algoritmo di clustering basato su densita.\n",
    "\n",
    "## Sezione 8 - Didactic changelog (max 10 voci)\n",
    "1. Riorganizzate le 8 sezioni con obiettivi, teoria, schema mentale, demo ed esercizi guidati.\n",
    "2. Ripulito testo da emoji e portato a ASCII, aggiungendo rationales pre-demo/esercizi.\n",
    "3. Inseriti Methods explained, errori comuni, checklist e glossario nelle sezioni finali.\n",
    "4. Commentati e rinforzati i code cell con assert su shape/NaN e note su silhouette senza noise.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
