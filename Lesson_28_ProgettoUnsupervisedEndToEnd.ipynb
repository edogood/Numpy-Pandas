{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Titolo e obiettivi\n",
    "\n",
    "Lezione 28: Progetto End-to-End Unsupervised - Segmentazione clienti e Anomaly Detection\n",
    "\n",
    "---\n",
    "\n",
    "## Mappa della lezione\n",
    "\n",
    "| Sezione | Contenuto | Tempo stimato |\n",
    "|---------|-----------|---------------|\n",
    "| 1 | Titolo, obiettivi, contesto business | 5 min |\n",
    "| 2 | Teoria: workflow progetto unsupervised | 10 min |\n",
    "| 3 | Schema mentale: 7 fasi del progetto | 5 min |\n",
    "| 4 | Implementazione: tutte le 7 fasi con codice | 40 min |\n",
    "| 5 | Esercizio riassuntivo | 10 min |\n",
    "| 6 | Conclusione operativa | 10 min |\n",
    "| 7 | Checklist di fine lezione + glossario | 5 min |\n",
    "| 8 | Changelog didattico | 2 min |\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivi della lezione\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "| # | Obiettivo | Verifica |\n",
    "|---|-----------|----------|\n",
    "| 1 | Costruire una **pipeline completa unsupervised** | Sai concatenare tutte le fasi? |\n",
    "| 2 | Applicare **preprocessing robusto** | Sai usare clipping e RobustScaler? |\n",
    "| 3 | **Scegliere k** con metriche multiple | Sai confrontare Silhouette/CH/DB? |\n",
    "| 4 | **Profilare i cluster** per il business | Sai calcolare medie per segmento? |\n",
    "| 5 | **Separare anomalie** prima del clustering | Sai usare IF/LOF come filtro? |\n",
    "| 6 | **Validare** con algoritmi alternativi | Sai usare ARI per confronto? |\n",
    "\n",
    "---\n",
    "\n",
    "## Contesto business: segmentazione clienti\n",
    "\n",
    "```\n",
    "DATI CLIENTI:                         OUTPUT ATTESO:\n",
    "\n",
    "customer_id | recency | frequency     Segmento A: \"Clienti dormienti\"\n",
    "     001    |    120  |      2        Segmento B: \"Clienti fedeli\"\n",
    "     002    |     10  |     45        Segmento C: \"Nuovi attivi\"\n",
    "     003    |     30  |     15        Anomalie: clienti fuori pattern\n",
    "     ...    |    ...  |    ...\n",
    "\n",
    "                   ↓ PIPELINE ↓\n",
    "\n",
    "┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐\n",
    "│  Load   │→ │  Scale  │→ │   PCA   │→ │ Cluster │→ │ Profile │\n",
    "└─────────┘  └─────────┘  └─────────┘  └─────────┘  └─────────┘\n",
    "                                              ↓\n",
    "                                       ┌─────────────┐\n",
    "                                       │   Anomaly   │\n",
    "                                       │  Detection  │\n",
    "                                       └─────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Le 7 fasi del progetto\n",
    "\n",
    "| Fase | Cosa fa | Strumento |\n",
    "|------|---------|-----------|\n",
    "| 1 | **Load & Explore** | pandas, describe, isna |\n",
    "| 2 | **Preprocessing** | clip, RobustScaler |\n",
    "| 3 | **PCA** | PCA, cumsum varianza |\n",
    "| 4 | **Scelta k** | Silhouette, CH, DB |\n",
    "| 5 | **Clustering** | KMeans su PCA |\n",
    "| 6 | **Profiling** | groupby().mean() |\n",
    "| 7 | **Anomaly + Validation** | IF, LOF, Agglomerative, ARI |\n",
    "\n",
    "---\n",
    "\n",
    "## Perché questo workflow\n",
    "\n",
    "| Scelta | Motivazione |\n",
    "|--------|-------------|\n",
    "| **RobustScaler** | Resistente a outlier (usa mediana/IQR) |\n",
    "| **PCA prima di cluster** | Distanze più stabili, clustering più veloce |\n",
    "| **Metriche multiple per k** | Nessuna metrica è perfetta da sola |\n",
    "| **Anomaly dopo segmentazione** | Evita che outlier distorcano i centroidi |\n",
    "| **Validazione con altro algoritmo** | Conferma che la struttura è reale |\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset RFM simulato\n",
    "\n",
    "| Feature | Significato | Range tipico |\n",
    "|---------|-------------|--------------|\n",
    "| recency | Giorni dall'ultimo acquisto | 0-365 |\n",
    "| frequency | Numero acquisti | 1-100 |\n",
    "| monetary | Spesa totale | 10-10000 |\n",
    "| tenure | Mesi da iscrizione | 1-120 |\n",
    "| online_ratio | % acquisti online | 0-1 |\n",
    "\n",
    "**Output atteso:** 3-5 segmenti + 3-5% anomalie isolate.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisiti\n",
    "\n",
    "| Concetto | Dove lo trovi | Verifica |\n",
    "|----------|---------------|----------|\n",
    "| PCA | Lezione 24 | Sai scegliere n_components? |\n",
    "| K-Means | Lezione 20 | Sai usare fit_predict? |\n",
    "| Scelta k | Lezione 21 | Sai interpretare Silhouette? |\n",
    "| IF, LOF | Lezione 26 | Sai settare contamination? |\n",
    "| Feature Engineering | Lezione 27 | Sai creare feature da cluster? |\n",
    "\n",
    "**Tecniche usate:** RobustScaler, PCA, KMeans, AgglomerativeClustering, DBSCAN, IsolationForest, LOF, silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Teoria concettuale\n",
    "- Progetto unsupervised = scoprire struttura nei dati: segmenti e outlier.\n",
    "- Riduzione dimensionale (PCA) rende piu' stabili distanze e velocizza clustering.\n",
    "- Scelta del numero di cluster: confrontare silhouette, Calinski-Harabasz, Davies-Bouldin e il contesto business.\n",
    "- Anomaly detection: separare clienti fuori pattern per non contaminare i segmenti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Schema mentale / mappa decisionale\n",
    "1. Genera o carica i dati e verifica la qualita'.\n",
    "2. Preprocessing: clipping outlier leggero e scaling robusto.\n",
    "3. PCA per compattare l'informazione (80-90% varianza) e rendere stabili le distanze.\n",
    "4. Scelta del numero di cluster con piu' metriche su PCA.\n",
    "5. Clustering principale e calcolo profili.\n",
    "6. Rilevazione anomalie per separare punti sospetti.\n",
    "7. Validazione con algoritmi alternativi e preparazione deliverable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sezione dimostrativa\n",
    "Fasi del progetto implementate di seguito (tutte commentate e con checkpoint):\n",
    "- Fase 2: data loading ed esplorazione.\n",
    "- Fase 3: preprocessing e scaling.\n",
    "- Fase 4: PCA e varianza spiegata.\n",
    "- Fase 5: scelta del numero di cluster.\n",
    "- Fase 6: clustering con KMeans e profili.\n",
    "- Fase 7: anomaly detection.\n",
    "- Fase 8: validazione con altri algoritmi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase 2: Data loading & exploration\n",
    "# Scopo: generare un dataset clienti simulato, controllare NaN e statistiche base.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "plt.close('all')\n",
    "\n",
    "n = 1200\n",
    "recency = np.random.exponential(scale=35, size=n)\n",
    "frequency = np.random.poisson(lam=6, size=n)\n",
    "monetary = np.random.gamma(shape=2.5, scale=80, size=n)\n",
    "avg_basket = monetary / (frequency + 1)\n",
    "tenure = np.random.uniform(3, 60, size=n)\n",
    "online_ratio = np.random.beta(a=2, b=5, size=n)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'recency': recency,\n",
    "    'frequency': frequency,\n",
    "    'monetary': monetary,\n",
    "    'avg_basket': avg_basket,\n",
    "    'tenure': tenure,\n",
    "    'online_ratio': online_ratio\n",
    "})\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "assert not df.isna().any().any(), \"Ci sono NaN nel dataset simulato\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase 3: preprocessing\n",
    "# Scopo: ridurre effetto outlier con clipping e scalare con RobustScaler per robustezza.\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "# Clipping leggero al 1-99 percentile per gestire code lunghe\n",
    "clipped = df.copy()\n",
    "for col in clipped.columns:\n",
    "    lower, upper = np.percentile(clipped[col], [1, 99])\n",
    "    clipped[col] = clipped[col].clip(lower, upper)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(clipped)\n",
    "print(f\"Shape dopo scaling: {X_scaled.shape}\")\n",
    "assert X_scaled.shape[0] == df.shape[0], \"Shape incoerente dopo scaling\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase 4: PCA\n",
    "# Scopo: ridurre dimensionalita' mantenendo ~90% varianza e preparare dati per clustering.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.90, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "var_cum = pca.explained_variance_ratio_.sum()\n",
    "print(f\"Componenti PCA: {X_pca.shape[1]}, Varianza cumulata: {var_cum:.3f}\")\n",
    "assert var_cum >= 0.85, \"Varianza spiegata troppo bassa\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "ax.set_ylabel('Varianza cumulata')\n",
    "ax.set_xlabel('Numero componenti')\n",
    "ax.axhline(0.90, color='gray', linestyle='--')\n",
    "ax.set_title('Scree plot cumulativo PCA')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase 5: scelta del numero di cluster\n",
    "# Scopo: confrontare k=2..6 usando silhouette, Calinski-Harabasz e Davies-Bouldin su PCA.\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "results = []\n",
    "for k in range(2, 7):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_pca)\n",
    "    sil = silhouette_score(X_pca, labels)\n",
    "    ch = calinski_harabasz_score(X_pca, labels)\n",
    "    db = davies_bouldin_score(X_pca, labels)\n",
    "    results.append({'k': k, 'silhouette': sil, 'calinski_harabasz': ch, 'davies_bouldin': db})\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "print(res_df)\n",
    "best_k = res_df.sort_values(by='silhouette', ascending=False).iloc[0]['k']\n",
    "print(f\"k scelto (silhouette): {int(best_k)}\")\n",
    "assert best_k >= 2, \"k non valido\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase 6: clustering con KMeans\n",
    "# Scopo: applicare KMeans con k scelto e creare profili di cluster.\n",
    "best_k = int(best_k)\n",
    "km_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "labels_final = km_final.fit_predict(X_pca)\n",
    "\n",
    "sil_final = silhouette_score(X_pca, labels_final)\n",
    "ch_final = calinski_harabasz_score(X_pca, labels_final)\n",
    "db_final = davies_bouldin_score(X_pca, labels_final)\n",
    "print(f\"Silhouette: {sil_final:.3f}, CH: {ch_final:.1f}, DB: {db_final:.3f}\")\n",
    "\n",
    "# Dataset arricchito\n",
    "proj_df = df.copy()\n",
    "proj_df['cluster'] = labels_final\n",
    "proj_df['pc1'] = X_pca[:,0]\n",
    "proj_df['pc2'] = X_pca[:,1] if X_pca.shape[1] > 1 else 0\n",
    "cluster_profiles = proj_df.groupby('cluster').mean()\n",
    "print(cluster_profiles)\n",
    "assert proj_df['cluster'].nunique() == best_k, \"Numero cluster incoerente\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase 7: anomaly detection\n",
    "# Scopo: identificare clienti atipici per non contaminarli con cluster sbagliati.\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "iso = IsolationForest(contamination=0.03, random_state=42)\n",
    "iso_preds = iso.fit_predict(X_scaled)\n",
    "iso_flag = (iso_preds == -1).astype(int)\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=25, contamination=0.03)\n",
    "lof_preds = lof.fit_predict(X_scaled)\n",
    "lof_flag = (lof_preds == -1).astype(int)\n",
    "\n",
    "anom_flag = ((iso_flag + lof_flag) > 0).astype(int)\n",
    "proj_df['anomaly_flag'] = anom_flag\n",
    "print(proj_df['anomaly_flag'].value_counts())\n",
    "assert proj_df['anomaly_flag'].sum() > 0, \"Nessuna anomalia rilevata\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase 8: validazione con algoritmi alternativi\n",
    "# Scopo: confrontare Agglomerative e DBSCAN come stress test dei cluster.\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=best_k)\n",
    "labels_agg = agg.fit_predict(X_pca)\n",
    "\n",
    "# DBSCAN su PCA 2D\n",
    "dbscan = DBSCAN(eps=1.2, min_samples=8)\n",
    "labels_db = dbscan.fit_predict(X_pca[:, :2])\n",
    "\n",
    "rows = []\n",
    "rows.append({'modello': 'Agglomerative', 'silhouette': silhouette_score(X_pca, labels_agg), 'ari_vs_kmeans': adjusted_rand_score(labels_final, labels_agg)})\n",
    "mask = labels_db != -1\n",
    "if mask.sum() > 0 and np.unique(labels_db[mask]).size > 1:\n",
    "    sil_db = silhouette_score(X_pca[mask], labels_db[mask])\n",
    "else:\n",
    "    sil_db = np.nan\n",
    "rows.append({'modello': 'DBSCAN', 'silhouette': sil_db, 'ari_vs_kmeans': adjusted_rand_score(labels_final, labels_db)})\n",
    "\n",
    "val_df = pd.DataFrame(rows)\n",
    "print(val_df)\n",
    "\n",
    "# Deliverable: dataset arricchito pronto per analisi/report\n",
    "print(proj_df.head())\n",
    "assert proj_df.shape[0] == n, \"Shape finale incoerente\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Esercizi svolti (passo-passo)\n",
    "Esercizio unico: replica la pipeline e verifica ad ogni fase le attese (NaN, varianza, metriche, profili, anomalie). Usa le check-list indicate nei commenti del codice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Conclusione operativa\n",
    "\n",
    "## 5 take-home messages\n",
    "\n",
    "| # | Messaggio | Perché importante |\n",
    "|---|-----------|-------------------|\n",
    "| 1 | **Preprocessing robusto** | RobustScaler + clipping protegge da outlier |\n",
    "| 2 | **PCA stabilizza clustering** | Distanze più significative in bassa D |\n",
    "| 3 | **Metriche multiple per k** | Silhouette + CH + DB + business sense |\n",
    "| 4 | **Profili interpretativi** | Dai nomi ai cluster per stakeholder |\n",
    "| 5 | **Validazione incrociata** | ARI con altro algoritmo conferma struttura |\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow riassuntivo del progetto\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│                    PIPELINE UNSUPERVISED END-TO-END                      │\n",
    "├──────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                          │\n",
    "│   FASE 1: Load          df = pd.read_csv(...) + info() + describe()     │\n",
    "│      ↓                                                                   │\n",
    "│   FASE 2: Preprocess    clip outlier + RobustScaler                      │\n",
    "│      ↓                                                                   │\n",
    "│   FASE 3: PCA           n_components → 90% varianza                      │\n",
    "│      ↓                                                                   │\n",
    "│   FASE 4: Scelta k      Loop k=2..10, confronta Sil/CH/DB               │\n",
    "│      ↓                                                                   │\n",
    "│   FASE 5: Cluster       KMeans(k_best).fit_predict(X_pca)               │\n",
    "│      ↓                                                                   │\n",
    "│   FASE 6: Profile       df.groupby('cluster').mean()                    │\n",
    "│      ↓                                                                   │\n",
    "│   FASE 7: Anomaly       IF/LOF → flag anomalie                          │\n",
    "│      ↓                                                                   │\n",
    "│   FASE 8: Validate      Agglomerative/DBSCAN → ARI vs KMeans            │\n",
    "│                                                                          │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Metodi spiegati: reference card progetto\n",
    "\n",
    "| Metodo | Fase | Input | Output |\n",
    "|--------|------|-------|--------|\n",
    "| `RobustScaler` | 2 | X raw | X scalato (mediana, IQR) |\n",
    "| `PCA(n_components)` | 3 | X scalato | X_pca, varianza spiegata |\n",
    "| `silhouette_score` | 4 | X, labels | score (-1..1) |\n",
    "| `calinski_harabasz_score` | 4 | X, labels | score (alto = buono) |\n",
    "| `davies_bouldin_score` | 4 | X, labels | score (basso = buono) |\n",
    "| `KMeans(k)` | 5 | X_pca | labels, centroids |\n",
    "| `groupby().mean()` | 6 | df con label | profili segmenti |\n",
    "| `IsolationForest` | 7 | X | anomaly labels |\n",
    "| `adjusted_rand_score` | 8 | labels1, labels2 | ARI (-1..1) |\n",
    "\n",
    "---\n",
    "\n",
    "## Errori comuni e debug rapido\n",
    "\n",
    "| Errore | Perché sbagliato | Fix |\n",
    "|--------|-----------------|-----|\n",
    "| Silhouette bassa (<0.2) | k sbagliato o dati non separabili | Prova k diversi, verifica PCA |\n",
    "| Nessuna anomalia | contamination troppo bassa | Aumenta a 0.03-0.05 |\n",
    "| DBSCAN trova 1 solo cluster | eps troppo grande | Usa k-distance plot per eps |\n",
    "| Profili tutti simili | Cluster non discriminanti | Rivedi k o feature |\n",
    "| ARI vicino a 0 | Algoritmi non concordano | Struttura debole, aumenta dati |\n",
    "\n",
    "---\n",
    "\n",
    "## Template deliverable progetto\n",
    "\n",
    "```python\n",
    "# Output finale del progetto:\n",
    "# 1. df_segmented: DataFrame con colonna 'segment' e profili\n",
    "# 2. df_anomalies: DataFrame con anomalie isolate\n",
    "# 3. report_metrics: dict con Silhouette, CH, DB, ARI\n",
    "# 4. fig_profiles: grafico radar o barplot dei profili\n",
    "\n",
    "# Esempio export:\n",
    "df_segmented.to_csv('clienti_segmentati.csv', index=False)\n",
    "df_anomalies.to_csv('clienti_anomali.csv', index=False)\n",
    "\n",
    "import json\n",
    "with open('metriche_progetto.json', 'w') as f:\n",
    "    json.dump(report_metrics, f, indent=2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Prossimi passi\n",
    "\n",
    "| Lezione | Argomento | Collegamento |\n",
    "|---------|-----------|--------------|\n",
    "| 29 | Fondamenti AI | Passaggio a ML/AI concetti |\n",
    "| 30+ | NLP, Deep Learning | Estensioni avanzate |\n",
    "\n",
    "**Fine modulo Unsupervised!** Hai completato clustering, PCA, anomaly detection e progetto end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Checklist di fine lezione\n",
    "- [ ] Ho controllato NaN e outlier prima del clustering.\n",
    "- [ ] Ho scalato e ridotto con PCA verificando la varianza cumulata.\n",
    "- [ ] Ho scelto k confrontando piu' metriche e ho documentato la scelta.\n",
    "- [ ] Ho calcolato profili di cluster interpretabili (recency/frequency/monetary/... ).\n",
    "- [ ] Ho eseguito anomaly detection separando i punti anomali.\n",
    "- [ ] Ho validato con almeno un algoritmo alternativo (Agglomerative/DBSCAN).\n",
    "\n",
    "Glossario\n",
    "- RFM: Recency, Frequency, Monetary, base della segmentazione clienti.\n",
    "- PCA: riduzione dimensionale preservando varianza.\n",
    "- Silhouette: misura coesione/separazione cluster (-1..1).\n",
    "- Calinski-Harabasz / Davies-Bouldin: metriche di qualita' cluster (alto CH, basso DB).\n",
    "- KMeans: clustering a centroidi con k fissato.\n",
    "- Agglomerative: clustering gerarchico bottom-up.\n",
    "- DBSCAN: clustering per densita' con etichetta -1 per rumore.\n",
    "- IsolationForest / LOF: algoritmi di anomaly detection.\n",
    "- Contamination: percentuale attesa di anomalie.\n",
    "- Profilo di cluster: medie/quantili delle feature per ogni cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Changelog didattico\n",
    "\n",
    "| Versione | Data | Modifiche |\n",
    "|----------|------|-----------|\n",
    "| 1.0 | 2024-01-25 | Creazione: pipeline base segmentazione |\n",
    "| 1.1 | 2024-02-05 | Aggiunta anomaly detection e profiling |\n",
    "| 2.0 | 2024-02-12 | Integrata validazione con ARI |\n",
    "| 2.1 | 2024-02-18 | Refactor con controlli e checkpoint |\n",
    "| **2.3** | **2024-12-19** | **ESPANSIONE COMPLETA:** mappa lezione 8 sezioni, tabella obiettivi, ASCII pipeline 7 fasi, contesto business RFM, 5 take-home messages, workflow diagramma completo, reference card metodi per fase, template deliverable export, errori comuni con fix |\n",
    "\n",
    "---\n",
    "\n",
    "## Note per lo studente\n",
    "\n",
    "Questa lezione conclude il **modulo Unsupervised Learning**:\n",
    "\n",
    "| Lesson | Argomento | Stato |\n",
    "|--------|-----------|-------|\n",
    "| 19 | Intro Unsupervised | COMPLETATO |\n",
    "| 20 | K-Means | COMPLETATO |\n",
    "| 21 | Scelta K | COMPLETATO |\n",
    "| 22 | Gerarchico | COMPLETATO |\n",
    "| 23 | DBSCAN | COMPLETATO |\n",
    "| 24 | PCA | COMPLETATO |\n",
    "| 25 | PCA + Clustering | COMPLETATO |\n",
    "| 26 | Anomaly Detection | COMPLETATO |\n",
    "| 27 | Feature Engineering | COMPLETATO |\n",
    "| **28** | **Progetto End-to-End** | **COMPLETATO** |\n",
    "\n",
    "**Hai ora le competenze per:**\n",
    "- Segmentare dataset senza label\n",
    "- Scegliere algoritmi e iperparametri con criterio\n",
    "- Validare e consegnare risultati al business\n",
    "\n",
    "**Prossimo modulo:** AI, NLP, Deep Learning (Lessons 29+)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
