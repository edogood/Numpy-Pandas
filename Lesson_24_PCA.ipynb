{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7b49ed",
   "metadata": {},
   "source": [
    "# Lezione 24 - PCA: Principal Component Analysis\n",
    "\n",
    "## Sezione 1 - Titolo e obiettivi\n",
    "\n",
    "---\n",
    "\n",
    "## Mappa della lezione\n",
    "\n",
    "| Sezione | Contenuto | Tempo stimato |\n",
    "|---------|-----------|---------------|\n",
    "| 1 | Titolo, obiettivi, quando usare PCA | 5 min |\n",
    "| 2 | Teoria profonda: varianza, autovettori, loadings | 25 min |\n",
    "| 3 | Schema mentale: workflow PCA | 5 min |\n",
    "| 4 | Demo: Iris, scree plot, interpretazione | 20 min |\n",
    "| 5 | Esercizi risolti + errori comuni | 15 min |\n",
    "| 6 | Conclusione operativa | 10 min |\n",
    "| 7 | Checklist di fine lezione + glossario | 5 min |\n",
    "| 8 | Changelog didattico | 2 min |\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivi della lezione\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "| # | Obiettivo | Verifica |\n",
    "|---|-----------|----------|\n",
    "| 1 | Comprendere cosa sono le **componenti principali** | Sai perché PC1 ha più varianza di PC2? |\n",
    "| 2 | Scegliere **n_components** con criteri oggettivi | Sai usare scree plot e soglia 95%? |\n",
    "| 3 | Interpretare i **loadings** | Sai collegare PC1 alle feature originali? |\n",
    "| 4 | Usare PCA per **visualizzazione** e **preprocessing** | Sai inserirla in una pipeline? |\n",
    "| 5 | Riconoscere quando PCA è **inappropriata** | Sai che non funziona con categoriche? |\n",
    "\n",
    "---\n",
    "\n",
    "## L'idea centrale di PCA\n",
    "\n",
    "```\n",
    "DATI ORIGINALI (D dimensioni):       DOPO PCA (k << D):\n",
    "\n",
    "    ●                                      ●\n",
    "   ● ●                                    ●●\n",
    "  ●   ●  ← correlate                     ●●● ← combinazione lineare\n",
    " ●     ●                                ●●●●\n",
    "●       ●                              ●●●●●\n",
    "\n",
    "D feature correlate              →    k componenti indipendenti\n",
    "Ridondanza                       →    Informazione compressa\n",
    "Difficile da visualizzare        →    2D/3D plot possibile\n",
    "```\n",
    "\n",
    "**Formula intuitiva:**\n",
    "$$PC_i = w_{i1} \\cdot X_1 + w_{i2} \\cdot X_2 + ... + w_{iD} \\cdot X_D$$\n",
    "\n",
    "Dove i pesi $w$ (loadings) sono scelti per massimizzare la varianza.\n",
    "\n",
    "---\n",
    "\n",
    "## Quando usare PCA: tabella decisionale\n",
    "\n",
    "| Situazione | Usa PCA? | Motivo |\n",
    "|------------|----------|--------|\n",
    "| Molte feature correlate (>10-20) | ✅ Sì | Compressione efficace |\n",
    "| Visualizzare dati HD | ✅ Sì | Riduzione a 2D/3D |\n",
    "| Preprocessing per ML | ✅ Sì | Riduce overfitting, velocizza |\n",
    "| Poche feature già interpretabili | ❌ No | Perdi interpretabilità |\n",
    "| Dati categorici | ❌ No | PCA richiede numerici continui |\n",
    "| Feature non correlate | ⚠️ Poco utile | Poca compressione possibile |\n",
    "\n",
    "---\n",
    "\n",
    "## I 4 passi di PCA (concettualmente)\n",
    "\n",
    "```\n",
    "1. CENTRA i dati (media = 0)\n",
    "   X_centered = X - mean(X)\n",
    "\n",
    "2. CALCOLA matrice di covarianza\n",
    "   Cov = X_centered.T @ X_centered / (n-1)\n",
    "\n",
    "3. ESTRAI autovalori/autovettori\n",
    "   λ₁ > λ₂ > ... > λ_D    (varianze)\n",
    "   v₁, v₂, ..., v_D       (direzioni)\n",
    "\n",
    "4. PROIETTA sui primi k autovettori\n",
    "   X_pca = X_centered @ [v₁, v₂, ..., v_k]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisiti minimi\n",
    "\n",
    "| Concetto | Dove lo trovi | Verifica |\n",
    "|----------|---------------|----------|\n",
    "| StandardScaler | Lezione 13, 20 | Sai perché è OBBLIGATORIO prima di PCA? |\n",
    "| Matrice di covarianza | Statistica base | Conosci la correlazione tra feature? |\n",
    "| Pipeline sklearn | Lezione 13 | Sai evitare data leakage? |\n",
    "\n",
    "**Micro-checkpoint prerequisiti:**\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "print(\"OK!\" if hasattr(PCA(), 'explained_variance_ratio_') else \"Rivedi sklearn\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061eae91",
   "metadata": {},
   "source": [
    "## Sezione 2 - Teoria profonda\n",
    "\n",
    "### 1.1 Perche ridurre la dimensionalita\n",
    "- Curse of dimensionality: le distanze perdono significato quando le dimensioni crescono.\n",
    "- Overfitting: troppe feature rispetto ai campioni.\n",
    "- Visualizzazione: impossibile plottare >3 dimensioni senza riduzione.\n",
    "\n",
    "### 1.2 Cosa fa PCA (passi)\n",
    "1. Centra i dati (media zero).\n",
    "2. Calcola la matrice di covarianza.\n",
    "3. Estrae autovalori/autovettori (varianza e direzioni).\n",
    "4. Proietta i dati sulle prime k direzioni (componenti principali).\n",
    "\n",
    "### 1.3 Varianza spiegata e scelta di k\n",
    "- Varianza spiegata = quota di informazione catturata da una PC.\n",
    "- Criteri: soglia cumulativa (es. 90-95%), gomito nello scree plot, Kaiser (autovalore>1 su dati standardizzati).\n",
    "\n",
    "### 1.4 Loadings e interpretazione\n",
    "- Loadings = pesi delle feature nella PC (pca.components_).\n",
    "- Loadings alti in modulo indicano feature che guidano quella componente.\n",
    "- Segno positivo/negativo indica associazione direzionale.\n",
    "\n",
    "### 1.5 Quando usare / evitare PCA\n",
    "- Usa: molte feature correlate, visualizzazione HD, riduzione rumore, velocizzare training.\n",
    "- Evita: poche feature gia interpretabili, dati categorici, quando serve interpretabilita diretta sulle feature originali.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be81c1",
   "metadata": {},
   "source": [
    "## Sezione 3 - Schema mentale e decision map\n",
    "\n",
    "Workflow sintetico dopo scaling:\n",
    "\n",
    "```\n",
    "Dati numerici -> StandardScaler -> PCA() esplorativo\n",
    "            -> Scree plot / soglia varianza -> scegli k o n_components=0.95\n",
    "            -> PCA finale -> varianza spiegata + loadings\n",
    "            -> Usa X_pca per visualizzare o addestrare modelli\n",
    "```\n",
    "\n",
    "Checklist rapida\n",
    "- [ ] Dati scalati, nessun NaN.\n",
    "- [ ] Obiettivo chiaro (visual, compressione, noise reduction, preprocessing).\n",
    "- [ ] Varianza cumulativa controllata e scelta di k motivata.\n",
    "- [ ] Loadings letti per interpretare le componenti.\n",
    "- [ ] Valutato l'impatto su prestazioni (accuracy, tempo) se usata prima di modelli.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13de423",
   "metadata": {},
   "source": [
    "## Sezione 4 - Notebook dimostrativo\n",
    "\n",
    "### Perche questo passo (Demo 1 - PCA Iris)\n",
    "Mostrare come PCA compatta 4 feature in 2 PC mantenendo quasi tutta la varianza e separando le classi nel piano PC1-PC2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9c8ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.4 Interpretare i Loadings\n",
    "\n",
    "I **loadings** ti dicono come ogni feature originale contribuisce alla PC:\n",
    "\n",
    "```\n",
    "Esempio: PC1 per dati di clienti\n",
    "\n",
    "Feature          Loading\n",
    "─────────────────────────\n",
    "età              +0.45    ← contribuisce molto (positivo)\n",
    "reddito          +0.52    ← contribuisce molto (positivo)\n",
    "spesa_mensile    +0.48    ← contribuisce molto (positivo)\n",
    "n_figli          -0.10    ← contribuisce poco\n",
    "\n",
    "Interpretazione: PC1 cattura il \"potere d'acquisto\"\n",
    "(età + reddito + spesa vanno insieme)\n",
    "```\n",
    "\n",
    "### ️ Importante: Scaling!\n",
    "\n",
    "**PCA è sensibile alla scala!** Feature con valori grandi dominano.\n",
    "\n",
    "```\n",
    "SBAGLIATO:                    CORRETTO:\n",
    "età: 20-80                    StandardScaler prima di PCA!\n",
    "reddito: 20000-200000  ← domina tutto\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5afed5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.5 Quando Usare PCA\n",
    "\n",
    "|  Usa PCA quando... |  Evita PCA quando... |\n",
    "|----------------------|------------------------|\n",
    "| Hai troppe feature (>20) | Le feature sono già poche |\n",
    "| Vuoi visualizzare dati HD | Ogni feature è importante singolarmente |\n",
    "| C'è multicollinearità | I dati non sono correlati |\n",
    "| Vuoi ridurre overfitting | Hai bisogno di interpretabilità diretta |\n",
    "| Come preprocessing per ML | I dati sono categorici |\n",
    "\n",
    "###  PCA nel Pipeline ML\n",
    "\n",
    "```\n",
    "Dati ─→ StandardScaler ─→ PCA ─→ Modello ML\n",
    "         (necessario!)    ↓\n",
    "                      n_components=0.95\n",
    "                      (mantieni 95% varianza)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371870c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  2. Schema Mentale\n",
    "\n",
    "### Workflow PCA\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                          PCA WORKFLOW                               │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  1. PREPARAZIONE                                                    │\n",
    "│     └── StandardScaler() — OBBLIGATORIO per PCA!                    │\n",
    "│     └── Verifica: dati numerici, no NaN                             │\n",
    "│                                                                     │\n",
    "│  2. PCA ESPLORATIVO (tutte le componenti)                           │\n",
    "│     └── pca = PCA()  # senza n_components                           │\n",
    "│     └── pca.fit(X_scaled)                                           │\n",
    "│     └── Analizza: explained_variance_ratio_                         │\n",
    "│                                                                     │\n",
    "│  3. SCEGLI n_components                                             │\n",
    "│     └── Plot varianza cumulativa                                    │\n",
    "│     └── Soglia: 90-95% varianza                                     │\n",
    "│     └── Oppure: PCA(n_components=0.95) automatico                   │\n",
    "│                                                                     │\n",
    "│  4. FIT FINALE                                                      │\n",
    "│     └── pca = PCA(n_components=k)                                   │\n",
    "│     └── X_pca = pca.fit_transform(X_scaled)                         │\n",
    "│                                                                     │\n",
    "│  5. INTERPRETAZIONE                                                 │\n",
    "│     └── pca.components_ → loadings                                  │\n",
    "│     └── Quali feature contribuiscono a ogni PC?                     │\n",
    "│                                                                     │\n",
    "│  6. USO                                                             │\n",
    "│     └── Visualizzazione (2D/3D)                                     │\n",
    "│     └── Input per clustering o classificazione                      │\n",
    "│     └── Riduzione rumore                                            │\n",
    "│                                                                     │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "###  Checklist Pre-PCA\n",
    "\n",
    "```\n",
    "□ Dati scalati con StandardScaler?\n",
    "□ Nessun valore mancante?\n",
    "□ Solo feature numeriche?\n",
    "□ Hai un obiettivo chiaro? (visualizzazione, preprocessing, noise reduction)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6f519",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  3. Demo Pratiche\n",
    "\n",
    "### Demo 1 — Primo PCA: Dataset Iris\n",
    "\n",
    "Visualizziamo il classico dataset Iris (4 feature) in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb402ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1 - PCA sul dataset Iris (da 4 feature a 2 PC)\n",
    "# Intento: mostrare quanta varianza resta con 2 componenti e separazione classi.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "assert X_iris.ndim == 2 and not np.isnan(X_iris).any(), \"Dati Iris malformati\"\n",
    "\n",
    "# Scaling obbligatorio\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_iris)\n",
    "assert X_scaled.shape == X_iris.shape\n",
    "\n",
    "# PCA esplorativa\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# PCA 2 componenti per visualizzazione\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Varianza spiegata da 2 PC: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, name in enumerate(iris.target_names):\n",
    "    mask = y_iris == i\n",
    "    axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[i], label=name, s=50, alpha=0.7, edgecolors='black')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "axes[0].set_title('Iris in 2D con PCA')\n",
    "axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].bar(range(1, 5), pca_full.explained_variance_ratio_, alpha=0.7, color='steelblue', label='Varianza singola')\n",
    "axes[1].plot(range(1, 5), np.cumsum(pca_full.explained_variance_ratio_), 'ro-', label='Cumulativa')\n",
    "axes[1].axhline(y=0.95, color='red', linestyle='--', alpha=0.6, label='95%')\n",
    "axes[1].set_xlabel('Componente'); axes[1].set_ylabel('Varianza spiegata'); axes[1].set_title('Scree plot')\n",
    "axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8456e27",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 2 - Interpretare i loadings)\n",
    "Collegare PC1/PC2 alle feature originali per capire cosa rappresentano le direzioni principali.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2 - Interpretare i loadings\n",
    "# Intento: collegare PC1/PC2 alle feature originali tramite loadings e biplot.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert 'pca' in globals() and 'X_pca' in globals(), \"Esegui prima Demo 1\"\n",
    "\n",
    "loadings = pca.components_\n",
    "df_load = pd.DataFrame(loadings, columns=feature_names, index=['PC1','PC2'])\n",
    "print(\"Loadings PC1/PC2:\")\n",
    "print(df_load.round(3))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
    "im = axes[0].imshow(loadings, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[0].set_xticks(range(len(feature_names)))\n",
    "axes[0].set_xticklabels([f.replace(' (cm)','') for f in feature_names], rotation=45, ha='right')\n",
    "axes[0].set_yticks([0,1]); axes[0].set_yticklabels(['PC1','PC2'])\n",
    "axes[0].set_title('Heatmap loadings')\n",
    "for i in range(loadings.shape[0]):\n",
    "    for j in range(loadings.shape[1]):\n",
    "        axes[0].text(j, i, f\"{loadings[i,j]:.2f}\", ha='center', va='center', color='white' if abs(loadings[i,j])>0.5 else 'black')\n",
    "plt.colorbar(im, ax=axes[0], label='Loading')\n",
    "\n",
    "ax = axes[1]\n",
    "colors = ['red','green','blue']\n",
    "for i, name in enumerate(iris.target_names):\n",
    "    mask = y_iris == i\n",
    "    ax.scatter(X_pca[mask,0], X_pca[mask,1], c=colors[i], s=30, alpha=0.5, label=name)\n",
    "scale=3\n",
    "for i, feat in enumerate(feature_names):\n",
    "    ax.arrow(0,0, loadings[0,i]*scale, loadings[1,i]*scale, head_width=0.08, head_length=0.08, fc='black', ec='black')\n",
    "    ax.text(loadings[0,i]*scale*1.15, loadings[1,i]*scale*1.15, feat.replace(' (cm)',''), fontsize=9, ha='center')\n",
    "ax.set_title('Biplot PC1-PC2 + loadings'); ax.grid(True, alpha=0.3); ax.axhline(0,color='gray',linewidth=0.5); ax.axvline(0,color='gray',linewidth=0.5); ax.legend(loc='lower left')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29acb2f",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 3 - Scelta automatica di n_components)\n",
    "Usare soglia di varianza e scree plot su un dataset piu grande (digits) per decidere quante componenti tenere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d629b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3 - Scelta automatica di n_components (digits)\n",
    "# Intento: usare soglie di varianza e scree plot per decidere quante componenti tenere.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X_digits = digits.data; y_digits = digits.target\n",
    "assert X_digits.ndim == 2 and not np.isnan(X_digits).any()\n",
    "\n",
    "scaler_digits = StandardScaler()\n",
    "X_digits_scaled = scaler_digits.fit_transform(X_digits)\n",
    "assert X_digits_scaled.shape == X_digits.shape\n",
    "\n",
    "soglie = [0.80, 0.90, 0.95, 0.99]\n",
    "for soglia in soglie:\n",
    "    p = PCA(n_components=soglia)\n",
    "    p.fit(X_digits_scaled)\n",
    "    print(f\"Varianza {soglia*100:.0f}% -> {p.n_components_} componenti\")\n",
    "\n",
    "pca_full_digits = PCA()\n",
    "pca_full_digits.fit(X_digits_scaled)\n",
    "cumsum = np.cumsum(pca_full_digits.explained_variance_ratio_)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "axes[0].plot(range(1,65), pca_full_digits.explained_variance_ratio_, 'b-o', markersize=3)\n",
    "axes[0].set_title('Scree plot'); axes[0].set_xlabel('Componente'); axes[0].set_ylabel('Varianza spiegata'); axes[0].grid(True, alpha=0.3)\n",
    "axes[1].plot(range(1,65), cumsum, 'g-o', markersize=3)\n",
    "axes[1].axhline(y=0.90, color='orange', linestyle='--', label='90%'); axes[1].axhline(y=0.95, color='red', linestyle='--', label='95%')\n",
    "axes[1].set_title('Varianza cumulativa'); axes[1].set_xlabel('Componente'); axes[1].set_ylabel('Varianza cumulativa'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703fc78",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 4 - Visualizzare cifre in PCA 2D)\n",
    "Vedere come le cifre si distribuiscono in 2D e quanto si separano le classi con sole due componenti.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b33230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 4 - Distribuzione delle cifre in PCA 2D\n",
    "# Intento: visualizzare digits in 2D e osservare separazione fra classi.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "assert 'X_digits_scaled' in globals(), \"Esegui Demo 3 prima\"\n",
    "\n",
    "pca2 = PCA(n_components=2)\n",
    "X_digits_2d = pca2.fit_transform(X_digits_scaled)\n",
    "assert X_digits_2d.shape[1] == 2\n",
    "\n",
    "fig, axes = plt.subplots(2,5, figsize=(15,6))\n",
    "axes = axes.flatten()\n",
    "for digit in range(10):\n",
    "    ax = axes[digit]\n",
    "    mask = y_digits == digit\n",
    "    ax.scatter(X_digits_2d[mask,0], X_digits_2d[mask,1], c='blue', s=10, alpha=0.7)\n",
    "    ax.set_title(f'Cifra {digit}'); ax.set_xticks([]); ax.set_yticks([]); ax.grid(True, alpha=0.2)\n",
    "plt.suptitle('Digits in PCA 2D'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188906e1",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 5 - PCA per denoising)\n",
    "Usare poche componenti per ricostruire immagini rumorose, osservando il trade-off tra dettaglio e riduzione del rumore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 5 - PCA per riduzione rumore\n",
    "# Intento: ricostruire immagini rumorose con poche componenti per filtrare il rumore.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "assert 'digits' in globals() and 'X_digits_scaled' in globals(), \"Esegui Demo 3 prima\"\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = [0,100,200,300,400]\n",
    "X_sample = digits.images[indices].copy()\n",
    "noise_level = 5\n",
    "X_noisy = X_sample + np.random.normal(0, noise_level, X_sample.shape)\n",
    "X_noisy = np.clip(X_noisy, 0, 16)\n",
    "X_noisy_flat = X_noisy.reshape(len(indices), -1)\n",
    "\n",
    "n_components_list = [5,10,20,40]\n",
    "fig, axes = plt.subplots(len(indices), len(n_components_list)+2, figsize=(16,10))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[i,0].imshow(X_sample[i], cmap='gray'); axes[i,0].set_title('Originale' if i==0 else ''); axes[i,0].axis('off')\n",
    "    axes[i,1].imshow(X_noisy[i], cmap='gray'); axes[i,1].set_title(f'+Rumore' if i==0 else ''); axes[i,1].axis('off')\n",
    "    for j, n_comp in enumerate(n_components_list):\n",
    "        pca_dn = PCA(n_components=n_comp)\n",
    "        pca_dn.fit(X_digits_scaled)\n",
    "        x_noisy_scaled = (X_noisy_flat[i] - scaler_digits.mean_) / scaler_digits.scale_\n",
    "        x_pca = pca_dn.transform(x_noisy_scaled.reshape(1,-1))\n",
    "        x_rec = pca_dn.inverse_transform(x_pca)\n",
    "        x_rec = x_rec * scaler_digits.scale_ + scaler_digits.mean_\n",
    "        axes[i, j+2].imshow(x_rec.reshape(8,8), cmap='gray')\n",
    "        if i==0:\n",
    "            var = pca_dn.explained_variance_ratio_.sum()*100\n",
    "            axes[i, j+2].set_title(f'n={n_comp}({var:.0f}%)')\n",
    "        axes[i, j+2].axis('off')\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[i,0].set_ylabel(f'Cifra {y_digits[idx]}')\n",
    "plt.suptitle('PCA per denoising'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44147d87",
   "metadata": {},
   "source": [
    "## Sezione 5 - Esercizi guidati (step by step)\n",
    "\n",
    "### Perche questo esercizio (24.1)\n",
    "Applicare il flusso completo PCA sul dataset Wine: scelta di k, visualizzazione e interpretazione loadings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633263de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 24.1 - PCA su dataset Wine\n",
    "# Intento: scegliere componenti per 80/95% varianza, visualizzare e leggere i loadings.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "wine = load_wine()\n",
    "X_wine = wine.data; y_wine = wine.target; feature_names_wine = wine.feature_names\n",
    "assert not np.isnan(X_wine).any()\n",
    "\n",
    "scaler_wine = StandardScaler(); X_wine_scaled = scaler_wine.fit_transform(X_wine)\n",
    "assert X_wine_scaled.shape == X_wine.shape\n",
    "\n",
    "pca_wine = PCA(); pca_wine.fit(X_wine_scaled)\n",
    "cumsum_var = np.cumsum(pca_wine.explained_variance_ratio_)\n",
    "n_80 = int(np.argmax(cumsum_var>=0.80)+1)\n",
    "n_95 = int(np.argmax(cumsum_var>=0.95)+1)\n",
    "print(f\"Componenti per 80%: {n_80}, per 95%: {n_95}\")\n",
    "\n",
    "pca_2d = PCA(n_components=2); X_wine_2d = pca_2d.fit_transform(X_wine_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(16,5))\n",
    "axes[0].bar(range(1,len(pca_wine.explained_variance_ratio_)+1), pca_wine.explained_variance_ratio_, color='steelblue', alpha=0.7)\n",
    "axes[0].plot(range(1,len(cumsum_var)+1), cumsum_var, 'ro-', label='Cumulativa')\n",
    "axes[0].axhline(y=0.80, color='orange', linestyle='--'); axes[0].axhline(y=0.95, color='red', linestyle='--')\n",
    "axes[0].set_title('Scree plot'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "colors = ['red','green','blue']\n",
    "for i,name in enumerate(wine.target_names):\n",
    "    mask = y_wine == i\n",
    "    axes[1].scatter(X_wine_2d[mask,0], X_wine_2d[mask,1], c=colors[i], s=50, alpha=0.7, edgecolors='black', label=name)\n",
    "axes[1].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "axes[1].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)'); axes[1].set_title('Wine in 2D'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "loadings = pca_2d.components_\n",
    "axes[2].barh(range(len(feature_names_wine)), loadings[0], alpha=0.7, label='PC1')\n",
    "axes[2].barh(range(len(feature_names_wine)), loadings[1], alpha=0.7, label='PC2', left=loadings[0])\n",
    "axes[2].set_yticks(range(len(feature_names_wine))); axes[2].set_yticklabels(feature_names_wine, fontsize=8)\n",
    "axes[2].set_title('Loadings PC1/PC2'); axes[2].legend(); axes[2].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Top feature per interpretazione\n",
    "import pandas as pd\n",
    "df_load = pd.DataFrame({'feature':feature_names_wine, 'PC1':loadings[0], 'PC2':loadings[1]})\n",
    "print(df_load.reindex(df_load['PC1'].abs().sort_values(ascending=False).index).head(3))\n",
    "print(df_load.reindex(df_load['PC2'].abs().sort_values(ascending=False).index).head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb30d6e2",
   "metadata": {},
   "source": [
    "### Perche questo esercizio (24.2)\n",
    "Valutare l'impatto di PCA sul training di un classificatore confrontando tempo e accuracy prima/dopo la riduzione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfbe818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 24.2 - PCA come preprocessing per classificazione (Digits)\n",
    "# Intento: confrontare tempo e accuracy con/ senza PCA al 90% di varianza.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Dati\n",
    "if 'digits' not in globals():\n",
    "    digits = load_digits()\n",
    "X = digits.data; y = digits.target\n",
    "assert not np.isnan(X).any()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler_cls = StandardScaler(); X_train_s = scaler_cls.fit_transform(X_train); X_test_s = scaler_cls.transform(X_test)\n",
    "\n",
    "# Senza PCA\n",
    "start = time.time(); lr_no = LogisticRegression(max_iter=5000, random_state=42)\n",
    "lr_no.fit(X_train_s, y_train); t_no = time.time()-start; acc_no = lr_no.score(X_test_s, y_test)\n",
    "\n",
    "# Con PCA (90%)\n",
    "pca_cls = PCA(n_components=0.90)\n",
    "X_train_pca = pca_cls.fit_transform(X_train_s); X_test_pca = pca_cls.transform(X_test_s)\n",
    "start = time.time(); lr_pca = LogisticRegression(max_iter=5000, random_state=42)\n",
    "lr_pca.fit(X_train_pca, y_train); t_pca = time.time()-start; acc_pca = lr_pca.score(X_test_pca, y_test)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Metrica':['Feature','Tempo (s)','Accuracy (%)'],\n",
    "    'Senza PCA':[X_train_s.shape[1], f\"{t_no:.4f}\", f\"{acc_no*100:.2f}\"],\n",
    "    'Con PCA':[pca_cls.n_components_, f\"{t_pca:.4f}\", f\"{acc_pca*100:.2f}\"]\n",
    "})\n",
    "print(comparison.to_string(index=False))\n",
    "feature_reduction = (1 - pca_cls.n_components_/X_train_s.shape[1]) * 100\n",
    "print(f\"Riduzione feature: {feature_reduction:.1f}% | Speedup ~{(t_no/t_pca if t_pca>0 else 1):.2f}x | Delta accuracy {((acc_pca-acc_no)*100):+.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc54f7",
   "metadata": {},
   "source": [
    "### Perche questo esercizio (24.3)\n",
    "Confrontare separabilita nello spazio originale vs PCA calcolando distanze tra centroidi e visualizzando le classi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3063998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 24.3 - Confronto separabilita spazio originale vs PCA (Iris)\n",
    "# Intento: misurare quanto PCA aumenta la distanza tra classi rispetto a due feature originali.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "iris = load_iris(); X = iris.data; y = iris.target; target_names = iris.target_names\n",
    "assert not np.isnan(X).any()\n",
    "\n",
    "X_orig = X[:, :2]  # sepal length, sepal width\n",
    "scaler_ir = StandardScaler(); X_scaled = scaler_ir.fit_transform(X)\n",
    "pca_ir = PCA(n_components=2); X_pca = pca_ir.fit_transform(X_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,6))\n",
    "colors=['red','green','blue']; markers=['o','s','^']\n",
    "for i,name in enumerate(target_names):\n",
    "    mask = y==i\n",
    "    axes[0].scatter(X_orig[mask,0], X_orig[mask,1], c=colors[i], marker=markers[i], s=70, edgecolors='black', alpha=0.7, label=name)\n",
    "axes[0].set_title('Spazio originale (2 feature)'); axes[0].set_xlabel('Sepal length'); axes[0].set_ylabel('Sepal width'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "for i,name in enumerate(target_names):\n",
    "    mask = y==i\n",
    "    axes[1].scatter(X_pca[mask,0], X_pca[mask,1], c=colors[i], marker=markers[i], s=70, edgecolors='black', alpha=0.7, label=name)\n",
    "axes[1].set_title(f'Spazio PCA (varianza {pca_ir.explained_variance_ratio_.sum()*100:.1f}%)'); axes[1].set_xlabel('PC1'); axes[1].set_ylabel('PC2'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "def dist_centroidi(Xmat, yvec):\n",
    "    centroids=[]\n",
    "    for c in np.unique(yvec):\n",
    "        centroids.append(Xmat[yvec==c].mean(axis=0))\n",
    "    centroids=np.array(centroids)\n",
    "    d=cdist(centroids, centroids)\n",
    "    vals=d[np.triu_indices_from(d, k=1)]\n",
    "    return vals.mean()\n",
    "\n",
    "dist_orig = dist_centroidi(X_orig, y)\n",
    "dist_pca = dist_centroidi(X_pca, y)\n",
    "print(f\"Distanza media centroidi - originale: {dist_orig:.3f} | PCA: {dist_pca:.3f} | Delta: {(dist_pca-dist_orig)/dist_orig*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a7946",
   "metadata": {},
   "source": [
    "## Sezione 6 - Conclusione operativa\n",
    "\n",
    "---\n",
    "\n",
    "## I 5 Take-Home Messages\n",
    "\n",
    "### 1. Scaling è OBBLIGATORIO prima di PCA\n",
    "```python\n",
    "# SBAGLIATO: PCA su dati non scalati\n",
    "pca = PCA().fit(X)  # Feature con range grande dominano!\n",
    "\n",
    "# CORRETTO: Sempre StandardScaler prima\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "pca = PCA().fit(X_scaled)\n",
    "```\n",
    "\n",
    "### 2. Scegli n_components con criterio oggettivo\n",
    "\n",
    "| Metodo | Come funziona | Quando usarlo |\n",
    "|--------|---------------|---------------|\n",
    "| **Soglia varianza** | `n_components=0.95` → mantieni 95% | Default consigliato |\n",
    "| **Scree plot** | Cerca il \"gomito\" nel grafico | Quando vuoi visualizzare |\n",
    "| **Kaiser** | Tieni PC con autovalore > 1 | Dopo StandardScaler |\n",
    "| **Fisso** | `n_components=2` | Solo per visualizzazione |\n",
    "\n",
    "```python\n",
    "# Template: soglia varianza automatica\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(f\"PC usate: {pca.n_components_}, Varianza: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "```\n",
    "\n",
    "### 3. I loadings rivelano il significato delle PC\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Loadings come DataFrame\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    index=feature_names,\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)]\n",
    ")\n",
    "\n",
    "# Feature più importanti per PC1\n",
    "loadings['PC1'].abs().sort_values(ascending=False).head()\n",
    "```\n",
    "\n",
    "### 4. PCA per visualizzazione: sempre 2D o 3D\n",
    "```python\n",
    "# Template: scatter plot PCA\n",
    "pca_vis = PCA(n_components=2)\n",
    "X_2d = pca_vis.fit_transform(X_scaled)\n",
    "\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel(f'PC1 ({pca_vis.explained_variance_ratio_[0]:.1%})')\n",
    "plt.ylabel(f'PC2 ({pca_vis.explained_variance_ratio_[1]:.1%})')\n",
    "plt.title('PCA Visualization')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 5. PCA in pipeline ML: evita data leakage\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# CORRETTO: PCA dentro la pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)  # Fit solo su train!\n",
    "score = pipe.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Template completo PCA\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. PREPROCESSING\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. PCA ESPLORATIVO (tutte le componenti)\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# 3. SCREE PLOT\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(pca_full.explained_variance_ratio_)+1), \n",
    "        pca_full.explained_variance_ratio_)\n",
    "plt.xlabel('PC'); plt.ylabel('Varianza spiegata')\n",
    "plt.title('Scree Plot')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(pca_full.explained_variance_ratio_)+1),\n",
    "         np.cumsum(pca_full.explained_variance_ratio_), 'bo-')\n",
    "plt.axhline(0.95, color='r', linestyle='--', label='95%')\n",
    "plt.xlabel('Numero PC'); plt.ylabel('Varianza cumulativa')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. PCA FINALE\n",
    "n_comp = np.argmax(np.cumsum(pca_full.explained_variance_ratio_) >= 0.95) + 1\n",
    "pca = PCA(n_components=n_comp)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(f\"Da {X.shape[1]} a {X_pca.shape[1]} dimensioni ({pca.explained_variance_ratio_.sum():.1%} varianza)\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Prossimi passi\n",
    "→ **Lezione 25**: PCA + Clustering - combinare riduzione dimensionale e segmentazione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891fa629",
   "metadata": {},
   "source": [
    "## Sezione 7 - End-of-lesson checklist e glossario\n",
    "\n",
    "### Checklist finale\n",
    "- [ ] Dati numerici e scalati con `StandardScaler`.\n",
    "- [ ] Varianza spiegata cumulativa valutata (soglia 90-95%).\n",
    "- [ ] Numero di componenti scelto e motivato (scree plot/gomito o n_components=valore).\n",
    "- [ ] Loadings letti per interpretare le PC.\n",
    "- [ ] Se usata in pipeline ML: verificato impatto su tempo e accuracy.\n",
    "- [ ] Silhouette/metriche calcolate nel nuovo spazio se usato per clustering.\n",
    "\n",
    "### Glossario (termini usati)\n",
    "- Componente Principale (PC): nuova direzione di massima varianza.\n",
    "- Varianza spiegata: quota di varianza catturata da una PC.\n",
    "- Scree plot: grafico autovalori/varianza per PC.\n",
    "- Loadings: pesi delle feature nella PC.\n",
    "- Kaiser criterion: tieni PC con autovalore>1 (dopo scaling).\n",
    "- Denoising: riduzione rumore ricostruendo con poche PC.\n",
    "- n_components: parametro PCA per fissare numero o soglia di varianza.\n",
    "- StandardScaler: standardizza feature a media 0 e dev. std 1.\n",
    "- Curse of dimensionality: difficolta dovute ad alto numero di feature.\n",
    "- Eigenvalue/eigenvector: autovalore/autovettore di covarianza (varianza e direzione).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sezione 8 - Didactic changelog\n",
    "\n",
    "| Versione | Data | Modifiche |\n",
    "|----------|------|-----------|\n",
    "| 1.0 | Originale | Versione iniziale del notebook |\n",
    "| 2.0 | 2026-01-XX | Riorganizzata nelle 8 sezioni obbligatorie |\n",
    "| 2.1 | 2026-01-XX | Aggiunte rationale e checkpoint a demo e esercizi |\n",
    "| 2.2 | 2026-01-XX | Inserite Methods explained, Common errors, Glossario |\n",
    "| 2.3 | 2026-01-XX | **Espansione didattica completa**: mappa lezione con tempi; ASCII visualization compressione; formula PC intuitiva; tabella quando usare PCA; 4 passi concettuali; 5 take-home messages; template scree plot; template loadings DataFrame; pipeline ML corretta; template completo. |\n",
    "\n",
    "---\n",
    "\n",
    "## Note di rilascio v2.3\n",
    "\n",
    "### Contenuti aggiunti\n",
    "- **Header**: mappa temporale, ASCII compressione, formula intuitiva, tabella decisionale, 4 passi\n",
    "- **Conclusione**: 5 principi, template scree plot, loadings, pipeline, codice completo\n",
    "\n",
    "### Miglioramenti pedagogici\n",
    "- Obbligo scaling enfatizzato con esempio sbagliato/corretto\n",
    "- Tabella metodi scelta n_components con contesto d'uso\n",
    "- Template loadings come DataFrame per interpretazione\n",
    "- Pipeline sklearn per evitare data leakage\n",
    "\n",
    "### Competenze verificabili\n",
    "Dopo questa lezione lo studente può:\n",
    "1. Applicare PCA con scaling corretto\n",
    "2. Scegliere n_components con criterio oggettivo\n",
    "3. Interpretare loadings e nominare le PC\n",
    "4. Inserire PCA in pipeline ML senza leakage\n",
    "\n",
    "---\n",
    "\n",
    "**Fine della lezione**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
