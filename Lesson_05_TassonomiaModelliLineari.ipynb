{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "002ae5a4",
      "metadata": {},
      "source": [
        "# Lezione 5 — Introduzione a Scikit-Learn: Modelli Lineari e Logistic Regression\n",
        "\n",
        "---\n",
        "\n",
        "## Obiettivi della Lezione\n",
        "\n",
        "Al termine di questa lezione sarai in grado di:\n",
        "\n",
        "1. Comprendere le **quattro famiglie di modelli** in Scikit-Learn\n",
        "2. Capire la **matematica dei modelli lineari** (combinazione lineare, sigmoide, logit)\n",
        "3. Applicare la **pipeline standard ML**: split, scaling, fit, predict, score\n",
        "4. Implementare una **Logistic Regression** completa\n",
        "5. Interpretare i **coefficienti** del modello\n",
        "6. Scegliere il modello appropriato per diversi problemi\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisiti\n",
        "\n",
        "- Lezione 4: GroupBy e Transform (Pandas avanzato)\n",
        "- Conoscenza base di NumPy e Pandas\n",
        "- Concetti matematici: funzioni, esponenziale, logaritmo\n",
        "\n",
        "---\n",
        "\n",
        "## Indice\n",
        "\n",
        "1. **SEZIONE 1** — Teoria Concettuale Approfondita\n",
        "2. **SEZIONE 2** — Schema Mentale / Mappa Decisionale\n",
        "3. **SEZIONE 3** — Notebook Dimostrativo\n",
        "4. **SEZIONE 4** — Metodi Spiegati\n",
        "5. **SEZIONE 5** — Glossario\n",
        "6. **SEZIONE 6** — Errori Comuni e Debug Rapido\n",
        "7. **SEZIONE 7** — Conclusione Operativa\n",
        "8. **SEZIONE 8** — Checklist di Fine Lezione\n",
        "9. **SEZIONE 9** — Changelog Didattico\n",
        "\n",
        "---\n",
        "\n",
        "## Librerie Utilizzate\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "473f78dd",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SEZIONE 1 — Teoria Concettuale Approfondita\n",
        "\n",
        "## 1.1 Glossario Preliminare\n",
        "\n",
        "Prima di leggere qualsiasi formula, fissiamo un glossario chiaro:\n",
        "\n",
        "| Termine | Definizione |\n",
        "|---------|-------------|\n",
        "| **Feature** | Variabile di input che descrive un'osservazione (es. eta, reddito) |\n",
        "| **Target** | Variabile da predire (output del modello) |\n",
        "| **Modello lineare** | Combina le feature con pesi e bias per produrre un output |\n",
        "| **Logit** | Trasformazione $\\log \\frac{p}{1-p}$ che converte probabilita in quantita illimitata |\n",
        "| **Sigmoid** | Funzione $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ che schiaccia valori tra 0 e 1 |\n",
        "| **Odds** | Rapporto tra probabilita di successo e insuccesso: $\\frac{p}{1-p}$ |\n",
        "| **Regularizzazione** | Penalita sui pesi per ridurre overfitting |\n",
        "\n",
        "## 1.2 Le Quattro Famiglie di Modelli\n",
        "\n",
        "Scikit-Learn offre molti modelli, ma pensarli per **famiglie concettuali** aiuta a scegliere:\n",
        "\n",
        "### Famiglia 1: Modelli Lineari\n",
        "- **Esempi**: `LinearRegression`, `LogisticRegression`, `Ridge`, `Lasso`\n",
        "- **Idea**: stimano una combinazione lineare delle feature\n",
        "- **Formula**: $\\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b$\n",
        "- **Pro**: interpretabili, veloci, ottime baseline\n",
        "- **Contro**: assumono relazioni lineari\n",
        "\n",
        "### Famiglia 2: Tree-Based (Alberi)\n",
        "- **Esempi**: `DecisionTreeClassifier`, `RandomForestClassifier`, `GradientBoostingClassifier`\n",
        "- **Idea**: suddivisione dello spazio tramite soglie\n",
        "- **Pro**: catturano non-linearita, robusti\n",
        "- **Contro**: rischio overfitting, meno interpretabili\n",
        "\n",
        "### Famiglia 3: Distanza / Vicinanza (KNN)\n",
        "- **Esempi**: `KNeighborsClassifier`, `KNeighborsRegressor`\n",
        "- **Idea**: classificano in base ai vicini piu simili\n",
        "- **Pro**: nessuna assunzione forte, intuitivi\n",
        "- **Contro**: lenti in predizione, sensibili allo scaling\n",
        "\n",
        "### Famiglia 4: Probabilistici / Bayesiani\n",
        "- **Esempi**: `GaussianNB`, `MultinomialNB`\n",
        "- **Idea**: applicano il teorema di Bayes\n",
        "- **Pro**: velocissimi, ottimi su dati testuali\n",
        "- **Contro**: assumono indipendenza tra feature\n",
        "\n",
        "## 1.3 Matematica della Logistic Regression\n",
        "\n",
        "### Step 1: Combinazione Lineare (Logit)\n",
        "$$z = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b = \\mathbf{w}^T \\mathbf{x} + b$$\n",
        "\n",
        "### Step 2: Funzione Sigmoide\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "La sigmoide trasforma z (che va da $-\\infty$ a $+\\infty$) in una probabilita (tra 0 e 1).\n",
        "\n",
        "### Probabilita Stimata\n",
        "$$\\hat{p} = P(y=1 \\mid x) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)$$\n",
        "\n",
        "### Interpretazione dei Coefficienti\n",
        "- $w_j > 0$: aumentando la feature j, aumenta la probabilita della classe 1\n",
        "- $w_j < 0$: aumentando la feature j, diminuisce la probabilita\n",
        "- $|w_j|$ grande: la feature ha forte impatto\n",
        "\n",
        "## 1.4 Perche Standardizzare?\n",
        "\n",
        "**StandardScaler** applica:\n",
        "$$x' = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "**Benefici:**\n",
        "- Stabilita numerica\n",
        "- Convergenza piu rapida\n",
        "- Coefficienti comparabili tra feature\n",
        "- **Necessario** per modelli lineari, KNN, SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa8a6352",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SEZIONE 2 — Schema Mentale / Mappa Decisionale\n",
        "\n",
        "## Pipeline Standard ML\n",
        "\n",
        "```\n",
        "[Dati Grezzi]\n",
        "      |\n",
        "      v\n",
        "[Feature Engineering]  ← Crea/trasforma variabili\n",
        "      |\n",
        "      v\n",
        "[Train/Test Split]     ← Dividi per validazione\n",
        "      |\n",
        "      v\n",
        "[Preprocessing]        ← Scaling, encoding\n",
        "      |\n",
        "      v\n",
        "[Fit del Modello]      ← Apprendimento\n",
        "      |\n",
        "      v\n",
        "[Predict]              ← Previsioni su test\n",
        "      |\n",
        "      v\n",
        "[Score/Valutazione]    ← Metriche\n",
        "      |\n",
        "      v\n",
        "[Iterazione]           ← Migliora se necessario\n",
        "```\n",
        "\n",
        "## Come Scegliere un Modello?\n",
        "\n",
        "```\n",
        "TIPO DI PROBLEMA?\n",
        "       |\n",
        "   ┌───┴───┐\n",
        "   v       v\n",
        "CLASSIF. REGRESS.\n",
        "   |       |\n",
        "   v       v\n",
        "Quante classi?  Target continuo?\n",
        "   |              |\n",
        "   v              v\n",
        "BINARIA → LogisticRegression, RandomForest\n",
        "MULTI   → Stessi + SoftMax\n",
        "CONTINUO → LinearRegression, RandomForest\n",
        "```\n",
        "\n",
        "## Euristiche di Scelta Rapida\n",
        "\n",
        "| Situazione | Modello Consigliato |\n",
        "|------------|---------------------|\n",
        "| Dati quasi lineari | Logistic/Linear Regression |\n",
        "| Non-linearita sospette | RandomForest, GradientBoosting |\n",
        "| Dataset enorme | GradientBoosting, SGDClassifier |\n",
        "| Testo (NLP classico) | Naive Bayes + TF-IDF |\n",
        "| Dataset piccolo | KNN |\n",
        "| Serve interpretabilita | Logistic Regression, Decision Tree |\n",
        "\n",
        "## Checklist Pre-Modellazione\n",
        "\n",
        "1. Ho definito chiaramente il target?\n",
        "2. Ho separato train e test PRIMA di qualsiasi preprocessing?\n",
        "3. Ho standardizzato le feature (per modelli lineari)?\n",
        "4. Ho verificato il bilanciamento delle classi?\n",
        "5. Ho scelto le metriche appropriate?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9040ed34",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SEZIONE 3 — Notebook Dimostrativo\n",
        "\n",
        "## 3.1 Setup e Creazione Dataset Sintetico\n",
        "\n",
        "**Perche questo passaggio:** Usiamo `make_classification` per creare un dataset controllato. Questo ci permette di capire il flusso senza preoccuparci della qualita dei dati reali.\n",
        "\n",
        "**Parametri chiave:**\n",
        "- `n_samples`: numero di osservazioni\n",
        "- `n_features`: numero di feature\n",
        "- `n_informative`: feature effettivamente utili\n",
        "- `class_sep`: separabilita delle classi (piu alto = piu facile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f70d8bc2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape X: (600, 6)\n",
            "Positive class ratio: 0.50\n"
          ]
        }
      ],
      "source": [
        "# === SETUP: Import librerie e creazione dataset sintetico ===\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Creiamo un dataset sintetico per classificazione binaria\n",
        "X, y = make_classification(\n",
        "    n_samples=600,       # 600 osservazioni\n",
        "    n_features=6,        # 6 feature\n",
        "    n_informative=4,     # 4 feature informative\n",
        "    n_redundant=0,       # 0 feature ridondanti\n",
        "    class_sep=1.5,       # Buona separabilita\n",
        "    random_state=42      # Riproducibilita\n",
        ")\n",
        "\n",
        "# --- MICRO-CHECKPOINT ---\n",
        "print(f\"Shape X: {X.shape}\")\n",
        "print(f\"Shape y: {y.shape}\")\n",
        "print(f\"Classi uniche: {np.unique(y)}\")\n",
        "print(f\"Bilanciamento classi: {y.mean():.2f} (0.5 = perfettamente bilanciato)\")\n",
        "\n",
        "assert X.shape == (600, 6), \"Shape X non corretta!\"\n",
        "assert y.shape == (600,), \"Shape y non corretta!\"\n",
        "print(\"\\n--- Micro-checkpoint: dataset creato correttamente ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "687f94e3",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3.2 Train/Test Split\n",
        "\n",
        "**Perche questo passaggio:** Dividiamo i dati in training (75%) e test (25%). Il test set serve per valutare il modello su dati MAI visti durante l'addestramento.\n",
        "\n",
        "**Parametri chiave:**\n",
        "- `test_size=0.25`: 25% per il test\n",
        "- `stratify=y`: mantiene le proporzioni delle classi\n",
        "- `random_state=42`: riproducibilita"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb4fb90d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(450, 6) (150, 6)\n"
          ]
        }
      ],
      "source": [
        "# === TRAIN/TEST SPLIT ===\n",
        "\n",
        "# Dividiamo i dati: 75% training, 25% test\n",
        "# stratify=y mantiene le proporzioni delle classi in entrambi i set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.25, \n",
        "    stratify=y, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# --- MICRO-CHECKPOINT ---\n",
        "print(f\"Training set: {X_train.shape[0]} campioni\")\n",
        "print(f\"Test set: {X_test.shape[0]} campioni\")\n",
        "print(f\"Proporzione classe 1 in train: {y_train.mean():.2f}\")\n",
        "print(f\"Proporzione classe 1 in test: {y_test.mean():.2f}\")\n",
        "\n",
        "# Verifica stratificazione\n",
        "assert abs(y_train.mean() - y_test.mean()) < 0.05, \"Stratificazione non corretta!\"\n",
        "print(\"\\n--- Micro-checkpoint: split stratificato corretto ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd4c9674",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3.3 Standardizzazione (Scaling)\n",
        "\n",
        "**Perche questo passaggio:** I modelli lineari sono sensibili alla scala delle feature. Standardizziamo per avere media=0 e std=1.\n",
        "\n",
        "**IMPORTANTE:** \n",
        "- Fit dello scaler SOLO su training set\n",
        "- Transform su ENTRAMBI train e test\n",
        "- Mai fare fit sul test set (data leakage!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc16d71",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 7.35214359e-17 -4.49455288e-16  3.50337043e-17  2.91125149e-17\n",
            "  5.21188031e-16 -1.48029737e-18]\n"
          ]
        }
      ],
      "source": [
        "# === STANDARDIZZAZIONE ===\n",
        "\n",
        "# Creiamo lo scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit_transform su training: calcola media/std E trasforma\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# SOLO transform su test: usa media/std del training\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- MICRO-CHECKPOINT ---\n",
        "print(\"Medie delle feature dopo scaling (training):\")\n",
        "print(f\"  {X_train_scaled.mean(axis=0).round(10)}\")\n",
        "print(\"\\nDeviazioni standard dopo scaling (training):\")\n",
        "print(f\"  {X_train_scaled.std(axis=0).round(2)}\")\n",
        "\n",
        "# Dopo standardizzazione, media deve essere ~0 e std ~1\n",
        "assert np.allclose(X_train_scaled.mean(axis=0), 0, atol=1e-10), \"Media non zero!\"\n",
        "assert np.allclose(X_train_scaled.std(axis=0), 1, atol=0.01), \"Std non 1!\"\n",
        "print(\"\\n--- Micro-checkpoint: scaling corretto (media=0, std=1) ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c31f405a",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3.4 Fit del Modello (Addestramento)\n",
        "\n",
        "**Perche questo passaggio:** Addestriamo la Logistic Regression sui dati di training. Il modello impara i coefficienti ottimali per classificare.\n",
        "\n",
        "**Interpretazione dei coefficienti:**\n",
        "- Coefficiente positivo → feature aumenta la probabilita della classe 1\n",
        "- Coefficiente negativo → feature diminuisce la probabilita\n",
        "- Coefficiente vicino a 0 → feature poco influente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a8283d8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "0",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "bda666bd-a8c6-490b-ab67-e75d201e499f",
              "rows": [
                [
                  "feature_1",
                  "1.1586538968827325"
                ],
                [
                  "feature_4",
                  "0.7823028257283188"
                ],
                [
                  "feature_5",
                  "0.598549870239413"
                ],
                [
                  "feature_2",
                  "0.09697855791010884"
                ],
                [
                  "feature_3",
                  "0.06530627582463851"
                ],
                [
                  "feature_0",
                  "-0.8244110452495775"
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 6
              }
            },
            "text/plain": [
              "feature_1    1.158654\n",
              "feature_4    0.782303\n",
              "feature_5    0.598550\n",
              "feature_2    0.096979\n",
              "feature_3    0.065306\n",
              "feature_0   -0.824411\n",
              "dtype: float64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === FIT DEL MODELLO ===\n",
        "\n",
        "# Creiamo e addestriamo la Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)  # max_iter per garantire convergenza\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Estraiamo i coefficienti\n",
        "coeff = pd.Series(\n",
        "    model.coef_.ravel(), \n",
        "    index=[f\"feature_{i}\" for i in range(6)]\n",
        ")\n",
        "\n",
        "# --- MICRO-CHECKPOINT ---\n",
        "print(\"Coefficienti del modello (ordinati per importanza):\")\n",
        "print(coeff.sort_values(ascending=False))\n",
        "print(f\"\\nIntercetta (bias): {model.intercept_[0]:.4f}\")\n",
        "\n",
        "# Verifica che il modello sia stato addestrato\n",
        "assert hasattr(model, 'coef_'), \"Modello non addestrato!\"\n",
        "print(\"\\n--- Micro-checkpoint: modello addestrato correttamente ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a46f5aa",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3.5 Predict e Valutazione\n",
        "\n",
        "**Perche questo passaggio:** Usiamo il modello addestrato per fare previsioni sul test set e valutiamo la performance con l'accuracy.\n",
        "\n",
        "**Accuracy:** proporzione di previsioni corrette\n",
        "$$\\text{Accuracy} = \\frac{\\text{Previsioni Corrette}}{\\text{Totale Previsioni}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de83218f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.727\n"
          ]
        }
      ],
      "source": [
        "# === PREDICT E VALUTAZIONE ===\n",
        "\n",
        "# Previsioni sul test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calcoliamo l'accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# --- MICRO-CHECKPOINT ---\n",
        "print(f\"Previsioni effettuate: {len(y_pred)}\")\n",
        "print(f\"Distribuzione previsioni: classe 0 = {(y_pred == 0).sum()}, classe 1 = {(y_pred == 1).sum()}\")\n",
        "print(f\"\\nAccuracy sul test set: {acc:.3f} ({acc*100:.1f}%)\")\n",
        "\n",
        "# Verifica che l'accuracy sia ragionevole (sopra il caso random 50%)\n",
        "assert acc > 0.5, \"Accuracy troppo bassa!\"\n",
        "print(\"\\n--- Micro-checkpoint: accuracy > 50% (meglio del caso) ---\")\n",
        "\n",
        "# Bonus: mostriamo anche le probabilita\n",
        "y_proba = model.predict_proba(X_test_scaled)\n",
        "print(f\"\\nEsempio probabilita (prime 5 osservazioni):\")\n",
        "print(f\"  P(classe=0)  P(classe=1)\")\n",
        "for i in range(5):\n",
        "    print(f\"  {y_proba[i, 0]:.3f}        {y_proba[i, 1]:.3f}  → Predetto: {y_pred[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddb33e95",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SEZIONE 4 — Metodi Spiegati\n",
        "\n",
        "## Creazione Dataset\n",
        "\n",
        "| Metodo | Sintassi | Cosa Fa |\n",
        "|--------|----------|---------|\n",
        "| `make_classification()` | `make_classification(n_samples, n_features, ...)` | Genera dataset sintetico |\n",
        "\n",
        "## Train/Test Split\n",
        "\n",
        "| Metodo | Sintassi | Cosa Fa |\n",
        "|--------|----------|---------|\n",
        "| `train_test_split()` | `train_test_split(X, y, test_size=0.25)` | Divide dati in train e test |\n",
        "| Parametro `stratify` | `stratify=y` | Mantiene proporzioni classi |\n",
        "| Parametro `random_state` | `random_state=42` | Riproducibilita |\n",
        "\n",
        "## Preprocessing\n",
        "\n",
        "| Metodo | Sintassi | Cosa Fa |\n",
        "|--------|----------|---------|\n",
        "| `StandardScaler()` | `scaler = StandardScaler()` | Crea oggetto scaler |\n",
        "| `.fit_transform()` | `scaler.fit_transform(X_train)` | Calcola parametri E trasforma |\n",
        "| `.transform()` | `scaler.transform(X_test)` | Solo trasforma (usa parametri esistenti) |\n",
        "\n",
        "## Modello\n",
        "\n",
        "| Metodo | Sintassi | Cosa Fa |\n",
        "|--------|----------|---------|\n",
        "| `LogisticRegression()` | `model = LogisticRegression()` | Crea modello |\n",
        "| `.fit()` | `model.fit(X_train, y_train)` | Addestra il modello |\n",
        "| `.predict()` | `model.predict(X_test)` | Predice le classi |\n",
        "| `.predict_proba()` | `model.predict_proba(X_test)` | Predice probabilita |\n",
        "| `.coef_` | `model.coef_` | Coefficienti appresi |\n",
        "| `.intercept_` | `model.intercept_` | Intercetta (bias) |\n",
        "\n",
        "## Valutazione\n",
        "\n",
        "| Metodo | Sintassi | Cosa Fa |\n",
        "|--------|----------|---------|\n",
        "| `accuracy_score()` | `accuracy_score(y_true, y_pred)` | Proporzione corrette |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6240f41a",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SEZIONE 5 — Glossario\n",
        "\n",
        "| Termine | Definizione |\n",
        "|---------|-------------|\n",
        "| **Feature** | Variabile di input (colonna X) usata per la previsione |\n",
        "| **Target** | Variabile da predire (y) |\n",
        "| **Training Set** | Dati usati per addestrare il modello |\n",
        "| **Test Set** | Dati usati per valutare il modello (mai visti in training) |\n",
        "| **Fit** | Processo di addestramento del modello |\n",
        "| **Predict** | Generazione di previsioni dal modello addestrato |\n",
        "| **Standardizzazione** | Trasformazione per avere media=0 e std=1 |\n",
        "| **Data Leakage** | Uso improprio di informazioni del test nel training |\n",
        "| **Accuracy** | Proporzione di previsioni corrette |\n",
        "| **Coefficiente** | Peso assegnato a ogni feature dal modello |\n",
        "| **Intercetta (Bias)** | Termine costante del modello |\n",
        "| **Sigmoide** | Funzione che trasforma valori in probabilita [0,1] |\n",
        "| **Logit** | Logaritmo degli odds, output lineare prima della sigmoide |\n",
        "| **Stratificazione** | Mantenere le proporzioni delle classi nel split |\n",
        "| **Overfitting** | Modello troppo adattato ai dati di training |\n",
        "| **Regularizzazione** | Tecnica per prevenire overfitting |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e5e89b2",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SEZIONE 6 — Errori Comuni e Debug Rapido\n",
        "\n",
        "## Errore 1: Fit dello Scaler sul Test Set (Data Leakage!)\n",
        "\n",
        "```python\n",
        "# SBAGLIATO - data leakage!\n",
        "scaler.fit_transform(X_test)  # NO! Usa info del test\n",
        "\n",
        "# CORRETTO\n",
        "scaler.fit_transform(X_train)  # Fit solo su train\n",
        "scaler.transform(X_test)       # Solo transform su test\n",
        "```\n",
        "\n",
        "## Errore 2: Dimenticare di Scalare il Test Set\n",
        "\n",
        "```python\n",
        "# SBAGLIATO - test non scalato!\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred = model.predict(X_test)  # X_test non scalato!\n",
        "\n",
        "# CORRETTO\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "```\n",
        "\n",
        "## Errore 3: Confondere `.predict()` e `.predict_proba()`\n",
        "\n",
        "```python\n",
        "# .predict() restituisce le CLASSI (0 o 1)\n",
        "y_pred = model.predict(X_test)  # [0, 1, 1, 0, ...]\n",
        "\n",
        "# .predict_proba() restituisce le PROBABILITA\n",
        "y_proba = model.predict_proba(X_test)  # [[0.3, 0.7], [0.8, 0.2], ...]\n",
        "```\n",
        "\n",
        "## Errore 4: Non Impostare `random_state`\n",
        "\n",
        "```python\n",
        "# SBAGLIATO - risultati non riproducibili\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# CORRETTO - sempre riproducibile\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "```\n",
        "\n",
        "## Errore 5: Split Prima del Preprocessing? NO!\n",
        "\n",
        "```python\n",
        "# SBAGLIATO - ordine errato\n",
        "X_scaled = scaler.fit_transform(X)  # Scala TUTTO prima\n",
        "X_train, X_test = train_test_split(X_scaled)  # Poi split\n",
        "\n",
        "# CORRETTO - split PRIMA, poi scala\n",
        "X_train, X_test = train_test_split(X)  # Prima split\n",
        "X_train_scaled = scaler.fit_transform(X_train)  # Poi scala train\n",
        "X_test_scaled = scaler.transform(X_test)  # Poi scala test\n",
        "```\n",
        "\n",
        "## Errore 6: Ignorare ConvergenceWarning\n",
        "\n",
        "```python\n",
        "# WARNING: ConvergenceWarning: lbfgs failed to converge\n",
        "\n",
        "# SOLUZIONE: aumenta max_iter\n",
        "model = LogisticRegression(max_iter=1000)  # Default e 100\n",
        "```\n",
        "\n",
        "## Errore 7: Usare Accuracy su Classi Sbilanciate\n",
        "\n",
        "```python\n",
        "# Se 95% dei dati e classe 0, accuracy=0.95 e ingannevole!\n",
        "# Usa altre metriche:\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cb6d0e0",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SEZIONE 7 — Conclusione Operativa\n",
        "\n",
        "## Cosa Hai Imparato\n",
        "\n",
        "In questa lezione hai acquisito le basi del Machine Learning con Scikit-Learn:\n",
        "\n",
        "1. **Quattro famiglie di modelli**: lineari, tree-based, distanza, bayesiani\n",
        "2. **Pipeline ML standard**: split → scale → fit → predict → score\n",
        "3. **Logistic Regression**: modello lineare per classificazione binaria\n",
        "4. **Standardizzazione**: necessaria per modelli lineari\n",
        "5. **Interpretazione coefficienti**: segno e magnitudo\n",
        "6. **Metriche**: accuracy come prima metrica\n",
        "\n",
        "## Pattern Ricorrente: Pipeline di Classificazione\n",
        "\n",
        "```python\n",
        "# 1. Split dei dati\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
        "\n",
        "# 2. Scaling (fit solo su train!)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Addestramento\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 4. Previsione e valutazione\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "```\n",
        "\n",
        "## Collegamento alla Prossima Lezione\n",
        "\n",
        "Nella Lezione 6 approfondiremo:\n",
        "- Altre metriche (precision, recall, F1)\n",
        "- Confusion matrix\n",
        "- Modelli tree-based"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbd411f3",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SEZIONE 8 — Checklist di Fine Lezione\n",
        "\n",
        "Prima di procedere alla prossima lezione, verifica di saper fare tutto quanto segue:\n",
        "\n",
        "- [ ] Conosco le quattro famiglie di modelli (lineari, tree, distanza, bayesiani)\n",
        "- [ ] So quando usare un modello lineare vs tree-based\n",
        "- [ ] So usare `train_test_split()` con `stratify` e `random_state`\n",
        "- [ ] Capisco perche lo scaling e necessario per modelli lineari\n",
        "- [ ] So usare `StandardScaler` correttamente (fit su train, transform su entrambi)\n",
        "- [ ] So creare e addestrare una `LogisticRegression`\n",
        "- [ ] Capisco la differenza tra `.predict()` e `.predict_proba()`\n",
        "- [ ] So interpretare i coefficienti del modello (segno e magnitudo)\n",
        "- [ ] So calcolare l'accuracy con `accuracy_score()`\n",
        "- [ ] Capisco cos'e il data leakage e come evitarlo\n",
        "- [ ] So che l'ordine corretto e: split → scale → fit → predict\n",
        "- [ ] Ricordo di impostare `random_state` per riproducibilita\n",
        "\n",
        "**Se hai dubbi su qualche punto, rileggi la sezione corrispondente prima di proseguire.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a1b5a49",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SEZIONE 9 — Changelog Didattico\n",
        "\n",
        "| Versione | Data       | Modifiche                                                                 |\n",
        "|----------|------------|---------------------------------------------------------------------------|\n",
        "| 1.0      | Originale  | Versione iniziale del notebook (teoria estesa, poco codice)              |\n",
        "| 2.0      | 2025-12-30 | Ristrutturazione completa secondo template standard a 9 sezioni          |\n",
        "|          |            | + Nuovo header con obiettivi, prerequisiti e indice                      |\n",
        "|          |            | + Consolidata SEZIONE 1: Teoria (4 famiglie, matematica logistic)        |\n",
        "|          |            | + Nuova SEZIONE 2: Schema mentale / mappa decisionale                    |\n",
        "|          |            | + Riorganizzata SEZIONE 3: Notebook dimostrativo con spiegazioni         |\n",
        "|          |            | + Aggiunti \"Perche questo passaggio\" prima di ogni cella di codice       |\n",
        "|          |            | + Aggiunti micro-checkpoint con asserzioni e sanity check                |\n",
        "|          |            | + Aggiunto esempio di predict_proba per vedere probabilita               |\n",
        "|          |            | + Rimosse sezioni duplicate/verbose                                      |\n",
        "|          |            | + Aggiunta SEZIONE 4: Metodi spiegati con tabelle di riferimento         |\n",
        "|          |            | + Aggiunta SEZIONE 5: Glossario (16 termini)                             |\n",
        "|          |            | + Aggiunta SEZIONE 6: Errori comuni e debug rapido (7 errori)            |\n",
        "|          |            | + Aggiunta SEZIONE 7: Conclusione operativa                              |\n",
        "|          |            | + Aggiunta SEZIONE 8: Checklist di fine lezione (12 items)               |\n",
        "|          |            | + Aggiunta SEZIONE 9: Changelog didattico                                |\n",
        "\n",
        "---\n",
        "\n",
        "**Fine della Lezione 5**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.14.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
