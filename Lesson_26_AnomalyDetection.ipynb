{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e086e21",
   "metadata": {},
   "source": [
    "# üîç Lezione 26 ‚Äî Anomaly Detection: Trovare l'Ago nel Pagliaio\n",
    "\n",
    "## üìö Obiettivi di Apprendimento\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "1. **Capire** cosa sono le anomalie e perch√© rilevarle √® importante\n",
    "2. **Applicare** Isolation Forest per anomaly detection\n",
    "3. **Usare** Local Outlier Factor (LOF) per rilevare outliers locali\n",
    "4. **Confrontare** i diversi algoritmi e scegliere il pi√π appropriato\n",
    "5. **Valutare** le performance di un sistema di anomaly detection\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Perch√© Questa Lezione √® Importante\n",
    "\n",
    "Le anomalie sono ovunque nel mondo reale:\n",
    "- **Frodi bancarie:** transazioni sospette tra milioni di legittime\n",
    "- **Cybersecurity:** attacchi tra traffico di rete normale\n",
    "- **Manutenzione predittiva:** guasti imminenti in macchinari\n",
    "- **Healthcare:** pazienti con condizioni rare\n",
    "- **Quality control:** prodotti difettosi in produzione\n",
    "\n",
    "**Il Problema:** Le anomalie sono RARE (1-5% dei dati)\n",
    "- Non puoi usare classificazione supervisionata (poche label)\n",
    "- Devi trovare pattern \"strani\" senza sapere cosa cercare\n",
    "\n",
    "**La Soluzione:** Algoritmi unsupervised di Anomaly Detection!\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Struttura della Lezione\n",
    "\n",
    "| Sezione | Contenuto |\n",
    "|---------|-----------|\n",
    "| **1. Teoria** | Tipi di anomalie, approcci, metriche |\n",
    "| **2. Schema Mentale** | Workflow di anomaly detection |\n",
    "| **3. Demo Pratiche** | 5 esempi hands-on |\n",
    "| **4. Esercizi** | 3 esercizi con soluzioni |\n",
    "| **5. Conclusione** | Best practices e prossimi passi |\n",
    "| **6. Bignami** | Scheda di riferimento rapido |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3816d89",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìñ 1. Teoria ‚Äî Anomaly Detection Fondamenti\n",
    "\n",
    "## 1.1 Cos'√® un'Anomalia?\n",
    "\n",
    "Un'**anomalia** (o outlier) √® un'osservazione che si discosta significativamente dal comportamento normale.\n",
    "\n",
    "```\n",
    "üìä TIPI DI ANOMALIE:\n",
    "\n",
    "1. POINT ANOMALIES (Puntuali)\n",
    "   ‚Üí Un singolo punto √® anomalo\n",
    "   ‚Üí Es: Una transazione da 1M‚Ç¨ quando la media √® 100‚Ç¨\n",
    "   \n",
    "2. CONTEXTUAL ANOMALIES (Contestuali)  \n",
    "   ‚Üí Anomalo solo in un certo contesto\n",
    "   ‚Üí Es: Temperatura 30¬∞C √® normale d'estate, anomala d'inverno\n",
    "   \n",
    "3. COLLECTIVE ANOMALIES (Collettive)\n",
    "   ‚Üí Un gruppo di punti insieme √® anomalo\n",
    "   ‚Üí Es: Sequenza di 100 login falliti consecutivi\n",
    "```\n",
    "\n",
    "## 1.2 Approcci all'Anomaly Detection\n",
    "\n",
    "```\n",
    "üìä METODI PRINCIPALI:\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                ANOMALY DETECTION                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚îú‚îÄ‚îÄ DENSITY-BASED\n",
    "         ‚îÇ   ‚îî‚îÄ‚îÄ LOF (Local Outlier Factor)\n",
    "         ‚îÇ       ‚Üí Confronta densit√† locale vs vicini\n",
    "         ‚îÇ\n",
    "         ‚îú‚îÄ‚îÄ DISTANCE-BASED  \n",
    "         ‚îÇ   ‚îî‚îÄ‚îÄ K-Nearest Neighbors\n",
    "         ‚îÇ       ‚Üí Punti lontani dai vicini = anomali\n",
    "         ‚îÇ\n",
    "         ‚îú‚îÄ‚îÄ ISOLATION-BASED\n",
    "         ‚îÇ   ‚îî‚îÄ‚îÄ Isolation Forest\n",
    "         ‚îÇ       ‚Üí Anomalie si isolano facilmente\n",
    "         ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ RECONSTRUCTION-BASED\n",
    "             ‚îî‚îÄ‚îÄ Autoencoder\n",
    "                 ‚Üí Anomalie hanno alto errore di ricostruzione\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0a22c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 Isolation Forest ‚Äî L'Algoritmo Star\n",
    "\n",
    "**Intuizione:** Le anomalie sono FACILI da isolare!\n",
    "\n",
    "```\n",
    "üå≤ COME FUNZIONA ISOLATION FOREST:\n",
    "\n",
    "1. Costruisce alberi decisionali CASUALI\n",
    "   ‚Üí Seleziona feature casuale\n",
    "   ‚Üí Seleziona split casuale tra min e max\n",
    "   \n",
    "2. Le ANOMALIE vengono isolate in POCHI SPLIT\n",
    "   ‚Üí Sono \"diverse\" ‚Üí basta poco per separarle\n",
    "   \n",
    "3. I punti NORMALI richiedono MOLTI SPLIT\n",
    "   ‚Üí Sono \"simili\" ‚Üí difficili da separare\n",
    "   \n",
    "4. ANOMALY SCORE = profondit√† media negli alberi\n",
    "   ‚Üí Bassa profondit√† = anomalia\n",
    "   ‚Üí Alta profondit√† = normale\n",
    "\n",
    "üìä ESEMPIO VISIVO:\n",
    "\n",
    "  Punti normali (cluster)     Anomalia\n",
    "        ‚óè‚óè‚óè‚óè                     ‚òÖ\n",
    "        ‚óè‚óè‚óè‚óè\n",
    "        ‚óè‚óè‚óè‚óè\n",
    "        \n",
    "  Split necessari: 10+        Split necessari: 2\n",
    "```\n",
    "\n",
    "### Parametri Chiave\n",
    "\n",
    "| Parametro | Default | Descrizione |\n",
    "|-----------|---------|-------------|\n",
    "| `n_estimators` | 100 | Numero di alberi |\n",
    "| `contamination` | 'auto' | % attesa di anomalie |\n",
    "| `max_samples` | 'auto' | Campioni per albero |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70b4d27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 Local Outlier Factor (LOF) ‚Äî Densit√† Locale\n",
    "\n",
    "**Intuizione:** Un punto √® anomalo se la sua densit√† locale √® MOLTO diversa da quella dei vicini.\n",
    "\n",
    "```\n",
    "üìä COME FUNZIONA LOF:\n",
    "\n",
    "1. Per ogni punto, trova i K vicini pi√π prossimi\n",
    "\n",
    "2. Calcola la DENSIT√Ä LOCALE:\n",
    "   ‚Üí Quanti punti ci sono nel suo intorno?\n",
    "   \n",
    "3. Confronta con la densit√† dei VICINI:\n",
    "   ‚Üí Se io ho densit√† bassa ma i miei vicini alta ‚Üí ANOMALIA\n",
    "   \n",
    "4. LOF SCORE:\n",
    "   ‚Üí LOF ‚âà 1: normale (simile ai vicini)\n",
    "   ‚Üí LOF >> 1: anomalia (molto meno denso dei vicini)\n",
    "   ‚Üí LOF < 1: pi√π denso dei vicini (super-normale)\n",
    "\n",
    "üìä ESEMPIO:\n",
    "\n",
    "   ‚óè‚óè‚óè‚óè‚óè‚óè        ‚òÖ           ‚óè‚óè‚óè‚óè\n",
    "   ‚óè‚óè‚óè‚óè‚óè‚óè                    ‚óè‚óè‚óè‚óè\n",
    "   \n",
    "   Cluster A    Anomalia    Cluster B\n",
    "   (alta densit√†)  (LOF alto)  (alta densit√†)\n",
    "```\n",
    "\n",
    "### LOF vs Isolation Forest\n",
    "\n",
    "| Aspetto | Isolation Forest | LOF |\n",
    "|---------|-----------------|-----|\n",
    "| **Velocit√†** | Molto veloce | Pi√π lento (calcola distanze) |\n",
    "| **Scalabilit√†** | Ottima (O(n log n)) | Media (O(n¬≤)) |\n",
    "| **Anomalie locali** | Meno sensibile | Eccellente |\n",
    "| **Interpretabilit√†** | Media | Buona (score = densit√†) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d0b08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 Metriche di Valutazione\n",
    "\n",
    "Valutare anomaly detection √® difficile perch√© spesso non abbiamo label!\n",
    "\n",
    "```\n",
    "üìä METRICHE CON LABEL (se disponibili):\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    CONFUSION MATRIX                       ‚îÇ\n",
    "‚îÇ                                                          ‚îÇ\n",
    "‚îÇ                    Predetto                              ‚îÇ\n",
    "‚îÇ                 Normale  Anomalia                        ‚îÇ\n",
    "‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ\n",
    "‚îÇ Reale    ‚îÇ   TN    ‚îÇ    FP    ‚îÇ  ‚Üê False Positive       ‚îÇ\n",
    "‚îÇ Normale  ‚îÇ         ‚îÇ  (Allarme‚îÇ     (Allerta inutile)   ‚îÇ\n",
    "‚îÇ          ‚îÇ         ‚îÇ   falso) ‚îÇ                          ‚îÇ\n",
    "‚îÇ          ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                          ‚îÇ\n",
    "‚îÇ Reale    ‚îÇ   FN    ‚îÇ    TP    ‚îÇ  ‚Üê True Positive        ‚îÇ\n",
    "‚îÇ Anomalia ‚îÇ (Perso!)‚îÇ  (Trovata‚îÇ     (Anomalia rilevata) ‚îÇ\n",
    "‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "PRECISION = TP / (TP + FP)  ‚Üí Quante anomalie predette sono vere?\n",
    "RECALL    = TP / (TP + FN)  ‚Üí Quante anomalie vere sono state trovate?\n",
    "F1-SCORE  = 2 √ó (P √ó R) / (P + R)\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è Problema: Classi Sbilanciate\n",
    "\n",
    "Con 1% di anomalie:\n",
    "- Predire sempre \"normale\" d√† 99% accuracy ma √® INUTILE\n",
    "- Usa **Precision-Recall** invece di Accuracy\n",
    "- Usa **F1-Score** o **AUC-PR**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de206ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† 2. Schema Mentale ‚Äî Workflow Anomaly Detection\n",
    "\n",
    "## üìä Diagramma di Flusso\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    WORKFLOW ANOMALY DETECTION                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ DATI GREZZI ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚îÇ\n",
    "           ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     Scaling necessario?\n",
    "    ‚îÇ   SCALING   ‚îÇ ‚óÑ‚îÄ‚îÄ LOF: S√å (usa distanze)\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     Isolation Forest: OPZIONALE\n",
    "           ‚îÇ\n",
    "           ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     Quale algoritmo?\n",
    "    ‚îÇ  ALGORITMO  ‚îÇ ‚óÑ‚îÄ‚îÄ Isolation Forest: veloce, general purpose\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     LOF: anomalie locali\n",
    "           ‚îÇ            DBSCAN: clustering + outliers\n",
    "           ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     Come settare?\n",
    "    ‚îÇ CONTAMINATION‚îÇ ‚óÑ‚îÄ‚îÄ % attesa di anomalie\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     Se ignoto: prova 0.01-0.10\n",
    "           ‚îÇ\n",
    "           ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     Come valutare?\n",
    "    ‚îÇ VALUTAZIONE ‚îÇ ‚óÑ‚îÄ‚îÄ Con label: Precision, Recall, F1\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     Senza label: Visualizzazione\n",
    "           ‚îÇ\n",
    "           ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ   OUTPUT    ‚îÇ ‚Üí Anomalie identificate + Scores\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## ‚úÖ Checklist Anomaly Detection\n",
    "\n",
    "- [ ] Definito cosa significa \"anomalia\" nel contesto?\n",
    "- [ ] Stimata la % di anomalie attese?\n",
    "- [ ] Scaling applicato (se necessario)?\n",
    "- [ ] Algoritmo appropriato scelto?\n",
    "- [ ] Soglia di decisione definita?\n",
    "- [ ] Validazione effettuata?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2524b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üî¨ 3. Demo Pratiche\n",
    "\n",
    "## üéØ Demo 1 ‚Äî Isolation Forest: Primo Esempio\n",
    "\n",
    "Iniziamo con un dataset sintetico per capire come funziona Isolation Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83d7baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP INIZIALE\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, f1_score\n",
    "from sklearn.datasets import make_blobs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Setup completato!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# DEMO 1 ‚Äî Isolation Forest Base\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEMO 1 ‚Äî Isolation Forest: Primo Esempio\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 1: Crea dataset con anomalie\n",
    "# ============================================\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dati normali: 2 cluster\n",
    "X_normal, _ = make_blobs(n_samples=300, centers=2, cluster_std=0.5, random_state=42)\n",
    "\n",
    "# Anomalie: punti sparsi random\n",
    "n_anomalies = 20\n",
    "X_anomalies = np.random.uniform(low=-6, high=6, size=(n_anomalies, 2))\n",
    "\n",
    "# Combina\n",
    "X = np.vstack([X_normal, X_anomalies])\n",
    "y_true = np.array([0]*300 + [1]*n_anomalies)  # 0=normale, 1=anomalia\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"   - Punti normali: 300\")\n",
    "print(f\"   - Anomalie: {n_anomalies}\")\n",
    "print(f\"   - Contamination reale: {n_anomalies/(300+n_anomalies)*100:.1f}%\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 2: Applica Isolation Forest\n",
    "# ============================================\n",
    "# contamination = % attesa di anomalie\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=0.05,  # 5% anomalie attese\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit_predict ritorna: 1 = normale, -1 = anomalia\n",
    "y_pred_iso = iso_forest.fit_predict(X)\n",
    "\n",
    "# Converti a 0/1 per confronto con y_true\n",
    "y_pred_binary = (y_pred_iso == -1).astype(int)  # -1 ‚Üí 1 (anomalia)\n",
    "\n",
    "print(f\"\\nüìà Isolation Forest Results:\")\n",
    "print(f\"   - Anomalie rilevate: {(y_pred_iso == -1).sum()}\")\n",
    "print(f\"   - Anomalie reali: {y_true.sum()}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 3: Valutazione\n",
    "# ============================================\n",
    "# Calcola metriche\n",
    "tp = np.sum((y_true == 1) & (y_pred_binary == 1))\n",
    "fp = np.sum((y_true == 0) & (y_pred_binary == 1))\n",
    "fn = np.sum((y_true == 1) & (y_pred_binary == 0))\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä Performance:\")\n",
    "print(f\"   - Precision: {precision:.3f}\")\n",
    "print(f\"   - Recall: {recall:.3f}\")\n",
    "print(f\"   - F1-Score: {f1:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 4: Visualizzazione\n",
    "# ============================================\n",
    "# Ottieni anomaly scores\n",
    "scores = iso_forest.decision_function(X)  # Pi√π basso = pi√π anomalo\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Ground Truth\n",
    "colors_true = ['blue' if y == 0 else 'red' for y in y_true]\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=colors_true, s=30, alpha=0.7)\n",
    "axes[0].set_title('Ground Truth\\n(rosso = anomalie reali)', fontsize=12)\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# Plot 2: Predizioni Isolation Forest\n",
    "colors_pred = ['blue' if y == 1 else 'red' for y in y_pred_iso]\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=colors_pred, s=30, alpha=0.7)\n",
    "axes[1].set_title(f'Isolation Forest\\nPrecision={precision:.2f}, Recall={recall:.2f}', fontsize=12)\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "# Plot 3: Anomaly Scores\n",
    "scatter = axes[2].scatter(X[:, 0], X[:, 1], c=scores, cmap='RdYlBu', s=30, alpha=0.7)\n",
    "axes[2].set_title('Anomaly Scores\\n(rosso = pi√π anomalo)', fontsize=12)\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter, ax=axes[2], label='Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "üìå OSSERVAZIONI:\n",
    "   - Isolation Forest rileva bene le anomalie sparse\n",
    "   - Lo score indica quanto √® \"anomalo\" ogni punto\n",
    "   - Alcuni punti al bordo dei cluster sono erroneamente classificati\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f0123",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Demo 2 ‚Äî Local Outlier Factor (LOF)\n",
    "\n",
    "LOF √® eccellente per rilevare anomalie **locali** ‚Äî punti anomali rispetto ai loro vicini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a55de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 2 ‚Äî Local Outlier Factor (LOF)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEMO 2 ‚Äî Local Outlier Factor (LOF)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 1: Dataset con anomalie LOCALI\n",
    "# ============================================\n",
    "np.random.seed(42)\n",
    "\n",
    "# Cluster denso\n",
    "cluster_dense = np.random.randn(200, 2) * 0.3\n",
    "\n",
    "# Cluster sparso\n",
    "cluster_sparse = np.random.randn(100, 2) * 2 + [5, 5]\n",
    "\n",
    "# Anomalie: punti tra i due cluster (localmente anomali!)\n",
    "# Questi sono normali globalmente ma anomali localmente\n",
    "local_anomalies = np.array([\n",
    "    [2.5, 2.5],\n",
    "    [2.0, 3.0],\n",
    "    [3.0, 2.0],\n",
    "    [-2, -2],\n",
    "    [-2.5, -1.5]\n",
    "])\n",
    "\n",
    "# Anomalie globali\n",
    "global_anomalies = np.array([\n",
    "    [10, 10],\n",
    "    [-8, 8],\n",
    "    [8, -8]\n",
    "])\n",
    "\n",
    "X_lof = np.vstack([cluster_dense, cluster_sparse, local_anomalies, global_anomalies])\n",
    "y_true_lof = np.array([0]*200 + [0]*100 + [1]*5 + [1]*3)  # 8 anomalie totali\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"   - Cluster denso: 200 punti\")\n",
    "print(f\"   - Cluster sparso: 100 punti\")\n",
    "print(f\"   - Anomalie locali: 5\")\n",
    "print(f\"   - Anomalie globali: 3\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 2: Applica LOF\n",
    "# ============================================\n",
    "# n_neighbors: quanti vicini considerare\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=20,\n",
    "    contamination=0.03,\n",
    "    novelty=False  # False per fit_predict\n",
    ")\n",
    "\n",
    "y_pred_lof = lof.fit_predict(X_lof)\n",
    "y_pred_lof_binary = (y_pred_lof == -1).astype(int)\n",
    "\n",
    "# Ottieni LOF scores\n",
    "lof_scores = -lof.negative_outlier_factor_  # Pi√π alto = pi√π anomalo\n",
    "\n",
    "print(f\"\\nüìà LOF Results:\")\n",
    "print(f\"   - Anomalie rilevate: {(y_pred_lof == -1).sum()}\")\n",
    "print(f\"   - Anomalie reali: {y_true_lof.sum()}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 3: Valutazione\n",
    "# ============================================\n",
    "tp_lof = np.sum((y_true_lof == 1) & (y_pred_lof_binary == 1))\n",
    "fp_lof = np.sum((y_true_lof == 0) & (y_pred_lof_binary == 1))\n",
    "fn_lof = np.sum((y_true_lof == 1) & (y_pred_lof_binary == 0))\n",
    "\n",
    "precision_lof = tp_lof / (tp_lof + fp_lof) if (tp_lof + fp_lof) > 0 else 0\n",
    "recall_lof = tp_lof / (tp_lof + fn_lof) if (tp_lof + fn_lof) > 0 else 0\n",
    "f1_lof = 2 * precision_lof * recall_lof / (precision_lof + recall_lof) if (precision_lof + recall_lof) > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä Performance LOF:\")\n",
    "print(f\"   - Precision: {precision_lof:.3f}\")\n",
    "print(f\"   - Recall: {recall_lof:.3f}\")\n",
    "print(f\"   - F1-Score: {f1_lof:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 4: Confronto con Isolation Forest\n",
    "# ============================================\n",
    "iso_forest_compare = IsolationForest(contamination=0.03, random_state=42)\n",
    "y_pred_iso_compare = iso_forest_compare.fit_predict(X_lof)\n",
    "y_pred_iso_binary = (y_pred_iso_compare == -1).astype(int)\n",
    "\n",
    "tp_iso = np.sum((y_true_lof == 1) & (y_pred_iso_binary == 1))\n",
    "fp_iso = np.sum((y_true_lof == 0) & (y_pred_iso_binary == 1))\n",
    "fn_iso = np.sum((y_true_lof == 1) & (y_pred_iso_binary == 0))\n",
    "\n",
    "precision_iso = tp_iso / (tp_iso + fp_iso) if (tp_iso + fp_iso) > 0 else 0\n",
    "recall_iso = tp_iso / (tp_iso + fn_iso) if (tp_iso + fn_iso) > 0 else 0\n",
    "f1_iso = 2 * precision_iso * recall_iso / (precision_iso + recall_iso) if (precision_iso + recall_iso) > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä Performance Isolation Forest:\")\n",
    "print(f\"   - Precision: {precision_iso:.3f}\")\n",
    "print(f\"   - Recall: {recall_iso:.3f}\")\n",
    "print(f\"   - F1-Score: {f1_iso:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 5: Visualizzazione\n",
    "# ============================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Ground truth\n",
    "colors_true_lof = ['blue' if y == 0 else 'red' for y in y_true_lof]\n",
    "axes[0].scatter(X_lof[:, 0], X_lof[:, 1], c=colors_true_lof, s=30, alpha=0.7)\n",
    "axes[0].set_title('Ground Truth', fontsize=12)\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# LOF\n",
    "colors_lof = ['blue' if y == 1 else 'red' for y in y_pred_lof]\n",
    "axes[1].scatter(X_lof[:, 0], X_lof[:, 1], c=colors_lof, s=30, alpha=0.7)\n",
    "axes[1].set_title(f'LOF\\nF1={f1_lof:.2f}', fontsize=12)\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "\n",
    "# Isolation Forest\n",
    "colors_iso_c = ['blue' if y == 1 else 'red' for y in y_pred_iso_compare]\n",
    "axes[2].scatter(X_lof[:, 0], X_lof[:, 1], c=colors_iso_c, s=30, alpha=0.7)\n",
    "axes[2].set_title(f'Isolation Forest\\nF1={f1_iso:.2f}', fontsize=12)\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "üìå OSSERVAZIONI:\n",
    "   - LOF rileva meglio le anomalie LOCALI (tra i cluster)\n",
    "   - Isolation Forest pu√≤ perdere anomalie locali\n",
    "   - LOF considera la densit√† dei vicini, non solo la posizione globale\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ffa86",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Demo 3 ‚Äî Tuning del Parametro Contamination\n",
    "\n",
    "Il parametro `contamination` √® cruciale: indica la % di anomalie attese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ee823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 3 ‚Äî Tuning Contamination\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEMO 3 ‚Äî Impatto del Parametro Contamination\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Usiamo il dataset della Demo 1\n",
    "contamination_values = [0.01, 0.05, 0.10, 0.15, 0.20]\n",
    "\n",
    "results_contamination = []\n",
    "\n",
    "print(\"\\nüìä Test con diversi valori di contamination:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for cont in contamination_values:\n",
    "    iso = IsolationForest(n_estimators=100, contamination=cont, random_state=42)\n",
    "    y_pred = iso.fit_predict(X)\n",
    "    y_pred_bin = (y_pred == -1).astype(int)\n",
    "    \n",
    "    n_detected = (y_pred == -1).sum()\n",
    "    \n",
    "    tp = np.sum((y_true == 1) & (y_pred_bin == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred_bin == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred_bin == 0))\n",
    "    \n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "    \n",
    "    results_contamination.append({\n",
    "        'contamination': cont,\n",
    "        'detected': n_detected,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"   cont={cont:.2f}: detected={n_detected:3d}, P={prec:.2f}, R={rec:.2f}, F1={f1:.2f}\")\n",
    "\n",
    "# ============================================\n",
    "# Visualizzazione\n",
    "# ============================================\n",
    "df_cont = pd.DataFrame(results_contamination)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Anomalie rilevate\n",
    "axes[0].bar(range(len(contamination_values)), df_cont['detected'], color='steelblue', alpha=0.7)\n",
    "axes[0].axhline(y=20, color='red', linestyle='--', label=f'Anomalie reali: 20')\n",
    "axes[0].set_xticks(range(len(contamination_values)))\n",
    "axes[0].set_xticklabels([f'{c:.0%}' for c in contamination_values])\n",
    "axes[0].set_xlabel('Contamination')\n",
    "axes[0].set_ylabel('Anomalie Rilevate')\n",
    "axes[0].set_title('Numero Anomalie Rilevate vs Contamination')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Metriche\n",
    "axes[1].plot(contamination_values, df_cont['precision'], 'bo-', label='Precision', linewidth=2)\n",
    "axes[1].plot(contamination_values, df_cont['recall'], 'go-', label='Recall', linewidth=2)\n",
    "axes[1].plot(contamination_values, df_cont['f1'], 'ro-', label='F1-Score', linewidth=2)\n",
    "axes[1].set_xlabel('Contamination')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Precision, Recall, F1 vs Contamination')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Trova il migliore\n",
    "best_idx = df_cont['f1'].idxmax()\n",
    "best_cont = df_cont.loc[best_idx, 'contamination']\n",
    "best_f1 = df_cont.loc[best_idx, 'f1']\n",
    "\n",
    "print(f\"\"\"\n",
    "üìå OSSERVAZIONI:\n",
    "   - Contamination basso ‚Üí Alta Precision, Bassa Recall (perse anomalie)\n",
    "   - Contamination alto ‚Üí Alta Recall, Bassa Precision (troppi falsi positivi)\n",
    "   - Miglior F1 con contamination={best_cont:.2f} (F1={best_f1:.2f})\n",
    "   \n",
    "üí° REGOLA: Se non sai la % reale, inizia con 0.01-0.05 e adatta\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c2400",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Demo 4 ‚Äî Anomaly Detection su Dataset Reale\n",
    "\n",
    "Applichiamo anomaly detection al dataset Credit Card Fraud (simulato)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b4181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 4 ‚Äî Dataset Realistico: Credit Card Fraud\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEMO 4 ‚Äî Simulazione Credit Card Fraud Detection\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 1: Crea dataset simile a frodi bancarie\n",
    "# ============================================\n",
    "np.random.seed(42)\n",
    "\n",
    "n_normal = 5000\n",
    "n_fraud = 100  # 2% frodi\n",
    "\n",
    "# Transazioni normali: importi bassi-medi, orari diurni\n",
    "normal_amount = np.abs(np.random.exponential(50, n_normal))  # Media 50‚Ç¨\n",
    "normal_hour = np.random.normal(14, 4, n_normal) % 24  # Picco ore 14\n",
    "normal_freq = np.random.poisson(5, n_normal)  # 5 transazioni/settimana\n",
    "normal_distance = np.abs(np.random.normal(20, 10, n_normal))  # 20km da casa\n",
    "\n",
    "# Frodi: importi alti, orari notturni, distanza anomala\n",
    "fraud_amount = np.random.uniform(500, 5000, n_fraud)  # 500-5000‚Ç¨\n",
    "fraud_hour = np.random.uniform(0, 6, n_fraud)  # Notte\n",
    "fraud_freq = np.random.uniform(15, 30, n_fraud)  # Molte transazioni\n",
    "fraud_distance = np.random.uniform(500, 2000, n_fraud)  # Lontano da casa\n",
    "\n",
    "# Combina le feature\n",
    "X_fraud = np.column_stack([\n",
    "    np.concatenate([normal_amount, fraud_amount]),\n",
    "    np.concatenate([normal_hour, fraud_hour]),\n",
    "    np.concatenate([normal_freq, fraud_freq]),\n",
    "    np.concatenate([normal_distance, fraud_distance])\n",
    "])\n",
    "\n",
    "y_fraud = np.array([0]*n_normal + [1]*n_fraud)\n",
    "\n",
    "feature_names_fraud = ['Importo (‚Ç¨)', 'Ora', 'Frequenza', 'Distanza (km)']\n",
    "\n",
    "print(f\"\\nüìä Dataset Credit Card:\")\n",
    "print(f\"   - Transazioni normali: {n_normal}\")\n",
    "print(f\"   - Frodi: {n_fraud}\")\n",
    "print(f\"   - Fraud rate: {n_fraud/(n_normal+n_fraud)*100:.1f}%\")\n",
    "print(f\"   - Feature: {feature_names_fraud}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 2: Preprocessing\n",
    "# ============================================\n",
    "scaler = StandardScaler()\n",
    "X_fraud_scaled = scaler.fit_transform(X_fraud)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 3: Applica Isolation Forest e LOF\n",
    "# ============================================\n",
    "# Isolation Forest\n",
    "iso_fraud = IsolationForest(contamination=0.02, random_state=42)\n",
    "y_pred_iso_fraud = iso_fraud.fit_predict(X_fraud_scaled)\n",
    "y_pred_iso_bin = (y_pred_iso_fraud == -1).astype(int)\n",
    "\n",
    "# LOF\n",
    "lof_fraud = LocalOutlierFactor(n_neighbors=20, contamination=0.02)\n",
    "y_pred_lof_fraud = lof_fraud.fit_predict(X_fraud_scaled)\n",
    "y_pred_lof_bin = (y_pred_lof_fraud == -1).astype(int)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 4: Valutazione\n",
    "# ============================================\n",
    "def calc_metrics(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    \n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "    \n",
    "    return {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn, \n",
    "            'Precision': prec, 'Recall': rec, 'F1': f1}\n",
    "\n",
    "metrics_iso = calc_metrics(y_fraud, y_pred_iso_bin)\n",
    "metrics_lof = calc_metrics(y_fraud, y_pred_lof_bin)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONFRONTO RISULTATI\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "comparison_fraud = pd.DataFrame({\n",
    "    'Metrica': ['TP (Frodi trovate)', 'FP (Falsi allarmi)', 'FN (Frodi perse)', \n",
    "                'Precision', 'Recall', 'F1-Score'],\n",
    "    'Isolation Forest': [metrics_iso['TP'], metrics_iso['FP'], metrics_iso['FN'],\n",
    "                         f\"{metrics_iso['Precision']:.3f}\", \n",
    "                         f\"{metrics_iso['Recall']:.3f}\",\n",
    "                         f\"{metrics_iso['F1']:.3f}\"],\n",
    "    'LOF': [metrics_lof['TP'], metrics_lof['FP'], metrics_lof['FN'],\n",
    "            f\"{metrics_lof['Precision']:.3f}\",\n",
    "            f\"{metrics_lof['Recall']:.3f}\",\n",
    "            f\"{metrics_lof['F1']:.3f}\"]\n",
    "})\n",
    "print(comparison_fraud.to_string(index=False))\n",
    "\n",
    "# ============================================\n",
    "# PASSO 5: Visualizzazione\n",
    "# ============================================\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Riduci a 2D per visualizzazione\n",
    "pca_fraud = PCA(n_components=2)\n",
    "X_fraud_2d = pca_fraud.fit_transform(X_fraud_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Ground truth\n",
    "colors_gt = ['blue' if y == 0 else 'red' for y in y_fraud]\n",
    "axes[0].scatter(X_fraud_2d[:, 0], X_fraud_2d[:, 1], c=colors_gt, s=5, alpha=0.5)\n",
    "axes[0].set_title(f'Ground Truth\\n({n_fraud} frodi reali)', fontsize=12)\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "\n",
    "# Isolation Forest\n",
    "colors_iso_f = ['blue' if y == 1 else 'red' for y in y_pred_iso_fraud]\n",
    "axes[1].scatter(X_fraud_2d[:, 0], X_fraud_2d[:, 1], c=colors_iso_f, s=5, alpha=0.5)\n",
    "axes[1].set_title(f'Isolation Forest\\nF1={metrics_iso[\"F1\"]:.2f}', fontsize=12)\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "\n",
    "# LOF\n",
    "colors_lof_f = ['blue' if y == 1 else 'red' for y in y_pred_lof_fraud]\n",
    "axes[2].scatter(X_fraud_2d[:, 0], X_fraud_2d[:, 1], c=colors_lof_f, s=5, alpha=0.5)\n",
    "axes[2].set_title(f'LOF\\nF1={metrics_lof[\"F1\"]:.2f}', fontsize=12)\n",
    "axes[2].set_xlabel('PC1')\n",
    "axes[2].set_ylabel('PC2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\"\"\n",
    "üìå INTERPRETAZIONE BUSINESS:\n",
    "   - Su 100 frodi reali, Isolation Forest ne trova {metrics_iso['TP']}\n",
    "   - {metrics_iso['FP']} falsi allarmi da investigare manualmente\n",
    "   - {metrics_iso['FN']} frodi perse (potenziale danno economico!)\n",
    "   \n",
    "üí° Nel fraud detection, spesso RECALL √® pi√π importante di PRECISION\n",
    "   (meglio investigare falsi allarmi che perdere frodi)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b61a72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Demo 5 ‚Äî Ensemble: Combinare Pi√π Algoritmi\n",
    "\n",
    "Combinare pi√π algoritmi spesso migliora la detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c174975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 5 ‚Äî Ensemble di Anomaly Detectors\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEMO 5 ‚Äî Ensemble: Combinare Isolation Forest + LOF\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# Strategia: Un punto √® anomalia se ENTRAMBI gli algoritmi concordano\n",
    "# Oppure: Voting a maggioranza\n",
    "# ============================================\n",
    "\n",
    "# Riusiamo il dataset fraud\n",
    "# y_pred_iso_bin e y_pred_lof_bin gi√† calcolati\n",
    "\n",
    "# ============================================\n",
    "# STRATEGIA 1: AND (entrambi devono dire anomalia)\n",
    "# ============================================\n",
    "y_pred_and = ((y_pred_iso_bin == 1) & (y_pred_lof_bin == 1)).astype(int)\n",
    "\n",
    "metrics_and = calc_metrics(y_fraud, y_pred_and)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STRATEGIA AND (pi√π conservativa)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"   - Anomalie trovate: {y_pred_and.sum()}\")\n",
    "print(f\"   - Precision: {metrics_and['Precision']:.3f}\")\n",
    "print(f\"   - Recall: {metrics_and['Recall']:.3f}\")\n",
    "print(f\"   - F1-Score: {metrics_and['F1']:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# STRATEGIA 2: OR (almeno uno dice anomalia)\n",
    "# ============================================\n",
    "y_pred_or = ((y_pred_iso_bin == 1) | (y_pred_lof_bin == 1)).astype(int)\n",
    "\n",
    "metrics_or = calc_metrics(y_fraud, y_pred_or)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STRATEGIA OR (pi√π aggressiva)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"   - Anomalie trovate: {y_pred_or.sum()}\")\n",
    "print(f\"   - Precision: {metrics_or['Precision']:.3f}\")\n",
    "print(f\"   - Recall: {metrics_or['Recall']:.3f}\")\n",
    "print(f\"   - F1-Score: {metrics_or['F1']:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# STRATEGIA 3: Score averaging\n",
    "# ============================================\n",
    "# Ottieni gli scores\n",
    "iso_scores_fraud = -iso_fraud.decision_function(X_fraud_scaled)  # Pi√π alto = pi√π anomalo\n",
    "lof_scores_fraud = -lof_fraud.negative_outlier_factor_\n",
    "\n",
    "# Normalizza a [0, 1]\n",
    "iso_scores_norm = (iso_scores_fraud - iso_scores_fraud.min()) / (iso_scores_fraud.max() - iso_scores_fraud.min())\n",
    "lof_scores_norm = (lof_scores_fraud - lof_scores_fraud.min()) / (lof_scores_fraud.max() - lof_scores_fraud.min())\n",
    "\n",
    "# Media degli scores\n",
    "ensemble_scores = (iso_scores_norm + lof_scores_norm) / 2\n",
    "\n",
    "# Threshold: top 2% come anomalie\n",
    "threshold = np.percentile(ensemble_scores, 98)\n",
    "y_pred_ensemble = (ensemble_scores >= threshold).astype(int)\n",
    "\n",
    "metrics_ensemble = calc_metrics(y_fraud, y_pred_ensemble)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STRATEGIA SCORE AVERAGING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"   - Anomalie trovate: {y_pred_ensemble.sum()}\")\n",
    "print(f\"   - Precision: {metrics_ensemble['Precision']:.3f}\")\n",
    "print(f\"   - Recall: {metrics_ensemble['Recall']:.3f}\")\n",
    "print(f\"   - F1-Score: {metrics_ensemble['F1']:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# Confronto finale\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFRONTO TUTTE LE STRATEGIE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_results = pd.DataFrame({\n",
    "    'Strategia': ['Isolation Forest', 'LOF', 'AND', 'OR', 'Score Avg'],\n",
    "    'Detected': [(y_pred_iso_bin==1).sum(), (y_pred_lof_bin==1).sum(), \n",
    "                 y_pred_and.sum(), y_pred_or.sum(), y_pred_ensemble.sum()],\n",
    "    'Precision': [metrics_iso['Precision'], metrics_lof['Precision'],\n",
    "                  metrics_and['Precision'], metrics_or['Precision'], metrics_ensemble['Precision']],\n",
    "    'Recall': [metrics_iso['Recall'], metrics_lof['Recall'],\n",
    "               metrics_and['Recall'], metrics_or['Recall'], metrics_ensemble['Recall']],\n",
    "    'F1': [metrics_iso['F1'], metrics_lof['F1'],\n",
    "           metrics_and['F1'], metrics_or['F1'], metrics_ensemble['F1']]\n",
    "})\n",
    "print(all_results.to_string(index=False))\n",
    "\n",
    "# Visualizzazione\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(all_results))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, all_results['Precision'], width, label='Precision', color='steelblue')\n",
    "bars2 = ax.bar(x, all_results['Recall'], width, label='Recall', color='forestgreen')\n",
    "bars3 = ax.bar(x + width, all_results['F1'], width, label='F1-Score', color='coral')\n",
    "\n",
    "ax.set_xlabel('Strategia')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Confronto Strategie di Ensemble')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(all_results['Strategia'], rotation=15)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_strategy = all_results.loc[all_results['F1'].idxmax(), 'Strategia']\n",
    "print(f\"\"\"\n",
    "üìå CONCLUSIONI:\n",
    "   - AND: Alta Precision, Bassa Recall (conservativo)\n",
    "   - OR: Alta Recall, Bassa Precision (aggressivo)  \n",
    "   - Score Avg: Buon bilanciamento\n",
    "   - Miglior F1: {best_strategy}\n",
    "   \n",
    "üí° La scelta dipende dal costo di FP vs FN nel tuo business!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3220b6c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üèãÔ∏è 4. Esercizi Pratici\n",
    "\n",
    "## üèãÔ∏è Esercizio 26.1 ‚Äî Anomaly Detection su Dati Sensori\n",
    "\n",
    "**Obiettivo:** Simulare il monitoraggio di un macchinario industriale.\n",
    "\n",
    "**Consegna:**\n",
    "1. Crea un dataset di **10.000 letture** da sensori con 5 feature:\n",
    "   - Temperatura, Pressione, Vibrazione, RPM, Consumo energia\n",
    "2. Aggiungi **100 anomalie** (valori fuori range)\n",
    "3. Applica **Isolation Forest** con contamination corretto\n",
    "4. Calcola **Precision, Recall, F1**\n",
    "5. Visualizza le anomalie rilevate vs ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a34f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 26.1 ‚Äî SOLUZIONE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ESERCIZIO 26.1 ‚Äî Monitoraggio Sensori Industriali\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 1: Crea dataset sensori\n",
    "# ============================================\n",
    "np.random.seed(42)\n",
    "\n",
    "n_normal_sensors = 10000\n",
    "n_anomaly_sensors = 100\n",
    "\n",
    "# Letture normali (macchinario funzionante)\n",
    "temp_normal = np.random.normal(70, 5, n_normal_sensors)  # 70¬∞C ¬± 5\n",
    "pressure_normal = np.random.normal(100, 10, n_normal_sensors)  # 100 bar ¬± 10\n",
    "vibration_normal = np.random.normal(0.5, 0.1, n_normal_sensors)  # 0.5 mm/s ¬± 0.1\n",
    "rpm_normal = np.random.normal(3000, 100, n_normal_sensors)  # 3000 rpm ¬± 100\n",
    "energy_normal = np.random.normal(50, 5, n_normal_sensors)  # 50 kW ¬± 5\n",
    "\n",
    "# Anomalie (guasti imminenti!)\n",
    "temp_anomaly = np.random.uniform(100, 150, n_anomaly_sensors)  # Surriscaldamento\n",
    "pressure_anomaly = np.random.uniform(150, 200, n_anomaly_sensors)  # Pressione alta\n",
    "vibration_anomaly = np.random.uniform(2, 5, n_anomaly_sensors)  # Vibrazione anomala\n",
    "rpm_anomaly = np.random.uniform(1000, 2000, n_anomaly_sensors)  # RPM basso\n",
    "energy_anomaly = np.random.uniform(80, 120, n_anomaly_sensors)  # Consumo alto\n",
    "\n",
    "# Combina\n",
    "X_sensors = np.column_stack([\n",
    "    np.concatenate([temp_normal, temp_anomaly]),\n",
    "    np.concatenate([pressure_normal, pressure_anomaly]),\n",
    "    np.concatenate([vibration_normal, vibration_anomaly]),\n",
    "    np.concatenate([rpm_normal, rpm_anomaly]),\n",
    "    np.concatenate([energy_normal, energy_anomaly])\n",
    "])\n",
    "\n",
    "y_sensors = np.array([0]*n_normal_sensors + [1]*n_anomaly_sensors)\n",
    "sensor_names = ['Temperatura', 'Pressione', 'Vibrazione', 'RPM', 'Energia']\n",
    "\n",
    "print(f\"\\nüìä Dataset Sensori:\")\n",
    "print(f\"   - Letture normali: {n_normal_sensors}\")\n",
    "print(f\"   - Anomalie (guasti): {n_anomaly_sensors}\")\n",
    "print(f\"   - Contamination: {n_anomaly_sensors/(n_normal_sensors+n_anomaly_sensors)*100:.1f}%\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 2: Preprocessing\n",
    "# ============================================\n",
    "scaler_sensors = StandardScaler()\n",
    "X_sensors_scaled = scaler_sensors.fit_transform(X_sensors)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 3: Isolation Forest\n",
    "# ============================================\n",
    "iso_sensors = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=0.01,  # 1% anomalie attese\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_pred_sensors = iso_sensors.fit_predict(X_sensors_scaled)\n",
    "y_pred_sensors_bin = (y_pred_sensors == -1).astype(int)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 4: Valutazione\n",
    "# ============================================\n",
    "metrics_sensors = calc_metrics(y_sensors, y_pred_sensors_bin)\n",
    "\n",
    "print(f\"\\nüìà Risultati Isolation Forest:\")\n",
    "print(f\"   - Anomalie rilevate: {y_pred_sensors_bin.sum()}\")\n",
    "print(f\"   - Anomalie reali: {y_sensors.sum()}\")\n",
    "print(f\"   - Precision: {metrics_sensors['Precision']:.3f}\")\n",
    "print(f\"   - Recall: {metrics_sensors['Recall']:.3f}\")\n",
    "print(f\"   - F1-Score: {metrics_sensors['F1']:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 5: Visualizzazione\n",
    "# ============================================\n",
    "# PCA per visualizzazione\n",
    "from sklearn.decomposition import PCA\n",
    "pca_sensors = PCA(n_components=2)\n",
    "X_sensors_2d = pca_sensors.fit_transform(X_sensors_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Ground truth\n",
    "colors_gt_s = ['blue' if y == 0 else 'red' for y in y_sensors]\n",
    "axes[0].scatter(X_sensors_2d[:, 0], X_sensors_2d[:, 1], c=colors_gt_s, s=3, alpha=0.5)\n",
    "axes[0].set_title('Ground Truth (rosso = guasti)', fontsize=12)\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "\n",
    "# Predizioni\n",
    "colors_pred_s = ['blue' if y == 1 else 'red' for y in y_pred_sensors]\n",
    "axes[1].scatter(X_sensors_2d[:, 0], X_sensors_2d[:, 1], c=colors_pred_s, s=3, alpha=0.5)\n",
    "axes[1].set_title(f'Isolation Forest\\nF1={metrics_sensors[\"F1\"]:.2f}', fontsize=12)\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "\n",
    "# Distribuzione scores\n",
    "scores_sensors = -iso_sensors.decision_function(X_sensors_scaled)\n",
    "axes[2].hist(scores_sensors[y_sensors==0], bins=50, alpha=0.7, label='Normale', color='blue')\n",
    "axes[2].hist(scores_sensors[y_sensors==1], bins=50, alpha=0.7, label='Anomalia', color='red')\n",
    "axes[2].set_xlabel('Anomaly Score')\n",
    "axes[2].set_ylabel('Frequenza')\n",
    "axes[2].set_title('Distribuzione Anomaly Scores', fontsize=12)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\"\"\n",
    "üìå INTERPRETAZIONE:\n",
    "   - Le anomalie hanno scores pi√π alti (pi√π \"anomali\")\n",
    "   - L'algoritmo rileva {metrics_sensors['TP']} guasti su {n_anomaly_sensors}\n",
    "   - In produzione, queste anomalie attiverebbero un allarme!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4b6f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è Esercizio 26.2 ‚Äî Confronto LOF vs Isolation Forest\n",
    "\n",
    "**Obiettivo:** Determinare quale algoritmo √® migliore per diversi tipi di anomalie.\n",
    "\n",
    "**Consegna:**\n",
    "1. Crea un dataset con:\n",
    "   - **Anomalie globali** (punti lontani da tutti)\n",
    "   - **Anomalie locali** (punti in zone a bassa densit√† tra cluster)\n",
    "2. Applica **Isolation Forest** e **LOF**\n",
    "3. Confronta quale algoritmo rileva meglio quale tipo di anomalia\n",
    "4. Visualizza i risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 26.2 ‚Äî SOLUZIONE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ESERCIZIO 26.2 ‚Äî Confronto LOF vs Isolation Forest\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 1: Crea dataset con anomalie globali e locali\n",
    "# ============================================\n",
    "np.random.seed(42)\n",
    "\n",
    "# Due cluster con densit√† diversa\n",
    "cluster_dense = np.random.randn(500, 2) * 0.5  # Denso\n",
    "cluster_sparse = np.random.randn(500, 2) * 1.5 + [6, 6]  # Pi√π sparso\n",
    "\n",
    "# Anomalie GLOBALI: lontane da tutto\n",
    "global_anom = np.array([\n",
    "    [15, 15], [-10, 10], [10, -10], [-10, -10], [0, 15]\n",
    "])\n",
    "\n",
    "# Anomalie LOCALI: tra i cluster (localmente anomale)\n",
    "local_anom = np.array([\n",
    "    [3, 3], [2.5, 3.5], [3.5, 2.5], [4, 4], [2, 4]\n",
    "])\n",
    "\n",
    "X_compare = np.vstack([cluster_dense, cluster_sparse, global_anom, local_anom])\n",
    "\n",
    "# Labels: 0=normale, 1=global, 2=local\n",
    "y_type = np.array([0]*500 + [0]*500 + [1]*5 + [2]*5)\n",
    "y_anomaly = (y_type > 0).astype(int)  # 0=normale, 1=anomalia (qualsiasi tipo)\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"   - Punti normali: 1000\")\n",
    "print(f\"   - Anomalie globali: 5\")\n",
    "print(f\"   - Anomalie locali: 5\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 2: Applica Isolation Forest e LOF\n",
    "# ============================================\n",
    "contamination_rate = 10 / 1010  # ~1%\n",
    "\n",
    "# Isolation Forest\n",
    "iso_compare = IsolationForest(contamination=contamination_rate, random_state=42)\n",
    "y_pred_iso_c = iso_compare.fit_predict(X_compare)\n",
    "y_pred_iso_c_bin = (y_pred_iso_c == -1).astype(int)\n",
    "\n",
    "# LOF\n",
    "lof_compare = LocalOutlierFactor(n_neighbors=20, contamination=contamination_rate)\n",
    "y_pred_lof_c = lof_compare.fit_predict(X_compare)\n",
    "y_pred_lof_c_bin = (y_pred_lof_c == -1).astype(int)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 3: Analisi per tipo di anomalia\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DETECTION PER TIPO DI ANOMALIA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Anomalie globali rilevate\n",
    "global_mask = (y_type == 1)\n",
    "iso_global_detected = y_pred_iso_c_bin[global_mask].sum()\n",
    "lof_global_detected = y_pred_lof_c_bin[global_mask].sum()\n",
    "\n",
    "# Anomalie locali rilevate\n",
    "local_mask = (y_type == 2)\n",
    "iso_local_detected = y_pred_iso_c_bin[local_mask].sum()\n",
    "lof_local_detected = y_pred_lof_c_bin[local_mask].sum()\n",
    "\n",
    "print(f\"\\nüìä Anomalie GLOBALI (5 totali):\")\n",
    "print(f\"   - Isolation Forest: {iso_global_detected}/5 rilevate\")\n",
    "print(f\"   - LOF: {lof_global_detected}/5 rilevate\")\n",
    "\n",
    "print(f\"\\nüìä Anomalie LOCALI (5 totali):\")\n",
    "print(f\"   - Isolation Forest: {iso_local_detected}/5 rilevate\")\n",
    "print(f\"   - LOF: {lof_local_detected}/5 rilevate\")\n",
    "\n",
    "# Metriche globali\n",
    "metrics_iso_c = calc_metrics(y_anomaly, y_pred_iso_c_bin)\n",
    "metrics_lof_c = calc_metrics(y_anomaly, y_pred_lof_c_bin)\n",
    "\n",
    "print(f\"\\nüìà Metriche complessive:\")\n",
    "print(f\"   Isolation Forest: F1={metrics_iso_c['F1']:.3f}\")\n",
    "print(f\"   LOF: F1={metrics_lof_c['F1']:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 4: Visualizzazione\n",
    "# ============================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Ground truth con tipi\n",
    "colors_type = {0: 'blue', 1: 'red', 2: 'orange'}\n",
    "colors_gt_c = [colors_type[y] for y in y_type]\n",
    "axes[0].scatter(X_compare[:, 0], X_compare[:, 1], c=colors_gt_c, s=20, alpha=0.7)\n",
    "axes[0].scatter([], [], c='red', label='Globali', s=50)\n",
    "axes[0].scatter([], [], c='orange', label='Locali', s=50)\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Ground Truth\\n(rosso=globali, arancione=locali)', fontsize=11)\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('Y')\n",
    "\n",
    "# Isolation Forest\n",
    "colors_iso_det = ['red' if y == 1 else 'blue' for y in y_pred_iso_c_bin]\n",
    "axes[1].scatter(X_compare[:, 0], X_compare[:, 1], c=colors_iso_det, s=20, alpha=0.7)\n",
    "axes[1].set_title(f'Isolation Forest\\nGlobali: {iso_global_detected}/5, Locali: {iso_local_detected}/5', fontsize=11)\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('Y')\n",
    "\n",
    "# LOF\n",
    "colors_lof_det = ['red' if y == 1 else 'blue' for y in y_pred_lof_c_bin]\n",
    "axes[2].scatter(X_compare[:, 0], X_compare[:, 1], c=colors_lof_det, s=20, alpha=0.7)\n",
    "axes[2].set_title(f'LOF\\nGlobali: {lof_global_detected}/5, Locali: {lof_local_detected}/5', fontsize=11)\n",
    "axes[2].set_xlabel('X')\n",
    "axes[2].set_ylabel('Y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\"\"\n",
    "üìå CONCLUSIONI:\n",
    "   - Isolation Forest: eccellente per anomalie GLOBALI\n",
    "   - LOF: migliore per anomalie LOCALI (considera densit√† vicini)\n",
    "   \n",
    "üí° REGOLA PRATICA:\n",
    "   - Usa Isolation Forest come default (veloce, robusto)\n",
    "   - Usa LOF se sospetti anomalie locali/contestuali\n",
    "   - Combina entrambi per massima copertura\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7bf020",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è Esercizio 26.3 ‚Äî Threshold Optimization\n",
    "\n",
    "**Obiettivo:** Trovare la soglia ottimale per bilanciare Precision e Recall.\n",
    "\n",
    "**Consegna:**\n",
    "1. Usa il dataset Credit Card Fraud dalla Demo 4\n",
    "2. Ottieni gli **anomaly scores** da Isolation Forest\n",
    "3. Testa **diverse soglie** (percentili 90, 95, 97, 99)\n",
    "4. Trova la soglia che massimizza **F1-Score**\n",
    "5. Visualizza la curva Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 26.3 ‚Äî SOLUZIONE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ESERCIZIO 26.3 ‚Äî Threshold Optimization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 1: Ottieni anomaly scores\n",
    "# ============================================\n",
    "# Riusiamo il dataset fraud e il modello gi√† trainato\n",
    "scores_opt = -iso_fraud.decision_function(X_fraud_scaled)  # Pi√π alto = pi√π anomalo\n",
    "\n",
    "print(f\"\\nüìä Distribuzione Anomaly Scores:\")\n",
    "print(f\"   - Min: {scores_opt.min():.3f}\")\n",
    "print(f\"   - Max: {scores_opt.max():.3f}\")\n",
    "print(f\"   - Mean: {scores_opt.mean():.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 2: Test diverse soglie\n",
    "# ============================================\n",
    "percentiles = [90, 92, 94, 95, 96, 97, 98, 99]\n",
    "results_thresh = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Test diverse soglie (percentili)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for p in percentiles:\n",
    "    threshold = np.percentile(scores_opt, p)\n",
    "    y_pred_thresh = (scores_opt >= threshold).astype(int)\n",
    "    \n",
    "    metrics = calc_metrics(y_fraud, y_pred_thresh)\n",
    "    \n",
    "    results_thresh.append({\n",
    "        'percentile': p,\n",
    "        'threshold': threshold,\n",
    "        'n_detected': y_pred_thresh.sum(),\n",
    "        'precision': metrics['Precision'],\n",
    "        'recall': metrics['Recall'],\n",
    "        'f1': metrics['F1']\n",
    "    })\n",
    "    \n",
    "    print(f\"   P{p}: thresh={threshold:.3f}, detected={y_pred_thresh.sum():3d}, \"\n",
    "          f\"P={metrics['Precision']:.2f}, R={metrics['Recall']:.2f}, F1={metrics['F1']:.2f}\")\n",
    "\n",
    "df_thresh = pd.DataFrame(results_thresh)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 3: Trova la soglia ottimale\n",
    "# ============================================\n",
    "best_idx = df_thresh['f1'].idxmax()\n",
    "best_percentile = df_thresh.loc[best_idx, 'percentile']\n",
    "best_threshold = df_thresh.loc[best_idx, 'threshold']\n",
    "best_f1_opt = df_thresh.loc[best_idx, 'f1']\n",
    "\n",
    "print(f\"\\nü•á SOGLIA OTTIMALE:\")\n",
    "print(f\"   - Percentile: {best_percentile}\")\n",
    "print(f\"   - Threshold: {best_threshold:.3f}\")\n",
    "print(f\"   - F1-Score: {best_f1_opt:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 4: Precision-Recall Curve\n",
    "# ============================================\n",
    "# Calcola curva P-R\n",
    "precision_curve, recall_curve, thresholds_curve = precision_recall_curve(y_fraud, scores_opt)\n",
    "\n",
    "# Calcola F1 per ogni punto\n",
    "f1_curve = 2 * precision_curve * recall_curve / (precision_curve + recall_curve + 1e-10)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 5: Visualizzazione\n",
    "# ============================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Metriche vs Percentile\n",
    "axes[0].plot(df_thresh['percentile'], df_thresh['precision'], 'bo-', label='Precision', linewidth=2)\n",
    "axes[0].plot(df_thresh['percentile'], df_thresh['recall'], 'go-', label='Recall', linewidth=2)\n",
    "axes[0].plot(df_thresh['percentile'], df_thresh['f1'], 'ro-', label='F1-Score', linewidth=2)\n",
    "axes[0].axvline(x=best_percentile, color='gray', linestyle='--', alpha=0.7)\n",
    "axes[0].set_xlabel('Percentile Threshold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Metriche vs Soglia')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Precision-Recall Curve\n",
    "axes[1].plot(recall_curve, precision_curve, 'b-', linewidth=2)\n",
    "axes[1].fill_between(recall_curve, precision_curve, alpha=0.3)\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Distribuzione scores con soglia ottimale\n",
    "axes[2].hist(scores_opt[y_fraud==0], bins=50, alpha=0.7, label='Normali', color='blue', density=True)\n",
    "axes[2].hist(scores_opt[y_fraud==1], bins=50, alpha=0.7, label='Frodi', color='red', density=True)\n",
    "axes[2].axvline(x=best_threshold, color='green', linestyle='--', linewidth=2, label=f'Soglia ottimale')\n",
    "axes[2].set_xlabel('Anomaly Score')\n",
    "axes[2].set_ylabel('Densit√†')\n",
    "axes[2].set_title('Distribuzione con Soglia Ottimale')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\"\"\n",
    "üìå INTERPRETAZIONE:\n",
    "   - Soglia bassa (P90): alta Recall, bassa Precision (molti falsi allarmi)\n",
    "   - Soglia alta (P99): bassa Recall, alta Precision (perse molte frodi)\n",
    "   - Soglia ottimale (P{best_percentile}): bilanciamento F1\n",
    "   \n",
    "üí° NEL BUSINESS:\n",
    "   - Se i falsi allarmi costano poco ‚Üí soglia bassa (alta Recall)\n",
    "   - Se investigare costa molto ‚Üí soglia alta (alta Precision)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b534fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ 5. Conclusione ‚Äî Cosa Portarsi a Casa\n",
    "\n",
    "## ‚úÖ Concetti Chiave Appresi\n",
    "\n",
    "| Concetto | Descrizione |\n",
    "|----------|-------------|\n",
    "| **Anomaly Detection** | Trovare punti che si discostano dal comportamento normale |\n",
    "| **Isolation Forest** | Isola anomalie con pochi split (veloce, robusto) |\n",
    "| **LOF** | Confronta densit√† locale vs vicini (buono per anomalie locali) |\n",
    "| **Contamination** | % attesa di anomalie, parametro critico |\n",
    "\n",
    "## üö® Errori Comuni da Evitare\n",
    "\n",
    "1. **Ignorare lo sbilanciamento** ‚Üí Non usare Accuracy, usa F1/PR-AUC\n",
    "2. **Contamination sbagliato** ‚Üí Stima accurata della % anomalie\n",
    "3. **Non validare** ‚Üí Sempre visualizzare e calcolare metriche\n",
    "4. **Un solo algoritmo** ‚Üí Ensemble spesso migliora\n",
    "\n",
    "## üí° Quando Usare Quale Algoritmo\n",
    "\n",
    "```\n",
    "ISOLATION FOREST:\n",
    "‚úÖ Default per la maggior parte dei casi\n",
    "‚úÖ Dataset grandi (scalabile)\n",
    "‚úÖ Anomalie globali\n",
    "‚úÖ Quando la velocit√† conta\n",
    "\n",
    "LOF:\n",
    "‚úÖ Anomalie locali/contestuali\n",
    "‚úÖ Cluster con densit√† diverse\n",
    "‚úÖ Dataset medio-piccoli\n",
    "‚úÖ Quando serve interpretabilit√† (LOF score = densit√†)\n",
    "\n",
    "ENSEMBLE:\n",
    "‚úÖ Quando vuoi massima copertura\n",
    "‚úÖ Budget computazionale permette\n",
    "‚úÖ Anomalie di tipi diversi\n",
    "```\n",
    "\n",
    "## üîó Ponte verso la Prossima Lezione\n",
    "\n",
    "Nella **Lezione 27** esploreremo **Unsupervised Feature Engineering**:\n",
    "- Come creare nuove feature usando tecniche unsupervised\n",
    "- Embedding e rappresentazioni latenti\n",
    "- Preparare i dati per modelli supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f77a58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìö 6. Bignami ‚Äî Scheda di Riferimento Rapido\n",
    "\n",
    "## üìñ Definizioni Essenziali\n",
    "\n",
    "| Termine | Definizione |\n",
    "|---------|-------------|\n",
    "| **Anomalia** | Osservazione che si discosta dal comportamento normale |\n",
    "| **Contamination** | Percentuale attesa di anomalie nel dataset |\n",
    "| **Anomaly Score** | Valore numerico che indica quanto un punto √® anomalo |\n",
    "| **Point Anomaly** | Singolo punto anomalo |\n",
    "| **Contextual Anomaly** | Anomalo solo in un contesto specifico |\n",
    "\n",
    "## üìê Algoritmi a Confronto\n",
    "\n",
    "| Algoritmo | Complessit√† | Anomalie Locali | Scalabilit√† |\n",
    "|-----------|-------------|-----------------|-------------|\n",
    "| Isolation Forest | O(n log n) | Media | Ottima |\n",
    "| LOF | O(n¬≤) | Eccellente | Media |\n",
    "| DBSCAN | O(n log n) | Buona | Buona |\n",
    "\n",
    "## üîß Template di Codice\n",
    "\n",
    "### Isolation Forest\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=0.05,  # 5% anomalie attese\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# -1 = anomalia, 1 = normale\n",
    "y_pred = iso.fit_predict(X)\n",
    "\n",
    "# Scores (pi√π basso = pi√π anomalo)\n",
    "scores = iso.decision_function(X)\n",
    "```\n",
    "\n",
    "### Local Outlier Factor\n",
    "```python\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=20,\n",
    "    contamination=0.05\n",
    ")\n",
    "\n",
    "y_pred = lof.fit_predict(X)\n",
    "\n",
    "# LOF scores (pi√π alto = pi√π anomalo)\n",
    "lof_scores = -lof.negative_outlier_factor_\n",
    "```\n",
    "\n",
    "### Valutazione\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "# Converti a binary (anomalia = 1)\n",
    "y_pred_binary = (y_pred == -1).astype(int)\n",
    "\n",
    "# Metriche\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "```\n",
    "\n",
    "## ‚úÖ Checklist Anomaly Detection\n",
    "\n",
    "- [ ] Definito cosa significa \"anomalia\" nel contesto?\n",
    "- [ ] Stimata la % di anomalie (contamination)?\n",
    "- [ ] Dati scalati (per LOF)?\n",
    "- [ ] Algoritmo appropriato scelto?\n",
    "- [ ] Soglia ottimizzata?\n",
    "- [ ] Metriche calcolate (P, R, F1)?\n",
    "- [ ] Risultati visualizzati e validati?\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Fine Lezione 26 ‚Äî Anomaly Detection Masterclass!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
