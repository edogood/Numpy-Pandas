{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd10b28e",
   "metadata": {},
   "source": [
    "# ğŸ“ Lezione 28 â€” Progetto End-to-End: Unsupervised Learning\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Obiettivo del Progetto\n",
    "\n",
    "> **CASO STUDIO**: Segmentazione clienti per un e-commerce\n",
    "\n",
    "Applicheremo TUTTE le tecniche apprese nel blocco Unsupervised Learning (Lezioni 19-27) in un progetto completo.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Tecniche Utilizzate\n",
    "\n",
    "| Lezione | Tecnica | Uso nel Progetto |\n",
    "|---------|---------|------------------|\n",
    "| 19 | Unsupervised Intro | Comprensione problema |\n",
    "| 20 | K-Means | Segmentazione primaria |\n",
    "| 21 | Scelta K | Numero segmenti ottimale |\n",
    "| 22 | Clustering Gerarchico | Validazione alternativa |\n",
    "| 23 | DBSCAN | Identificazione outlier naturali |\n",
    "| 24 | PCA | Riduzione dimensionalitÃ  |\n",
    "| 25 | PCA + Clustering | Visualizzazione segmenti |\n",
    "| 26 | Anomaly Detection | Clienti anomali |\n",
    "| 27 | Feature Engineering | Arricchimento dati |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ Struttura del Progetto\n",
    "\n",
    "1. **Business Understanding** â€” Definizione del problema\n",
    "2. **Data Loading & Exploration** â€” Caricamento e EDA\n",
    "3. **Data Preprocessing** â€” Pulizia e preparazione\n",
    "4. **Feature Engineering** â€” Creazione nuove features\n",
    "5. **Dimensionality Reduction** â€” PCA\n",
    "6. **Clustering** â€” Segmentazione\n",
    "7. **Anomaly Detection** â€” Identificazione outlier\n",
    "8. **Interpretation** â€” Interpretazione business\n",
    "9. **Deliverables** â€” Output finali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17041df0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“– FASE 1: Business Understanding\n",
    "\n",
    "## ğŸ¯ Il Problema\n",
    "\n",
    "Un e-commerce vuole **segmentare i propri clienti** per:\n",
    "- ğŸ“§ Marketing personalizzato\n",
    "- ğŸ’° Ottimizzazione budget advertising\n",
    "- ğŸ Offerte mirate per segmento\n",
    "- âš ï¸ Identificazione clienti a rischio churn\n",
    "\n",
    "## ğŸ“‹ Domande Business\n",
    "\n",
    "1. **Quanti segmenti** di clienti abbiamo?\n",
    "2. **Come si caratterizzano** i segmenti?\n",
    "3. **Chi sono i clienti anomali**?\n",
    "4. **Quale strategia** per ogni segmento?\n",
    "\n",
    "## ğŸ“Š Dati Disponibili\n",
    "\n",
    "Simuleremo un dataset RFM (Recency, Frequency, Monetary) con metriche comportamentali:\n",
    "\n",
    "| Feature | Descrizione |\n",
    "|---------|-------------|\n",
    "| `recency` | Giorni dall'ultimo acquisto |\n",
    "| `frequency` | Numero di acquisti negli ultimi 12 mesi |\n",
    "| `monetary` | Spesa totale negli ultimi 12 mesi |\n",
    "| `avg_basket` | Valore medio del carrello |\n",
    "| `tenure` | Mesi da cliente |\n",
    "| `online_ratio` | % acquisti online vs negozio |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š FASE 2: Data Loading & Exploration\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Caricare i dati e fare una prima esplorazione\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurazione\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“Š FASE 2: DATA LOADING & EXPLORATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2.1 CREAZIONE DATASET SIMULATO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ“¦ Creazione dataset clienti e-commerce simulato...\")\n",
    "\n",
    "n_customers = 1000\n",
    "\n",
    "# Simulo 4 segmenti naturali di clienti\n",
    "segments = {\n",
    "    'VIP': {'n': 100, 'recency': (5, 10), 'frequency': (20, 30), 'monetary': (5000, 10000)},\n",
    "    'Regular': {'n': 350, 'recency': (15, 30), 'frequency': (8, 15), 'monetary': (1000, 3000)},\n",
    "    'Occasional': {'n': 350, 'recency': (45, 90), 'frequency': (2, 5), 'monetary': (200, 800)},\n",
    "    'Churned': {'n': 200, 'recency': (180, 365), 'frequency': (1, 3), 'monetary': (50, 300)}\n",
    "}\n",
    "\n",
    "data_list = []\n",
    "for seg_name, params in segments.items():\n",
    "    n = params['n']\n",
    "    data_list.append(pd.DataFrame({\n",
    "        'customer_id': range(len(data_list) * 1000, len(data_list) * 1000 + n),\n",
    "        'recency': np.random.uniform(*params['recency'], n),\n",
    "        'frequency': np.random.uniform(*params['frequency'], n),\n",
    "        'monetary': np.random.uniform(*params['monetary'], n),\n",
    "        'avg_basket': np.random.uniform(30, 150, n),\n",
    "        'tenure': np.random.uniform(6, 60, n),\n",
    "        'online_ratio': np.random.uniform(0.2, 0.9, n),\n",
    "        '_true_segment': seg_name  # Solo per validazione, non lo useremo\n",
    "    }))\n",
    "\n",
    "df = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# Aggiungo un po' di rumore e outlier\n",
    "noise_idx = np.random.choice(len(df), size=50, replace=False)\n",
    "df.loc[noise_idx[:20], 'monetary'] *= 3  # Super spenditori\n",
    "df.loc[noise_idx[20:35], 'recency'] = np.random.uniform(400, 500, 15)  # Super inattivi\n",
    "df.loc[noise_idx[35:], 'frequency'] *= 0.1  # Frequenza anomala\n",
    "\n",
    "# Shuffle\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Dataset creato: {df.shape[0]} clienti, {df.shape[1]} features\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2.2 PRIMA ESPLORAZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ“‹ STRUTTURA DATASET\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(f\"\\nğŸ“ Shape: {df.shape}\")\n",
    "print(f\"\\nğŸ“Š Tipi dati:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Statistiche descrittive:\")\n",
    "print(df.describe().round(2))\n",
    "\n",
    "# Features da usare per clustering (escludo customer_id e _true_segment)\n",
    "features_to_use = ['recency', 'frequency', 'monetary', 'avg_basket', 'tenure', 'online_ratio']\n",
    "df_features = df[features_to_use].copy()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2.3 VISUALIZZAZIONE EDA\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for idx, col in enumerate(features_to_use):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    ax.hist(df[col], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(df[col].mean(), color='red', linestyle='--', label=f'Mean: {df[col].mean():.1f}')\n",
    "    ax.axvline(df[col].median(), color='green', linestyle='--', label=f'Median: {df[col].median():.1f}')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Frequenza')\n",
    "    ax.set_title(f'ğŸ“Š Distribuzione {col}')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('ğŸ“Š EDA: Distribuzione Features', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "corr_matrix = df_features.corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, ax=ax, square=True)\n",
    "ax.set_title('ğŸ”— Matrice di Correlazione')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… FASE 2 COMPLETATA!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”§ FASE 3: Data Preprocessing\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Pulire e preparare i dati per il clustering\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”§ FASE 3: DATA PREPROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3.1 CHECK VALORI MANCANTI\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ“‹ Check valori mancanti:\")\n",
    "missing = df_features.isnull().sum()\n",
    "print(missing)\n",
    "print(f\"\\nâœ… Nessun valore mancante!\" if missing.sum() == 0 else \"âš ï¸ Valori mancanti presenti!\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3.2 CHECK OUTLIER CON IQR\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ“Š CHECK OUTLIER (metodo IQR)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def count_outliers_iqr(series, k=1.5):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - k * IQR\n",
    "    upper = Q3 + k * IQR\n",
    "    return ((series < lower) | (series > upper)).sum()\n",
    "\n",
    "print(\"\\nğŸ“ˆ Outlier per feature:\")\n",
    "for col in features_to_use:\n",
    "    n_outliers = count_outliers_iqr(df_features[col])\n",
    "    pct = n_outliers / len(df_features) * 100\n",
    "    print(f\"  {col}: {n_outliers} ({pct:.1f}%)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3.3 SCALING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ“ SCALING\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Uso RobustScaler per robustezza agli outlier\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(df_features)\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=features_to_use)\n",
    "\n",
    "print(\"\\nğŸ“Š Statistiche dopo RobustScaler:\")\n",
    "print(df_scaled.describe().round(3))\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3.4 VISUALIZZAZIONE PRIMA/DOPO SCALING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Prima\n",
    "ax1 = axes[0]\n",
    "df_features.boxplot(ax=ax1)\n",
    "ax1.set_title('ğŸ“Š Distribuzione PRIMA dello Scaling')\n",
    "ax1.set_xticklabels(features_to_use, rotation=45, ha='right')\n",
    "\n",
    "# Dopo\n",
    "ax2 = axes[1]\n",
    "df_scaled.boxplot(ax=ax2)\n",
    "ax2.set_title('ğŸ“Š Distribuzione DOPO RobustScaler')\n",
    "ax2.set_xticklabels(features_to_use, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… FASE 3 COMPLETATA!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936abf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ” FASE 4: Dimensionality Reduction (PCA)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Ridurre dimensionalitÃ  e visualizzare i dati\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ” FASE 4: DIMENSIONALITY REDUCTION (PCA)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4.1 PCA COMPLETA PER ANALISI\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Varianza spiegata\n",
    "print(\"\\nğŸ“ˆ Varianza spiegata per componente:\")\n",
    "cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "for i, (var, cum) in enumerate(zip(pca_full.explained_variance_ratio_, cumsum)):\n",
    "    print(f\"  PC{i+1}: {var:.1%} (cumulata: {cum:.1%})\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4.2 SCELTA NUMERO COMPONENTI\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Scelgo componenti che spiegano almeno 80% varianza\n",
    "n_components_80 = np.argmax(cumsum >= 0.80) + 1\n",
    "n_components_90 = np.argmax(cumsum >= 0.90) + 1\n",
    "\n",
    "print(f\"\\nğŸ“Š Componenti necessarie:\")\n",
    "print(f\"  Per 80% varianza: {n_components_80}\")\n",
    "print(f\"  Per 90% varianza: {n_components_90}\")\n",
    "\n",
    "# Per visualizzazione uso 2, per clustering uso 3\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_3d = PCA(n_components=3)\n",
    "\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nâœ… PCA 2D: {pca_2d.explained_variance_ratio_.sum():.1%} varianza spiegata\")\n",
    "print(f\"âœ… PCA 3D: {pca_3d.explained_variance_ratio_.sum():.1%} varianza spiegata\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4.3 ANALISI LOADINGS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ“Š LOADINGS (importanza features per componente)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "loadings = pd.DataFrame(\n",
    "    pca_2d.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=features_to_use\n",
    ")\n",
    "print(loadings.round(3))\n",
    "\n",
    "print(\"\\nğŸ“ Interpretazione:\")\n",
    "pc1_top = loadings['PC1'].abs().idxmax()\n",
    "pc2_top = loadings['PC2'].abs().idxmax()\n",
    "print(f\"  PC1 dominato da: {pc1_top}\")\n",
    "print(f\"  PC2 dominato da: {pc2_top}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4.4 VISUALIZZAZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Scree plot\n",
    "ax1 = axes[0]\n",
    "ax1.bar(range(1, len(pca_full.explained_variance_ratio_)+1), \n",
    "        pca_full.explained_variance_ratio_, alpha=0.7, label='Individuale')\n",
    "ax1.plot(range(1, len(cumsum)+1), cumsum, 'ro-', label='Cumulata')\n",
    "ax1.axhline(y=0.8, color='g', linestyle='--', label='80%')\n",
    "ax1.axhline(y=0.9, color='orange', linestyle='--', label='90%')\n",
    "ax1.set_xlabel('Componente')\n",
    "ax1.set_ylabel('Varianza Spiegata')\n",
    "ax1.set_title('ğŸ“Š Scree Plot')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Dati in spazio PCA 2D\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], alpha=0.5, s=20, c='steelblue')\n",
    "ax2.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})')\n",
    "ax2.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})')\n",
    "ax2.set_title('ğŸ“ Clienti in Spazio PCA 2D')\n",
    "\n",
    "# Plot 3: Loadings biplot\n",
    "ax3 = axes[2]\n",
    "ax3.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], alpha=0.2, s=10, c='gray')\n",
    "# Aggiungo vettori loadings\n",
    "scale = 3  # Fattore di scala per visualizzazione\n",
    "for i, feature in enumerate(features_to_use):\n",
    "    ax3.arrow(0, 0, loadings.loc[feature, 'PC1']*scale, loadings.loc[feature, 'PC2']*scale,\n",
    "              head_width=0.1, head_length=0.05, fc='red', ec='red')\n",
    "    ax3.text(loadings.loc[feature, 'PC1']*scale*1.1, loadings.loc[feature, 'PC2']*scale*1.1,\n",
    "             feature, fontsize=9, ha='center')\n",
    "ax3.set_xlabel('PC1')\n",
    "ax3.set_ylabel('PC2')\n",
    "ax3.set_title('ğŸ“Š Biplot: Loadings + Dati')\n",
    "ax3.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax3.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… FASE 4 COMPLETATA!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7097d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ¯ FASE 5: Scelta del Numero di Cluster\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Determinare il numero ottimale di segmenti\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ¯ FASE 5: SCELTA DEL NUMERO DI CLUSTER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5.1 METODO DEL GOMITO (ELBOW)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ“Š Calcolo metriche per K = 2 a 10...\")\n",
    "\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "calinski = []\n",
    "davies = []\n",
    "\n",
    "for k in k_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_scaled)\n",
    "    \n",
    "    inertias.append(km.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, labels))\n",
    "    calinski.append(calinski_harabasz_score(X_scaled, labels))\n",
    "    davies.append(davies_bouldin_score(X_scaled, labels))\n",
    "    \n",
    "    print(f\"  K={k}: Silhouette={silhouettes[-1]:.3f}, Inertia={inertias[-1]:.0f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5.2 IDENTIFICAZIONE K OTTIMALE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "best_k_silhouette = k_range[np.argmax(silhouettes)]\n",
    "best_k_calinski = k_range[np.argmax(calinski)]\n",
    "best_k_davies = k_range[np.argmin(davies)]\n",
    "\n",
    "print(f\"\\nğŸ“Š K ottimale secondo diverse metriche:\")\n",
    "print(f\"  Silhouette (max): K = {best_k_silhouette}\")\n",
    "print(f\"  Calinski-Harabasz (max): K = {best_k_calinski}\")\n",
    "print(f\"  Davies-Bouldin (min): K = {best_k_davies}\")\n",
    "\n",
    "# Decisione finale (considerando anche il business context: 4 segmenti ha senso)\n",
    "k_optimal = 4\n",
    "print(f\"\\nâœ… Scelta finale: K = {k_optimal} (bilanciando metriche e interpretabilitÃ )\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5.3 VISUALIZZAZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Elbow\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(list(k_range), inertias, 'bo-', linewidth=2)\n",
    "ax1.axvline(x=k_optimal, color='red', linestyle='--', label=f'K scelto = {k_optimal}')\n",
    "ax1.set_xlabel('Numero di Cluster (K)')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('ğŸ“Š Metodo del Gomito (Elbow)')\n",
    "ax1.legend()\n",
    "\n",
    "# Silhouette\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(list(k_range), silhouettes, 'go-', linewidth=2)\n",
    "ax2.axvline(x=k_optimal, color='red', linestyle='--', label=f'K scelto = {k_optimal}')\n",
    "ax2.set_xlabel('Numero di Cluster (K)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('ğŸ“Š Silhouette Score (piÃ¹ alto = meglio)')\n",
    "ax2.legend()\n",
    "\n",
    "# Calinski-Harabasz\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(list(k_range), calinski, 'mo-', linewidth=2)\n",
    "ax3.axvline(x=k_optimal, color='red', linestyle='--', label=f'K scelto = {k_optimal}')\n",
    "ax3.set_xlabel('Numero di Cluster (K)')\n",
    "ax3.set_ylabel('Calinski-Harabasz Score')\n",
    "ax3.set_title('ğŸ“Š Calinski-Harabasz (piÃ¹ alto = meglio)')\n",
    "ax3.legend()\n",
    "\n",
    "# Davies-Bouldin\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(list(k_range), davies, 'co-', linewidth=2)\n",
    "ax4.axvline(x=k_optimal, color='red', linestyle='--', label=f'K scelto = {k_optimal}')\n",
    "ax4.set_xlabel('Numero di Cluster (K)')\n",
    "ax4.set_ylabel('Davies-Bouldin Score')\n",
    "ax4.set_title('ğŸ“Š Davies-Bouldin (piÃ¹ basso = meglio)')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… FASE 5 COMPLETATA!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a3b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ¯ FASE 6: Clustering (K-Means)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Segmentare i clienti con K-Means\n",
    "\"\"\"\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ¯ FASE 6: CLUSTERING (K-MEANS)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 6.1 K-MEANS FINALE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Aggiungo al dataframe\n",
    "df['cluster'] = cluster_labels\n",
    "df_features['cluster'] = cluster_labels\n",
    "df_scaled['cluster'] = cluster_labels\n",
    "\n",
    "print(f\"\\nğŸ“Š Distribuzione cluster:\")\n",
    "cluster_counts = df['cluster'].value_counts().sort_index()\n",
    "for c, count in cluster_counts.items():\n",
    "    print(f\"  Cluster {c}: {count} clienti ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 6.2 PROFILING DEI CLUSTER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ“Š PROFILING DEI CLUSTER\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Medie per cluster\n",
    "cluster_profiles = df_features.groupby('cluster')[features_to_use].mean()\n",
    "print(\"\\nğŸ“ˆ Valori medi per cluster:\")\n",
    "print(cluster_profiles.round(2))\n",
    "\n",
    "# Normalizzato rispetto alla media globale\n",
    "global_means = df_features[features_to_use].mean()\n",
    "cluster_profiles_norm = (cluster_profiles - global_means) / global_means * 100\n",
    "print(\"\\nğŸ“ˆ Differenza % dalla media globale:\")\n",
    "print(cluster_profiles_norm.round(1))\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 6.3 NAMING DEI CLUSTER (INTERPRETAZIONE BUSINESS)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ·ï¸ NAMING DEI CLUSTER\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Ordino cluster per monetary (valore cliente)\n",
    "cluster_order = cluster_profiles['monetary'].sort_values(ascending=False).index.tolist()\n",
    "\n",
    "cluster_names = {}\n",
    "name_mapping = ['ğŸŒŸ VIP', 'ğŸ’¼ Regular', 'ğŸ¯ Occasional', 'âš ï¸ At Risk']\n",
    "\n",
    "for i, cluster_id in enumerate(cluster_order):\n",
    "    cluster_names[cluster_id] = name_mapping[i]\n",
    "    profile = cluster_profiles.loc[cluster_id]\n",
    "    print(f\"\\n{name_mapping[i]} (Cluster {cluster_id}):\")\n",
    "    print(f\"  Recency: {profile['recency']:.1f} giorni\")\n",
    "    print(f\"  Frequency: {profile['frequency']:.1f} acquisti\")\n",
    "    print(f\"  Monetary: â‚¬{profile['monetary']:.0f}\")\n",
    "    print(f\"  Clienti: {cluster_counts[cluster_id]}\")\n",
    "\n",
    "# Aggiungo nomi al dataframe\n",
    "df['segment_name'] = df['cluster'].map(cluster_names)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 6.4 VISUALIZZAZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Cluster in spazio PCA\n",
    "ax1 = axes[0, 0]\n",
    "colors = ['gold', 'steelblue', 'green', 'red']\n",
    "for c in range(k_optimal):\n",
    "    mask = cluster_labels == c\n",
    "    ax1.scatter(X_pca_2d[mask, 0], X_pca_2d[mask, 1], \n",
    "                c=colors[c], label=cluster_names[c], alpha=0.6, s=30)\n",
    "# Centroidi\n",
    "centroids_pca = pca_2d.transform(kmeans.cluster_centers_)\n",
    "ax1.scatter(centroids_pca[:, 0], centroids_pca[:, 1], \n",
    "            c='black', marker='X', s=200, edgecolors='white', linewidths=2)\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.set_title('ğŸ¯ Segmenti Clienti in Spazio PCA')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Radar chart profili\n",
    "ax2 = axes[0, 1]\n",
    "categories = features_to_use\n",
    "N = len(categories)\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Normalizzo per radar (0-1)\n",
    "cluster_profiles_radar = (cluster_profiles - cluster_profiles.min()) / (cluster_profiles.max() - cluster_profiles.min())\n",
    "\n",
    "for c in range(k_optimal):\n",
    "    values = cluster_profiles_radar.loc[c].values.tolist()\n",
    "    values += values[:1]\n",
    "    ax2.plot(angles, values, 'o-', linewidth=2, label=cluster_names[c], color=colors[c])\n",
    "    ax2.fill(angles, values, alpha=0.1, color=colors[c])\n",
    "\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(categories, fontsize=9)\n",
    "ax2.set_title('ğŸ“Š Profili Cluster (Radar Chart)')\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1))\n",
    "\n",
    "# Plot 3: Box plot monetary per cluster\n",
    "ax3 = axes[1, 0]\n",
    "df.boxplot(column='monetary', by='segment_name', ax=ax3)\n",
    "ax3.set_xlabel('Segmento')\n",
    "ax3.set_ylabel('Monetary (â‚¬)')\n",
    "ax3.set_title('ğŸ’° Distribuzione Monetary per Segmento')\n",
    "plt.suptitle('')\n",
    "\n",
    "# Plot 4: Scatter frequency vs monetary\n",
    "ax4 = axes[1, 1]\n",
    "for c in range(k_optimal):\n",
    "    mask = df['cluster'] == c\n",
    "    ax4.scatter(df.loc[mask, 'frequency'], df.loc[mask, 'monetary'],\n",
    "                c=colors[c], label=cluster_names[c], alpha=0.5, s=30)\n",
    "ax4.set_xlabel('Frequency (acquisti)')\n",
    "ax4.set_ylabel('Monetary (â‚¬)')\n",
    "ax4.set_title('ğŸ“Š Frequency vs Monetary per Segmento')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… FASE 6 COMPLETATA!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722f5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ” FASE 7: Anomaly Detection\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Identificare clienti anomali all'interno di ogni segmento\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ” FASE 7: ANOMALY DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 7.1 ISOLATION FOREST GLOBALE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸŒ² Isolation Forest (anomalie globali)...\")\n",
    "\n",
    "iso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
    "iso_pred = iso.fit_predict(X_scaled)\n",
    "iso_score = iso.decision_function(X_scaled)\n",
    "\n",
    "df['iso_anomaly'] = (iso_pred == -1).astype(int)\n",
    "df['iso_score'] = iso_score\n",
    "\n",
    "n_anomalies = df['iso_anomaly'].sum()\n",
    "print(f\"\\nâš ï¸ Anomalie globali: {n_anomalies} clienti ({n_anomalies/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Distribuzione anomalie per cluster\n",
    "print(\"\\nğŸ“Š Anomalie per segmento:\")\n",
    "anomaly_by_segment = df.groupby('segment_name')['iso_anomaly'].agg(['sum', 'mean'])\n",
    "anomaly_by_segment.columns = ['N Anomalie', '% Anomalie']\n",
    "anomaly_by_segment['% Anomalie'] = anomaly_by_segment['% Anomalie'] * 100\n",
    "print(anomaly_by_segment.round(1))\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 7.2 LOF PER ANOMALIE LOCALI\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ” Local Outlier Factor (anomalie locali)...\")\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "lof_pred = lof.fit_predict(X_scaled)\n",
    "lof_score = -lof.negative_outlier_factor_\n",
    "\n",
    "df['lof_anomaly'] = (lof_pred == -1).astype(int)\n",
    "df['lof_score'] = lof_score\n",
    "\n",
    "n_lof_anomalies = df['lof_anomaly'].sum()\n",
    "print(f\"\\nâš ï¸ Anomalie LOF: {n_lof_anomalies} clienti ({n_lof_anomalies/len(df)*100:.1f}%)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 7.3 CONSENSUS ANOMALIES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ¤ Consensus Anomalies (entrambi gli algoritmi)...\")\n",
    "\n",
    "df['consensus_anomaly'] = ((df['iso_anomaly'] == 1) & (df['lof_anomaly'] == 1)).astype(int)\n",
    "n_consensus = df['consensus_anomaly'].sum()\n",
    "print(f\"\\nâš ï¸ Anomalie concordi: {n_consensus} clienti\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 7.4 ANALISI ANOMALIE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ“Š PROFILO CLIENTI ANOMALI\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Confronto anomali vs normali\n",
    "normal_mask = df['iso_anomaly'] == 0\n",
    "anomaly_mask = df['iso_anomaly'] == 1\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Normali': df.loc[normal_mask, features_to_use].mean(),\n",
    "    'Anomali': df.loc[anomaly_mask, features_to_use].mean()\n",
    "})\n",
    "comparison['Diff %'] = ((comparison['Anomali'] - comparison['Normali']) / comparison['Normali'] * 100)\n",
    "print(comparison.round(2))\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 7.5 VISUALIZZAZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Anomalie in spazio PCA\n",
    "ax1 = axes[0, 0]\n",
    "normal = df['iso_anomaly'] == 0\n",
    "ax1.scatter(X_pca_2d[normal, 0], X_pca_2d[normal, 1], c='blue', alpha=0.3, s=20, label='Normale')\n",
    "ax1.scatter(X_pca_2d[~normal, 0], X_pca_2d[~normal, 1], c='red', alpha=0.8, s=50, marker='X', label='Anomalia')\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.set_title('ğŸ” Anomalie in Spazio PCA')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: ISO vs LOF scores\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['red' if c == 1 else 'blue' for c in df['consensus_anomaly']]\n",
    "ax2.scatter(df['iso_score'], df['lof_score'], c=colors, alpha=0.5, s=20)\n",
    "ax2.set_xlabel('Isolation Forest Score')\n",
    "ax2.set_ylabel('LOF Score')\n",
    "ax2.set_title('ğŸ“Š ISO vs LOF Scores (rosso = consensus)')\n",
    "ax2.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.axhline(y=np.percentile(df['lof_score'], 95), color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 3: Anomalie per segmento\n",
    "ax3 = axes[1, 0]\n",
    "segments = df['segment_name'].unique()\n",
    "n_normal = []\n",
    "n_anomaly = []\n",
    "for seg in segments:\n",
    "    mask = df['segment_name'] == seg\n",
    "    n_normal.append((df.loc[mask, 'iso_anomaly'] == 0).sum())\n",
    "    n_anomaly.append((df.loc[mask, 'iso_anomaly'] == 1).sum())\n",
    "\n",
    "x = np.arange(len(segments))\n",
    "width = 0.35\n",
    "ax3.bar(x - width/2, n_normal, width, label='Normali', color='steelblue')\n",
    "ax3.bar(x + width/2, n_anomaly, width, label='Anomalie', color='red')\n",
    "ax3.set_xlabel('Segmento')\n",
    "ax3.set_ylabel('N Clienti')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(segments, rotation=45, ha='right')\n",
    "ax3.set_title('ğŸ“Š Distribuzione Anomalie per Segmento')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Box plot monetary anomali vs normali\n",
    "ax4 = axes[1, 1]\n",
    "df.boxplot(column='monetary', by='iso_anomaly', ax=ax4)\n",
    "ax4.set_xticklabels(['Normale', 'Anomalia'])\n",
    "ax4.set_xlabel('Tipo')\n",
    "ax4.set_ylabel('Monetary (â‚¬)')\n",
    "ax4.set_title('ğŸ’° Monetary: Normali vs Anomalie')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top 5 anomalie\n",
    "print(\"\\nğŸ“‹ Top 5 Anomalie (per score):\")\n",
    "top_anomalies = df[df['iso_anomaly'] == 1].nsmallest(5, 'iso_score')[\n",
    "    ['customer_id', 'segment_name'] + features_to_use + ['iso_score']\n",
    "]\n",
    "print(top_anomalies.round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… FASE 7 COMPLETATA!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“Š FASE 8: Validazione con Approcci Alternativi\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Validare i risultati con altri algoritmi di clustering\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“Š FASE 8: VALIDAZIONE CON APPROCCI ALTERNATIVI\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 8.1 CLUSTERING GERARCHICO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸŒ³ Clustering Gerarchico (Agglomerative)...\")\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=k_optimal, linkage='ward')\n",
    "agg_labels = agg.fit_predict(X_scaled)\n",
    "\n",
    "# Confronto con K-Means\n",
    "ari_agg = adjusted_rand_score(cluster_labels, agg_labels)\n",
    "nmi_agg = normalized_mutual_info_score(cluster_labels, agg_labels)\n",
    "\n",
    "print(f\"\\nğŸ“Š Accordo con K-Means:\")\n",
    "print(f\"  Adjusted Rand Index: {ari_agg:.3f}\")\n",
    "print(f\"  Normalized Mutual Information: {nmi_agg:.3f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 8.2 DBSCAN\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ¯ DBSCAN...\")\n",
    "\n",
    "# Stima eps con k-distance\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=5)\n",
    "nn.fit(X_scaled)\n",
    "distances, _ = nn.kneighbors(X_scaled)\n",
    "k_distances = np.sort(distances[:, -1])\n",
    "\n",
    "# Eps stimato dal \"gomito\"\n",
    "eps_estimated = np.percentile(k_distances, 95)\n",
    "print(f\"  Eps stimato: {eps_estimated:.2f}\")\n",
    "\n",
    "dbscan = DBSCAN(eps=eps_estimated, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = (dbscan_labels == -1).sum()\n",
    "\n",
    "print(f\"\\nğŸ“Š DBSCAN Results:\")\n",
    "print(f\"  N cluster trovati: {n_clusters_dbscan}\")\n",
    "print(f\"  Noise points: {n_noise} ({n_noise/len(dbscan_labels)*100:.1f}%)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 8.3 CONFRONTO VISUALE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# K-Means\n",
    "ax1 = axes[0]\n",
    "scatter1 = ax1.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6, s=30)\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.set_title(f'ğŸ¯ K-Means (K={k_optimal})')\n",
    "plt.colorbar(scatter1, ax=ax1)\n",
    "\n",
    "# Hierarchical\n",
    "ax2 = axes[1]\n",
    "scatter2 = ax2.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=agg_labels, cmap='viridis', alpha=0.6, s=30)\n",
    "ax2.set_xlabel('PC1')\n",
    "ax2.set_ylabel('PC2')\n",
    "ax2.set_title(f'ğŸŒ³ Hierarchical (K={k_optimal})')\n",
    "plt.colorbar(scatter2, ax=ax2)\n",
    "\n",
    "# DBSCAN\n",
    "ax3 = axes[2]\n",
    "colors_db = ['gray' if l == -1 else plt.cm.viridis(l / max(dbscan_labels.max(), 1)) for l in dbscan_labels]\n",
    "ax3.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=colors_db, alpha=0.6, s=30)\n",
    "ax3.set_xlabel('PC1')\n",
    "ax3.set_ylabel('PC2')\n",
    "ax3.set_title(f'ğŸ¯ DBSCAN (K={n_clusters_dbscan}, noise={n_noise})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… I risultati K-Means sono confermati dal clustering gerarchico!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… FASE 8 COMPLETATA!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“‹ FASE 9: Interpretazione Business e Raccomandazioni\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Tradurre i risultati in insight actionable per il business\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“‹ FASE 9: INTERPRETAZIONE BUSINESS E RACCOMANDAZIONI\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 9.1 SUMMARY SEGMENTI\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ“Š SUMMARY FINALE SEGMENTI\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Calcolo metriche per ogni segmento\n",
    "segment_summary = df.groupby('segment_name').agg({\n",
    "    'customer_id': 'count',\n",
    "    'recency': 'mean',\n",
    "    'frequency': 'mean',\n",
    "    'monetary': ['mean', 'sum'],\n",
    "    'avg_basket': 'mean',\n",
    "    'tenure': 'mean',\n",
    "    'online_ratio': 'mean',\n",
    "    'iso_anomaly': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "segment_summary.columns = ['N_Clienti', 'Recency_Medio', 'Frequency_Media', \n",
    "                           'Monetary_Medio', 'Monetary_Totale', 'Basket_Medio',\n",
    "                           'Tenure_Medio', 'Online_Ratio', 'N_Anomalie']\n",
    "\n",
    "# Calcolo % del fatturato\n",
    "segment_summary['%_Fatturato'] = (segment_summary['Monetary_Totale'] / \n",
    "                                   segment_summary['Monetary_Totale'].sum() * 100).round(1)\n",
    "\n",
    "print(segment_summary)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 9.2 RACCOMANDAZIONI PER SEGMENTO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ’¡ RACCOMANDAZIONI STRATEGICHE\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "recommendations = {\n",
    "    'ğŸŒŸ VIP': {\n",
    "        'Descrizione': 'Clienti premium con alta frequenza e spesa',\n",
    "        'Strategia': 'RETAIN - Mantenere loyalty',\n",
    "        'Azioni': [\n",
    "            'âœ… Programma VIP esclusivo',\n",
    "            'âœ… Early access nuovi prodotti',\n",
    "            'âœ… Customer success manager dedicato',\n",
    "            'âœ… Offerte personalizzate premium'\n",
    "        ]\n",
    "    },\n",
    "    'ğŸ’¼ Regular': {\n",
    "        'Descrizione': 'Clienti attivi con potenziale di crescita',\n",
    "        'Strategia': 'GROW - Aumentare valore',\n",
    "        'Azioni': [\n",
    "            'âœ… Cross-selling prodotti complementari',\n",
    "            'âœ… Programma referral con incentivi',\n",
    "            'âœ… Upgrade a VIP con soglie raggiungibili',\n",
    "            'âœ… Newsletter personalizzate'\n",
    "        ]\n",
    "    },\n",
    "    'ğŸ¯ Occasional': {\n",
    "        'Descrizione': 'Acquisti sporadici, basso engagement',\n",
    "        'Strategia': 'ACTIVATE - Aumentare frequenza',\n",
    "        'Azioni': [\n",
    "            'âœ… Campagne riattivazione con sconto',\n",
    "            'âœ… Reminder carrello abbandonato',\n",
    "            'âœ… Offerte time-limited',\n",
    "            'âœ… Sondaggio preferenze'\n",
    "        ]\n",
    "    },\n",
    "    'âš ï¸ At Risk': {\n",
    "        'Descrizione': 'Clienti inattivi a rischio churn',\n",
    "        'Strategia': 'WIN-BACK - Riconquistare',\n",
    "        'Azioni': [\n",
    "            'âœ… Campagna win-back aggressiva',\n",
    "            'âœ… Survey motivi inattivitÃ ',\n",
    "            'âœ… Offerta \"ci manchi\" con super sconto',\n",
    "            'âœ… Considerare costo/beneficio retention'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for segment, rec in recommendations.items():\n",
    "    print(f\"\\n{segment}\")\n",
    "    print(f\"  ğŸ“ {rec['Descrizione']}\")\n",
    "    print(f\"  ğŸ¯ Strategia: {rec['Strategia']}\")\n",
    "    print(f\"  ğŸ“‹ Azioni:\")\n",
    "    for action in rec['Azioni']:\n",
    "        print(f\"      {action}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 9.3 ANALISI ANOMALIE - CASI SPECIALI\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"âš ï¸ GESTIONE ANOMALIE\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ” Clienti anomali identificati richiedono attenzione speciale:\n",
    "\n",
    "1. SUPER SPENDITORI ANOMALI (monetary >> media segmento)\n",
    "   â†’ Verificare autenticitÃ  transazioni\n",
    "   â†’ Potenziali candidati per upgrade VIP+\n",
    "   â†’ Analisi comportamento acquisto\n",
    "\n",
    "2. FREQUENZA ANOMALA (pattern irregolari)\n",
    "   â†’ Verificare account fraud\n",
    "   â†’ Analisi temporale acquisti\n",
    "   â†’ Possibili multi-account\n",
    "\n",
    "3. INATTIVITÃ€ ESTREMA (recency >>> media At Risk)\n",
    "   â†’ Candidati per rimozione da mailing list\n",
    "   â†’ Ultimo tentativo win-back\n",
    "   â†’ Archivio clienti \"dormienti\"\n",
    "\"\"\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 9.4 KPI E METRICHE DI SUCCESSO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ“Š KPI PER MONITORAGGIO\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“ˆ KPI da tracciare per ogni segmento:\n",
    "\n",
    "| Segmento    | KPI Primario        | KPI Secondario     | Target        |\n",
    "|-------------|---------------------|--------------------| --------------|\n",
    "| ğŸŒŸ VIP      | Retention Rate      | NPS Score          | >95% / >70    |\n",
    "| ğŸ’¼ Regular  | Upgrade Rate a VIP  | Avg Basket Growth  | >5% / +10%    |\n",
    "| ğŸ¯ Occasional| Activation Rate    | Frequency Growth   | >20% / +50%   |\n",
    "| âš ï¸ At Risk  | Win-back Rate       | Reactivation Cost  | >10% / <â‚¬20   |\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… FASE 9 COMPLETATA!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da7cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ“¦ FASE 10: Deliverables Finali\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Preparare output pronti per il business\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“¦ FASE 10: DELIVERABLES FINALI\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 10.1 DATASET ARRICCHITO FINALE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ“‹ Dataset finale con segmentazione...\")\n",
    "\n",
    "# Preparo dataset finale\n",
    "df_final = df[['customer_id', 'segment_name', 'cluster'] + features_to_use + \n",
    "              ['iso_anomaly', 'iso_score']].copy()\n",
    "\n",
    "# Aggiungo distanza dal centroide (proxy per \"tipicitÃ \")\n",
    "distances_to_centroid = cdist(X_scaled, kmeans.cluster_centers_)\n",
    "df_final['dist_to_centroid'] = [distances_to_centroid[i, cluster_labels[i]] \n",
    "                                 for i in range(len(df))]\n",
    "\n",
    "# Aggiungo componenti PCA\n",
    "df_final['PC1'] = X_pca_2d[:, 0]\n",
    "df_final['PC2'] = X_pca_2d[:, 1]\n",
    "\n",
    "print(f\"\\nğŸ“ Shape dataset finale: {df_final.shape}\")\n",
    "print(f\"\\nğŸ“‹ Colonne:\")\n",
    "for col in df_final.columns:\n",
    "    print(f\"  â€¢ {col}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 10.2 EXPORT FUNZIONE PREDIZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ”§ FUNZIONE DI SCORING PER NUOVI CLIENTI\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def score_new_customer(customer_data, scaler, kmeans, iso_forest, pca, cluster_names):\n",
    "    \"\"\"\n",
    "    Assegna un nuovo cliente al segmento appropriato.\n",
    "    \n",
    "    Args:\n",
    "        customer_data: dict con features del cliente\n",
    "        scaler: RobustScaler fittato\n",
    "        kmeans: modello K-Means fittato\n",
    "        iso_forest: modello IsolationForest fittato\n",
    "        pca: modello PCA fittato\n",
    "        cluster_names: dizionario mapping cluster -> nome\n",
    "        \n",
    "    Returns:\n",
    "        dict con risultati scoring\n",
    "    \"\"\"\n",
    "    # Prepara dati\n",
    "    X_new = pd.DataFrame([customer_data])[features_to_use]\n",
    "    \n",
    "    # Scala\n",
    "    X_scaled_new = scaler.transform(X_new)\n",
    "    \n",
    "    # Cluster prediction\n",
    "    cluster = kmeans.predict(X_scaled_new)[0]\n",
    "    segment = cluster_names[cluster]\n",
    "    \n",
    "    # Distanza dal centroide\n",
    "    dist = np.linalg.norm(X_scaled_new - kmeans.cluster_centers_[cluster])\n",
    "    \n",
    "    # Anomaly score\n",
    "    anomaly_score = iso_forest.decision_function(X_scaled_new)[0]\n",
    "    is_anomaly = iso_forest.predict(X_scaled_new)[0] == -1\n",
    "    \n",
    "    # PCA coordinates\n",
    "    pca_coords = pca.transform(X_scaled_new)[0]\n",
    "    \n",
    "    return {\n",
    "        'segment': segment,\n",
    "        'cluster_id': cluster,\n",
    "        'distance_to_centroid': dist,\n",
    "        'is_anomaly': is_anomaly,\n",
    "        'anomaly_score': anomaly_score,\n",
    "        'pc1': pca_coords[0],\n",
    "        'pc2': pca_coords[1]\n",
    "    }\n",
    "\n",
    "# Test con un cliente esempio\n",
    "test_customer = {\n",
    "    'recency': 10,\n",
    "    'frequency': 25,\n",
    "    'monetary': 6000,\n",
    "    'avg_basket': 100,\n",
    "    'tenure': 36,\n",
    "    'online_ratio': 0.7\n",
    "}\n",
    "\n",
    "result = score_new_customer(test_customer, scaler, kmeans, iso, pca_2d, cluster_names)\n",
    "print(\"\\nğŸ“‹ Test scoring nuovo cliente:\")\n",
    "print(f\"  Input: {test_customer}\")\n",
    "print(f\"\\n  Output:\")\n",
    "for k, v in result.items():\n",
    "    print(f\"    {k}: {v}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 10.3 DASHBOARD SUMMARY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"ğŸ“Š DASHBOARD SUMMARY\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Metriche chiave\n",
    "total_customers = len(df)\n",
    "total_revenue = df['monetary'].sum()\n",
    "avg_clv = df['monetary'].mean()\n",
    "vip_pct = (df['segment_name'] == 'ğŸŒŸ VIP').mean() * 100\n",
    "at_risk_pct = (df['segment_name'] == 'âš ï¸ At Risk').mean() * 100\n",
    "anomaly_pct = df['iso_anomaly'].mean() * 100\n",
    "\n",
    "print(f\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    ğŸ“Š CUSTOMER SEGMENTATION DASHBOARD                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  ğŸ‘¥ TOTAL CUSTOMERS: {total_customers:,}                                        â”‚\n",
    "â”‚  ğŸ’° TOTAL REVENUE: â‚¬{total_revenue:,.0f}                                  â”‚\n",
    "â”‚  ğŸ“ˆ AVG CLV: â‚¬{avg_clv:,.0f}                                              â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  SEGMENTATION                                                        â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚\n",
    "â”‚  ğŸŒŸ VIP:        {(df['segment_name'] == 'ğŸŒŸ VIP').sum():3d} ({vip_pct:4.1f}%)                                    â”‚\n",
    "â”‚  ğŸ’¼ Regular:    {(df['segment_name'] == 'ğŸ’¼ Regular').sum():3d} ({(df['segment_name'] == 'ğŸ’¼ Regular').mean()*100:4.1f}%)                                    â”‚\n",
    "â”‚  ğŸ¯ Occasional: {(df['segment_name'] == 'ğŸ¯ Occasional').sum():3d} ({(df['segment_name'] == 'ğŸ¯ Occasional').mean()*100:4.1f}%)                                    â”‚\n",
    "â”‚  âš ï¸ At Risk:    {(df['segment_name'] == 'âš ï¸ At Risk').sum():3d} ({at_risk_pct:4.1f}%)                                    â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  âš ï¸ ANOMALIES: {df['iso_anomaly'].sum():3d} ({anomaly_pct:4.1f}%)                                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 10.4 VISUALIZZAZIONE FINALE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Pie chart segmenti\n",
    "ax1 = axes[0, 0]\n",
    "segment_counts = df['segment_name'].value_counts()\n",
    "colors_pie = ['gold', 'steelblue', 'green', 'red']\n",
    "ax1.pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%',\n",
    "        colors=colors_pie, explode=[0.05, 0, 0, 0], shadow=True)\n",
    "ax1.set_title('ğŸ“Š Distribuzione Segmenti')\n",
    "\n",
    "# Plot 2: Revenue per segment\n",
    "ax2 = axes[0, 1]\n",
    "revenue_by_segment = df.groupby('segment_name')['monetary'].sum().sort_values(ascending=True)\n",
    "revenue_by_segment.plot(kind='barh', ax=ax2, color=['red', 'green', 'steelblue', 'gold'])\n",
    "ax2.set_xlabel('Revenue Totale (â‚¬)')\n",
    "ax2.set_title('ğŸ’° Revenue per Segmento')\n",
    "for i, v in enumerate(revenue_by_segment.values):\n",
    "    ax2.text(v + 1000, i, f'â‚¬{v:,.0f}', va='center')\n",
    "\n",
    "# Plot 3: Segmenti in spazio PCA con centroidi\n",
    "ax3 = axes[1, 0]\n",
    "for c in range(k_optimal):\n",
    "    mask = cluster_labels == c\n",
    "    ax3.scatter(X_pca_2d[mask, 0], X_pca_2d[mask, 1], \n",
    "                c=colors_pie[c], label=cluster_names[c], alpha=0.6, s=30)\n",
    "# Centroidi\n",
    "ax3.scatter(centroids_pca[:, 0], centroids_pca[:, 1], \n",
    "            c='black', marker='X', s=300, edgecolors='white', linewidths=2, zorder=5)\n",
    "ax3.set_xlabel('PC1')\n",
    "ax3.set_ylabel('PC2')\n",
    "ax3.set_title('ğŸ¯ Mappa Segmenti Clienti')\n",
    "ax3.legend(loc='upper right')\n",
    "\n",
    "# Plot 4: RFM 3D plot\n",
    "ax4 = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "for c in range(k_optimal):\n",
    "    mask = df['cluster'] == c\n",
    "    ax4.scatter(df.loc[mask, 'recency'], df.loc[mask, 'frequency'], \n",
    "                df.loc[mask, 'monetary'], c=colors_pie[c], \n",
    "                label=cluster_names[c], alpha=0.5, s=20)\n",
    "ax4.set_xlabel('Recency')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.set_zlabel('Monetary')\n",
    "ax4.set_title('ğŸ“Š RFM 3D View')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… FASE 10 COMPLETATA!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fcf157",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ¯ CONCLUSIONE PROGETTO\n",
    "\n",
    "## ğŸ† Risultati Ottenuti\n",
    "\n",
    "| Obiettivo | Risultato | Tecnica Usata |\n",
    "|-----------|-----------|---------------|\n",
    "| **Quanti segmenti?** | 4 segmenti naturali | K-Means + Elbow + Silhouette |\n",
    "| **Come si caratterizzano?** | VIP, Regular, Occasional, At Risk | Profiling + Radar Chart |\n",
    "| **Chi sono gli anomali?** | ~5% del dataset | Isolation Forest + LOF |\n",
    "| **Strategia per segmento?** | Retain, Grow, Activate, Win-back | Business Interpretation |\n",
    "\n",
    "## ğŸ“Š Tecniche Utilizzate (Lezioni 19-27)\n",
    "\n",
    "| Lezione | Tecnica | Applicazione |\n",
    "|---------|---------|--------------|\n",
    "| 19 | Unsupervised Intro | Framework mentale |\n",
    "| 20 | K-Means | Segmentazione primaria |\n",
    "| 21 | Scelta K | Elbow + Silhouette + CH + DB |\n",
    "| 22 | Hierarchical | Validazione alternativa |\n",
    "| 23 | DBSCAN | Noise detection naturale |\n",
    "| 24 | PCA | Riduzione 6D â†’ 2D |\n",
    "| 25 | PCA + Clustering | Visualizzazione segmenti |\n",
    "| 26 | Anomaly Detection | IsolationForest + LOF |\n",
    "| 27 | Feature Engineering | Distanze, scores, profili |\n",
    "\n",
    "## ğŸ’¡ Lesson Learned\n",
    "\n",
    "1. **Preprocessing Ã¨ fondamentale** â€” RobustScaler per outlier\n",
    "2. **Validare con multiple metriche** â€” Non affidarsi a una sola\n",
    "3. **Cross-validare con algoritmi diversi** â€” Gerarchico conferma K-Means\n",
    "4. **Business context guida le decisioni** â€” 4 segmenti interpretabili\n",
    "5. **Anomalie richiedono attenzione speciale** â€” Non sono solo errori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d11310",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“š BIGNAMI â€” Unsupervised Learning Completo\n",
    "\n",
    "## ğŸ”§ Pipeline Tipo Progetto Unsupervised\n",
    "\n",
    "```\n",
    "1. BUSINESS UNDERSTANDING\n",
    "   â””â”€â”€ Definire obiettivi e domande\n",
    "\n",
    "2. DATA LOADING & EDA\n",
    "   â””â”€â”€ Caricare, esplorare, visualizzare\n",
    "\n",
    "3. PREPROCESSING\n",
    "   â”œâ”€â”€ Handling missing values\n",
    "   â”œâ”€â”€ Outlier detection\n",
    "   â””â”€â”€ Scaling (StandardScaler/RobustScaler)\n",
    "\n",
    "4. DIMENSIONALITY REDUCTION\n",
    "   â”œâ”€â”€ PCA per visualizzazione\n",
    "   â””â”€â”€ Scegliere n_components (80-90% varianza)\n",
    "\n",
    "5. SCELTA K\n",
    "   â”œâ”€â”€ Elbow Method\n",
    "   â”œâ”€â”€ Silhouette Score\n",
    "   â”œâ”€â”€ Calinski-Harabasz\n",
    "   â””â”€â”€ Davies-Bouldin\n",
    "\n",
    "6. CLUSTERING\n",
    "   â”œâ”€â”€ K-Means (principale)\n",
    "   â”œâ”€â”€ Hierarchical (validazione)\n",
    "   â””â”€â”€ DBSCAN (noise detection)\n",
    "\n",
    "7. ANOMALY DETECTION\n",
    "   â”œâ”€â”€ Isolation Forest\n",
    "   â””â”€â”€ Local Outlier Factor\n",
    "\n",
    "8. VALIDATION\n",
    "   â”œâ”€â”€ Cross-validation con altri algoritmi\n",
    "   â””â”€â”€ Metriche: ARI, NMI\n",
    "\n",
    "9. INTERPRETATION\n",
    "   â”œâ”€â”€ Profiling cluster\n",
    "   â”œâ”€â”€ Business naming\n",
    "   â””â”€â”€ Raccomandazioni actionable\n",
    "\n",
    "10. DELIVERABLES\n",
    "    â”œâ”€â”€ Dataset arricchito\n",
    "    â”œâ”€â”€ Funzione scoring\n",
    "    â””â”€â”€ Dashboard/Report\n",
    "```\n",
    "\n",
    "## ğŸ“Š Metriche Chiave\n",
    "\n",
    "| Metrica | Formula/Comando | Ottimo |\n",
    "|---------|-----------------|--------|\n",
    "| Silhouette | `silhouette_score(X, labels)` | Max (-1 to 1) |\n",
    "| Calinski-Harabasz | `calinski_harabasz_score(X, labels)` | Max |\n",
    "| Davies-Bouldin | `davies_bouldin_score(X, labels)` | Min |\n",
    "| Inertia | `kmeans.inertia_` | Gomito |\n",
    "\n",
    "## ğŸ”§ Algoritmi Quick Reference\n",
    "\n",
    "```python\n",
    "# K-Means\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "labels = km.fit_predict(X)\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Isolation Forest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "iso = IsolationForest(contamination=0.05)\n",
    "anomalies = iso.fit_predict(X)\n",
    "\n",
    "# LOF\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "lof = LocalOutlierFactor(n_neighbors=20)\n",
    "anomalies = lof.fit_predict(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ‰ FINE BLOCCO UNSUPERVISED LEARNING!\n",
    "\n",
    "**Congratulazioni!** Hai completato il percorso completo di Unsupervised Learning:\n",
    "\n",
    "- âœ… Lezione 19: Introduzione\n",
    "- âœ… Lezione 20: K-Means\n",
    "- âœ… Lezione 21: Scelta K\n",
    "- âœ… Lezione 22: Clustering Gerarchico\n",
    "- âœ… Lezione 23: DBSCAN\n",
    "- âœ… Lezione 24: PCA\n",
    "- âœ… Lezione 25: PCA + Clustering\n",
    "- âœ… Lezione 26: Anomaly Detection\n",
    "- âœ… Lezione 27: Feature Engineering\n",
    "- âœ… Lezione 28: Progetto End-to-End\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸš€ Prossimi passi consigliati:**\n",
    "1. Applicare queste tecniche a dataset reali\n",
    "2. Esplorare altri algoritmi (HDBSCAN, t-SNE, UMAP)\n",
    "3. Integrare con Supervised Learning (semi-supervised)\n",
    "4. Studiare Deep Learning per Unsupervised (Autoencoders, VAE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
