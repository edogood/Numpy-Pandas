{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cef767",
   "metadata": {},
   "source": [
    "# ğŸ“Š Lezione 17 â€” Metriche Avanzate di Valutazione\n",
    "\n",
    "## Obiettivi della lezione\n",
    "\n",
    "In questa lezione imparerai:\n",
    "- ğŸ¯ **PerchÃ© Accuracy non basta** â€” il problema delle classi sbilanciate\n",
    "- ğŸ“Š **Precision, Recall, F1-Score** â€” metriche per classificazione\n",
    "- ğŸ”¢ **Matrice di Confusione** â€” capire TP, TN, FP, FN\n",
    "- ğŸ“ˆ **Curva ROC e AUC** â€” valutare il modello a diverse soglie\n",
    "- ğŸ“‰ **Curva Precision-Recall** â€” alternativa per classi sbilanciate\n",
    "- âš™ï¸ **Scelta della soglia** â€” ottimizzare per il business\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Posizione nel Corso\n",
    "\n",
    "```\n",
    "Lezione 15: Gradient Boosting (XGBoost, LightGBM)\n",
    "Lezione 16: Feature Importance\n",
    "ğŸ‘‰ Lezione 17: Metriche Avanzate di Valutazione â† SEI QUI\n",
    "Lezione 18: Clustering (K-Means, DBSCAN)\n",
    "Lezione 19: Riduzione DimensionalitÃ  (PCA)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## â“ Il Problema dell'Accuracy\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                SCENARIO: Rilevamento Frodi                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  Dataset: 10,000 transazioni                                    â”‚\n",
    "â”‚     - 9,900 legittime (99%)                                     â”‚\n",
    "â”‚     - 100 frodi (1%)                                            â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  Modello \"stupido\": predice SEMPRE \"legittima\"                  â”‚\n",
    "â”‚     â†’ Accuracy = 99%! ğŸ‰                                        â”‚\n",
    "â”‚     â†’ Ma non trova NESSUNA frode! ğŸ˜±                            â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  MORALE: Accuracy Ã¨ INGANNEVOLE con classi sbilanciate!         â”‚\n",
    "â”‚          Servono metriche che catturino i veri obiettivi.       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c9749",
   "metadata": {},
   "source": [
    "# Section 1 â€” La Matrice di Confusione\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¢ I 4 Tipi di Predizioni\n",
    "\n",
    "```\n",
    "                        PREDIZIONE DEL MODELLO\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚   POSITIVO    â”‚   NEGATIVO    â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        â”‚           â”‚     TP        â”‚     FN        â”‚\n",
    " REALTÃ€ â”‚ POSITIVO  â”‚ True Positive â”‚False Negative â”‚\n",
    "        â”‚           â”‚  (HIT! âœ…)    â”‚ (MISS! ğŸ˜±)    â”‚\n",
    "        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "        â”‚           â”‚     FP        â”‚     TN        â”‚\n",
    "        â”‚ NEGATIVO  â”‚False Positive â”‚ True Negative â”‚\n",
    "        â”‚           â”‚(Falso allarme)â”‚ (Corretto âœ…) â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ESEMPIO MEDICO (test per malattia):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â€¢ TP: Malato, test positivo    â†’ Diagnosi corretta!\n",
    "â€¢ TN: Sano, test negativo      â†’ Diagnosi corretta!\n",
    "â€¢ FP: Sano, test positivo      â†’ Falso allarme (ansia inutile)\n",
    "â€¢ FN: Malato, test negativo    â†’ PERICOLOSO! Malattia non rilevata\n",
    "```\n",
    "\n",
    "## ğŸ“Š Le Metriche Derivate\n",
    "\n",
    "```\n",
    "                    TP                          TP\n",
    "PRECISION = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      RECALL = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "             TP + FP                         TP + FN\n",
    "             \n",
    "\"Di quelli che ho detto            \"Di tutti i veri positivi,\n",
    " positivi, quanti lo erano?\"        quanti ne ho trovati?\"\n",
    " \n",
    "                    TN\n",
    "SPECIFICITY = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    (Recall sui negativi)\n",
    "               TN + FP\n",
    "\n",
    "              2 Ã— Precision Ã— Recall\n",
    "F1-SCORE = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   (Media armonica)\n",
    "              Precision + Recall\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f22f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO: Matrice di Confusione\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MATRICE DI CONFUSIONE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Creiamo un dataset sbilanciato (10% positivi)\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    weights=[0.9, 0.1],  # 90% classe 0, 10% classe 1\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset sbilanciato:\")\n",
    "print(f\"   Classe 0 (negativa): {(y == 0).sum()} ({(y == 0).mean():.1%})\")\n",
    "print(f\"   Classe 1 (positiva): {(y == 1).sum()} ({(y == 1).mean():.1%})\")\n",
    "\n",
    "# Training modello\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Matrice di confusione\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\nğŸ”¢ Matrice di Confusione:\")\n",
    "print(f\"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"   â”‚          PREDETTO               â”‚\")\n",
    "print(f\"   â”‚      Neg          Pos           â”‚\")\n",
    "print(f\"   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "print(f\"   â”‚ TN={tn:3d}        FP={fp:3d}    â† Reali Neg â”‚\")\n",
    "print(f\"   â”‚ FN={fn:3d}        TP={tp:3d}    â† Reali Pos â”‚\")\n",
    "print(f\"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "# Calcolo metriche manuale\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Metriche calcolate:\")\n",
    "print(f\"   Accuracy:    {accuracy:.4f}  â† (TP+TN) / Totale\")\n",
    "print(f\"   Precision:   {precision:.4f}  â† TP / (TP+FP)\")\n",
    "print(f\"   Recall:      {recall:.4f}  â† TP / (TP+FN)\")\n",
    "print(f\"   Specificity: {specificity:.4f}  â† TN / (TN+FP)\")\n",
    "print(f\"   F1-Score:    {f1:.4f}  â† Media armonica P e R\")\n",
    "\n",
    "# Visualizzazione\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Matrice confusione visuale\n",
    "ax1 = axes[0]\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['Negativo', 'Positivo'])\n",
    "disp.plot(ax=ax1, cmap='Blues', values_format='d')\n",
    "ax1.set_title('ğŸ“Š Matrice di Confusione')\n",
    "\n",
    "# Metriche come bar chart\n",
    "ax2 = axes[1]\n",
    "metriche = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity']\n",
    "valori = [accuracy, precision, recall, f1, specificity]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f39c12']\n",
    "bars = ax2.bar(metriche, valori, color=colors, edgecolor='black', alpha=0.8)\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.set_ylabel('Valore')\n",
    "ax2.set_title('ğŸ“ˆ Metriche di Classificazione')\n",
    "for bar, val in zip(bars, valori):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{val:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce731fd",
   "metadata": {},
   "source": [
    "# Section 2 â€” Precision vs Recall: Il Trade-off\n",
    "\n",
    "---\n",
    "\n",
    "## âš–ï¸ Non Puoi Avere Tutto!\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              PRECISION vs RECALL TRADE-OFF                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  ALTA PRECISION, BASSO RECALL:                                  â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚\n",
    "â”‚  \"Predico positivo SOLO quando sono sicurissimo\"                â”‚\n",
    "â”‚  â†’ Pochi falsi positivi, ma perdo molti veri positivi           â”‚\n",
    "â”‚  â†’ Esempio: Approvare prestiti solo ai clienti perfetti         â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  ALTO RECALL, BASSA PRECISION:                                  â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚\n",
    "â”‚  \"Predico positivo anche se ho qualche dubbio\"                  â”‚\n",
    "â”‚  â†’ Trovo quasi tutti i positivi, ma molti falsi allarmi         â”‚\n",
    "â”‚  â†’ Esempio: Test medico che non deve perdere nessun malato      â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## ğŸ¯ Quando Preferire Cosa?\n",
    "\n",
    "| Scenario | PrioritÃ  | PerchÃ© |\n",
    "|----------|----------|--------|\n",
    "| **Diagnosi cancro** | RECALL | Un FN = malato non curato â†’ GRAVE |\n",
    "| **Filtro spam** | PRECISION | Un FP = email importante persa â†’ fastidioso |\n",
    "| **Rilevamento frodi** | Dipende | Bilanciare costi FP vs FN |\n",
    "| **Ricerca web** | PRECISION | Meglio pochi risultati buoni che tanti inutili |\n",
    "| **Sistema allerta terremoti** | RECALL | Meglio falsi allarmi che mancarne uno |\n",
    "\n",
    "## ğŸ“Š F1-Score: La Soluzione?\n",
    "\n",
    "```\n",
    "F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)\n",
    "\n",
    "â€¢ Ãˆ la MEDIA ARMONICA di P e R\n",
    "â€¢ Penalizza valori sbilanciati\n",
    "â€¢ Se P=1.0 e R=0.1 â†’ F1=0.18 (non 0.55!)\n",
    "â€¢ Buon compromesso quando non sai cosa preferire\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a36cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO: Il Trade-off Precision-Recall\n",
    "# Variando la soglia di classificazione\n",
    "# ============================================\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, classification_report\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRADE-OFF PRECISION vs RECALL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Otteniamo le probabilitÃ  invece delle classi\n",
    "y_proba = model.predict_proba(X_test)[:, 1]  # ProbabilitÃ  classe 1\n",
    "\n",
    "print(f\"\\nğŸ“Š Distribuzione probabilitÃ  predette:\")\n",
    "print(f\"   Min: {y_proba.min():.4f}\")\n",
    "print(f\"   Max: {y_proba.max():.4f}\")\n",
    "print(f\"   Media: {y_proba.mean():.4f}\")\n",
    "\n",
    "# Calcoliamo precision e recall per diverse soglie\n",
    "precision_curve, recall_curve, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Visualizziamo l'effetto di diverse soglie\n",
    "soglie_esempio = [0.3, 0.5, 0.7]\n",
    "\n",
    "print(\"\\nğŸ“ˆ Effetto della soglia:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Soglia':<10} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "risultati_soglie = []\n",
    "for soglia in soglie_esempio:\n",
    "    y_pred_soglia = (y_proba >= soglia).astype(int)\n",
    "    \n",
    "    tp = ((y_pred_soglia == 1) & (y_test == 1)).sum()\n",
    "    fp = ((y_pred_soglia == 1) & (y_test == 0)).sum()\n",
    "    fn = ((y_pred_soglia == 0) & (y_test == 1)).sum()\n",
    "    \n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "    \n",
    "    risultati_soglie.append({'Soglia': soglia, 'Precision': prec, 'Recall': rec, 'F1': f1})\n",
    "    print(f\"{soglia:<10.1f} {prec:<12.4f} {rec:<12.4f} {f1:<12.4f}\")\n",
    "\n",
    "# Visualizzazione\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Curva Precision-Recall\n",
    "ax1 = axes[0]\n",
    "ax1.plot(recall_curve, precision_curve, linewidth=2, color='#3498db')\n",
    "ax1.fill_between(recall_curve, precision_curve, alpha=0.2, color='#3498db')\n",
    "\n",
    "# Punti per le soglie esempio\n",
    "for res in risultati_soglie:\n",
    "    ax1.scatter(res['Recall'], res['Precision'], s=150, zorder=5, \n",
    "                label=f\"Soglia={res['Soglia']}\")\n",
    "\n",
    "ax1.set_xlabel('Recall')\n",
    "ax1.set_ylabel('Precision')\n",
    "ax1.set_title('ğŸ“‰ Curva Precision-Recall')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim([0, 1.05])\n",
    "ax1.set_ylim([0, 1.05])\n",
    "\n",
    "# Precision e Recall vs Soglia\n",
    "ax2 = axes[1]\n",
    "# Nota: precision_curve ha un elemento in piÃ¹ di thresholds\n",
    "ax2.plot(thresholds, precision_curve[:-1], label='Precision', linewidth=2, color='#2ecc71')\n",
    "ax2.plot(thresholds, recall_curve[:-1], label='Recall', linewidth=2, color='#e74c3c')\n",
    "\n",
    "# Linea a soglia 0.5\n",
    "ax2.axvline(x=0.5, color='gray', linestyle='--', linewidth=1.5, label='Soglia default (0.5)')\n",
    "\n",
    "ax2.set_xlabel('Soglia di Classificazione')\n",
    "ax2.set_ylabel('Valore')\n",
    "ax2.set_title('ğŸ“Š Precision e Recall vs Soglia')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Osservazione:\")\n",
    "print(\"   - Soglia BASSA â†’ alto Recall, bassa Precision (piÃ¹ permissivo)\")\n",
    "print(\"   - Soglia ALTA â†’ alta Precision, basso Recall (piÃ¹ selettivo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba820786",
   "metadata": {},
   "source": [
    "# Section 3 â€” Curva ROC e AUC\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ ROC = Receiver Operating Characteristic\n",
    "\n",
    "```\n",
    "COSA MOSTRA LA CURVA ROC:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "Asse X: False Positive Rate (FPR) = FP / (FP + TN)\n",
    "        \"Quanti negativi ho sbagliato?\"\n",
    "        \n",
    "Asse Y: True Positive Rate (TPR) = TP / (TP + FN) = RECALL\n",
    "        \"Quanti positivi ho trovato?\"\n",
    "\n",
    "Ogni punto sulla curva = una soglia diversa.\n",
    "\n",
    "INTERPRETAZIONE:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "     TPR\n",
    "     1.0 â”¤        â•­â”€â”€â”€â”€â”€â”€â”€â”€ Modello perfetto\n",
    "         â”‚       â•±\n",
    "         â”‚     â•±\n",
    "         â”‚   â•±  â† Modello buono\n",
    "         â”‚ â•±\n",
    "     0.5 â”‚â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Random (diagonale)\n",
    "         â”‚\n",
    "     0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "         0.0    0.5    1.0\n",
    "                FPR\n",
    "```\n",
    "\n",
    "## ğŸ¯ AUC = Area Under the Curve\n",
    "\n",
    "| AUC | Interpretazione |\n",
    "|-----|-----------------|\n",
    "| 1.0 | Modello perfetto |\n",
    "| 0.9-1.0 | Eccellente |\n",
    "| 0.8-0.9 | Buono |\n",
    "| 0.7-0.8 | Discreto |\n",
    "| 0.5-0.7 | Scarso |\n",
    "| 0.5 | Random (inutile) |\n",
    "| < 0.5 | Peggio di random! |\n",
    "\n",
    "**AUC = ProbabilitÃ  che il modello assegni una probabilitÃ  piÃ¹ alta\n",
    "a un positivo casuale rispetto a un negativo casuale.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO: Curva ROC e AUC\n",
    "# ============================================\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CURVA ROC E AUC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Confrontiamo piÃ¹ modelli\n",
    "modelli = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Visualizzazione\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Curva ROC per ogni modello\n",
    "ax1 = axes[0]\n",
    "colors = ['#3498db', '#2ecc71']\n",
    "\n",
    "risultati_auc = []\n",
    "\n",
    "for (nome, mod), color in zip(modelli.items(), colors):\n",
    "    mod.fit(X_train, y_train)\n",
    "    y_proba_mod = mod.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcolo ROC\n",
    "    fpr, tpr, thresholds_roc = roc_curve(y_test, y_proba_mod)\n",
    "    auc_score = roc_auc_score(y_test, y_proba_mod)\n",
    "    \n",
    "    risultati_auc.append({'Modello': nome, 'AUC': auc_score})\n",
    "    \n",
    "    # Plot\n",
    "    ax1.plot(fpr, tpr, label=f'{nome} (AUC = {auc_score:.3f})', \n",
    "             linewidth=2, color=color)\n",
    "\n",
    "# Linea random\n",
    "ax1.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random (AUC = 0.5)')\n",
    "ax1.fill_between([0, 1], [0, 1], alpha=0.1, color='gray')\n",
    "\n",
    "ax1.set_xlabel('False Positive Rate (FPR)')\n",
    "ax1.set_ylabel('True Positive Rate (TPR = Recall)')\n",
    "ax1.set_title('ğŸ“ˆ Curva ROC - Confronto Modelli')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Zoom sulla parte interessante\n",
    "ax2 = axes[1]\n",
    "for (nome, mod), color in zip(modelli.items(), colors):\n",
    "    y_proba_mod = mod.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba_mod)\n",
    "    auc_score = roc_auc_score(y_test, y_proba_mod)\n",
    "    ax2.plot(fpr, tpr, label=f'{nome}', linewidth=2, color=color)\n",
    "\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=1.5)\n",
    "ax2.set_xlim([0, 0.3])\n",
    "ax2.set_ylim([0.7, 1.0])\n",
    "ax2.set_xlabel('False Positive Rate (FPR)')\n",
    "ax2.set_ylabel('True Positive Rate (TPR)')\n",
    "ax2.set_title('ğŸ” Zoom: Angolo in Alto a Sinistra')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Risultati\n",
    "print(\"\\nğŸ“Š Confronto AUC:\")\n",
    "print(\"-\" * 40)\n",
    "for res in risultati_auc:\n",
    "    quality = \"â­â­â­\" if res['AUC'] > 0.9 else \"â­â­\" if res['AUC'] > 0.8 else \"â­\"\n",
    "    print(f\"   {res['Modello']:25} AUC = {res['AUC']:.4f} {quality}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Interpretazione:\")\n",
    "print(\"   - AUC misura la capacitÃ  del modello di SEPARARE le classi\")\n",
    "print(\"   - PiÃ¹ la curva Ã¨ vicina all'angolo in alto a sinistra, meglio Ã¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb57fcc",
   "metadata": {},
   "source": [
    "# Section 4 â€” ROC vs Precision-Recall: Quando Usare Cosa?\n",
    "\n",
    "---\n",
    "\n",
    "## âš–ï¸ Confronto tra le Due Curve\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    ROC vs PRECISION-RECALL                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚         CURVA ROC            â”‚      CURVA PRECISION-RECALL      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Assi: FPR vs TPR             â”‚ Assi: Recall vs Precision        â”‚\n",
    "â”‚                              â”‚                                  â”‚\n",
    "â”‚ âœ… Buona per classi          â”‚ âœ… Migliore per classi           â”‚\n",
    "â”‚    bilanciate                â”‚    SBILANCIATE                   â”‚\n",
    "â”‚                              â”‚                                  â”‚\n",
    "â”‚ âš ï¸ PuÃ² essere ottimistica    â”‚ âœ… PiÃ¹ realistica su dataset     â”‚\n",
    "â”‚    con classi sbilanciate    â”‚    con molti negativi            â”‚\n",
    "â”‚                              â”‚                                  â”‚\n",
    "â”‚ Baseline: diagonale          â”‚ Baseline: linea orizzontale      â”‚\n",
    "â”‚ (AUC = 0.5)                  â”‚ (AP = % positivi)                â”‚\n",
    "â”‚                              â”‚                                  â”‚\n",
    "â”‚ Metrica: AUC-ROC             â”‚ Metrica: Average Precision (AP)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## ğŸ¯ Regola Pratica\n",
    "\n",
    "| Situazione | Curva da Usare |\n",
    "|------------|----------------|\n",
    "| Classi bilanciate (~50/50) | ROC va bene |\n",
    "| Classi sbilanciate (es. 95/5) | **Precision-Recall** |\n",
    "| Confrontare modelli diversi | Entrambe! |\n",
    "| Report per stakeholder non tecnici | ROC (piÃ¹ intuitiva) |\n",
    "| Focus sulla classe minoritaria | **Precision-Recall** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO: Confronto ROC vs Precision-Recall\n",
    "# Su dataset MOLTO sbilanciato\n",
    "# ============================================\n",
    "\n",
    "from sklearn.metrics import average_precision_score, PrecisionRecallDisplay\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ROC vs PRECISION-RECALL su Dataset Sbilanciato\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Creiamo un dataset MOLTO sbilanciato (2% positivi)\n",
    "X_unbal, y_unbal = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    weights=[0.98, 0.02],  # 98% classe 0, 2% classe 1\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(\n",
    "    X_unbal, y_unbal, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset MOLTO sbilanciato:\")\n",
    "print(f\"   Classe 0 (negativa): {(y_unbal == 0).sum()} ({(y_unbal == 0).mean():.1%})\")\n",
    "print(f\"   Classe 1 (positiva): {(y_unbal == 1).sum()} ({(y_unbal == 1).mean():.1%})\")\n",
    "print(f\"   Test set positivi: {(y_test_u == 1).sum()}\")\n",
    "\n",
    "# Training modello\n",
    "model_unbal = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "model_unbal.fit(X_train_u, y_train_u)\n",
    "y_proba_u = model_unbal.predict_proba(X_test_u)[:, 1]\n",
    "\n",
    "# Calcolo metriche\n",
    "fpr_u, tpr_u, _ = roc_curve(y_test_u, y_proba_u)\n",
    "prec_u, rec_u, _ = precision_recall_curve(y_test_u, y_proba_u)\n",
    "\n",
    "auc_roc_u = roc_auc_score(y_test_u, y_proba_u)\n",
    "ap_u = average_precision_score(y_test_u, y_proba_u)\n",
    "\n",
    "# Baseline per PR curve (proporzione di positivi)\n",
    "baseline_pr = (y_test_u == 1).mean()\n",
    "\n",
    "# Visualizzazione\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(fpr_u, tpr_u, linewidth=2, color='#3498db', \n",
    "         label=f'Random Forest (AUC = {auc_roc_u:.3f})')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random (AUC = 0.5)')\n",
    "ax1.fill_between(fpr_u, tpr_u, alpha=0.2, color='#3498db')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ğŸ“ˆ Curva ROC (sembra ottima!)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "ax2 = axes[1]\n",
    "ax2.plot(rec_u, prec_u, linewidth=2, color='#e74c3c', \n",
    "         label=f'Random Forest (AP = {ap_u:.3f})')\n",
    "ax2.axhline(y=baseline_pr, color='gray', linestyle='--', linewidth=1.5, \n",
    "            label=f'Baseline (= {baseline_pr:.3f})')\n",
    "ax2.fill_between(rec_u, prec_u, alpha=0.2, color='#e74c3c')\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('ğŸ“‰ Curva Precision-Recall (piÃ¹ realistica!)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim([0, 1.05])\n",
    "ax2.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analisi\n",
    "print(\"\\nğŸ“Š Confronto Metriche:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   AUC-ROC:            {auc_roc_u:.4f}  â† Sembra eccellente!\")\n",
    "print(f\"   Average Precision:  {ap_u:.4f}  â† PiÃ¹ realistica\")\n",
    "print(f\"   Baseline PR:        {baseline_pr:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Osservazione CRITICA:\")\n",
    "print(\"   - ROC AUC = 0.95+ sembra fantastico...\")\n",
    "print(\"   - Ma Average Precision rivela che il modello fa ancora fatica\")\n",
    "print(\"   - Con classi sbilanciate, PR curve Ã¨ piÃ¹ informativa!\")\n",
    "print(\"   - Il modello deve essere MOLTO meglio della baseline per essere utile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146879a",
   "metadata": {},
   "source": [
    "# Section 5 â€” Scegliere la Soglia Ottimale\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Come Scegliere la Soglia di Classificazione?\n",
    "\n",
    "```\n",
    "Di default, i classificatori usano soglia = 0.5\n",
    "Ma spesso NON Ã¨ la scelta ottimale!\n",
    "\n",
    "METODI PER SCEGLIERE LA SOGLIA:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "1ï¸âƒ£  MASSIMIZZARE F1-SCORE\n",
    "    â†’ Trova il punto dove P e R sono bilanciate\n",
    "    â†’ Buon default quando non hai preferenze\n",
    "\n",
    "2ï¸âƒ£  MASSIMIZZARE YOUDEN'S J (per ROC)\n",
    "    â†’ J = TPR - FPR = Sensitivity + Specificity - 1\n",
    "    â†’ Trova il punto piÃ¹ lontano dalla diagonale\n",
    "\n",
    "3ï¸âƒ£  VINCOLO SUL RECALL (business-driven)\n",
    "    â†’ Es: \"Voglio almeno il 90% di recall\"\n",
    "    â†’ Trovi la soglia che garantisce quel recall\n",
    "\n",
    "4ï¸âƒ£  ANALISI COSTI/BENEFICI\n",
    "    â†’ Quantifichi il costo di FP vs FN\n",
    "    â†’ Minimizzi il costo totale\n",
    "```\n",
    "\n",
    "## ğŸ’° Esempio: Rilevamento Frodi\n",
    "\n",
    "```\n",
    "COSTI:\n",
    "â€¢ False Positive (blocco transazione legittima): -$10 (fastidio cliente)\n",
    "â€¢ False Negative (frode non rilevata): -$1000 (perdita)\n",
    "\n",
    "CALCOLO:\n",
    "Costo Totale = FP Ã— 10 + FN Ã— 1000\n",
    "\n",
    "â†’ Meglio MOLTI falsi allarmi che perdere una frode!\n",
    "â†’ Soglia BASSA (es: 0.1) per massimizzare recall\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0379d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO: Trovare la Soglia Ottimale\n",
    "# ============================================\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TROVARE LA SOGLIA OTTIMALE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Usiamo il modello sul dataset sbilanciato\n",
    "y_proba_opt = model_unbal.predict_proba(X_test_u)[:, 1]\n",
    "\n",
    "# Metodo 1: Massimizzare F1-Score\n",
    "print(\"\\nğŸ“Œ Metodo 1: Massimizzare F1-Score\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "soglie_test = np.arange(0.05, 0.95, 0.05)\n",
    "f1_scores = []\n",
    "\n",
    "for soglia in soglie_test:\n",
    "    y_pred_temp = (y_proba_opt >= soglia).astype(int)\n",
    "    f1_temp = f1_score(y_test_u, y_pred_temp, zero_division=0)\n",
    "    f1_scores.append(f1_temp)\n",
    "\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "best_soglia_f1 = soglie_test[best_f1_idx]\n",
    "best_f1 = f1_scores[best_f1_idx]\n",
    "\n",
    "print(f\"   Soglia ottimale per F1: {best_soglia_f1:.2f}\")\n",
    "print(f\"   F1-Score massimo: {best_f1:.4f}\")\n",
    "\n",
    "# Metodo 2: Youden's J statistic (per ROC)\n",
    "print(\"\\nğŸ“Œ Metodo 2: Youden's J (massimizza TPR - FPR)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "fpr_opt, tpr_opt, thresholds_opt = roc_curve(y_test_u, y_proba_opt)\n",
    "j_scores = tpr_opt - fpr_opt\n",
    "best_j_idx = np.argmax(j_scores)\n",
    "best_soglia_j = thresholds_opt[best_j_idx]\n",
    "\n",
    "print(f\"   Soglia ottimale (Youden's J): {best_soglia_j:.4f}\")\n",
    "print(f\"   J statistic: {j_scores[best_j_idx]:.4f}\")\n",
    "\n",
    "# Metodo 3: Garantire minimo Recall\n",
    "print(\"\\nğŸ“Œ Metodo 3: Garantire Recall â‰¥ 90%\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "target_recall = 0.90\n",
    "for soglia in np.arange(0.01, 1.0, 0.01):\n",
    "    y_pred_temp = (y_proba_opt >= soglia).astype(int)\n",
    "    tp = ((y_pred_temp == 1) & (y_test_u == 1)).sum()\n",
    "    fn = ((y_pred_temp == 0) & (y_test_u == 1)).sum()\n",
    "    recall_temp = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    if recall_temp >= target_recall:\n",
    "        best_soglia_recall = soglia\n",
    "        achieved_recall = recall_temp\n",
    "        # Calcola precision a questa soglia\n",
    "        fp = ((y_pred_temp == 1) & (y_test_u == 0)).sum()\n",
    "        precision_at_recall = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        break\n",
    "\n",
    "print(f\"   Soglia per Recall â‰¥ 90%: {best_soglia_recall:.2f}\")\n",
    "print(f\"   Recall effettivo: {achieved_recall:.2%}\")\n",
    "print(f\"   Precision a questa soglia: {precision_at_recall:.4f}\")\n",
    "\n",
    "# Visualizzazione\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1-Score vs Soglia\n",
    "ax1 = axes[0]\n",
    "ax1.plot(soglie_test, f1_scores, linewidth=2, color='#9b59b6', marker='o')\n",
    "ax1.axvline(x=best_soglia_f1, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Ottimo: {best_soglia_f1:.2f}')\n",
    "ax1.axvline(x=0.5, color='gray', linestyle=':', linewidth=1.5, label='Default: 0.5')\n",
    "ax1.set_xlabel('Soglia')\n",
    "ax1.set_ylabel('F1-Score')\n",
    "ax1.set_title('ğŸ“Š F1-Score vs Soglia')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ROC con punto Youden\n",
    "ax2 = axes[1]\n",
    "ax2.plot(fpr_opt, tpr_opt, linewidth=2, color='#3498db')\n",
    "ax2.scatter([fpr_opt[best_j_idx]], [tpr_opt[best_j_idx]], s=200, color='red', \n",
    "            zorder=5, label=f\"Youden's J (soglia={best_soglia_j:.2f})\")\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=1.5)\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title(\"ğŸ“ˆ Punto Ottimale su ROC (Youden's J)\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Riepilogo\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“‹ RIEPILOGO SOGLIE OTTIMALI\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "   Soglia Default:         0.50\n",
    "   Soglia Max F1:          {best_soglia_f1:.2f}\n",
    "   Soglia Youden's J:      {best_soglia_j:.2f}\n",
    "   Soglia Recall â‰¥ 90%:    {best_soglia_recall:.2f}\n",
    "   \n",
    "ğŸ’¡ La scelta dipende dal CONTESTO BUSINESS!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72e9f6",
   "metadata": {},
   "source": [
    "# Section 6 â€” Schema Mentale\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Quale Metrica Usare?\n",
    "\n",
    "```\n",
    "                    SCEGLI LA METRICA GIUSTA\n",
    "                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚    Le classi sono bilanciate?   â”‚\n",
    "            â”‚         (~50/50)                â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚                               â”‚\n",
    "              â–¼                               â–¼\n",
    "            [ SÃ¬ ]                          [ No ]\n",
    "              â”‚                               â”‚\n",
    "              â–¼                               â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚ Accuracy  â”‚              â”‚ Precision/Recall â”‚\n",
    "        â”‚   o F1    â”‚              â”‚  F1 o PR-AUC     â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                            â”‚\n",
    "                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                          â”‚                                   â”‚\n",
    "                          â–¼                                   â–¼\n",
    "                   FN Ã¨ costoso?                        FP Ã¨ costoso?\n",
    "                   (es: diagnosi)                       (es: spam)\n",
    "                          â”‚                                   â”‚\n",
    "                          â–¼                                   â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  RECALL   â”‚                      â”‚ PRECISION  â”‚\n",
    "                    â”‚ prioritÃ   â”‚                      â”‚ prioritÃ    â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## ğŸ“‹ Tabella Riassuntiva Metriche\n",
    "\n",
    "| Metrica | Formula | Quando Usare |\n",
    "|---------|---------|--------------|\n",
    "| **Accuracy** | (TP+TN)/Tot | Classi bilanciate |\n",
    "| **Precision** | TP/(TP+FP) | FP Ã¨ costoso |\n",
    "| **Recall** | TP/(TP+FN) | FN Ã¨ costoso |\n",
    "| **F1-Score** | 2PR/(P+R) | Trade-off P/R |\n",
    "| **ROC-AUC** | Area ROC | Confronto modelli |\n",
    "| **PR-AUC** | Area PR | Classi sbilanciate |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc72f6b",
   "metadata": {},
   "source": [
    "# Section 7 â€” Esercizi Svolti\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Esercizio 17.1 â€” Analisi Completa con Classification Report\n",
    "\n",
    "**Obiettivo**: Usare `classification_report` di sklearn e interpretare i risultati.\n",
    "\n",
    "**Consegna**:\n",
    "1. Carica il dataset Breast Cancer\n",
    "2. Addestra un modello (a scelta)\n",
    "3. Genera il classification report completo\n",
    "4. Identifica la classe piÃ¹ difficile da predire\n",
    "5. Calcola manualmente una delle metriche per verificare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4274b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 17.1 â€” SOLUZIONE COMPLETA\n",
    "# Analisi Completa con Classification Report\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             ConfusionMatrixDisplay, precision_score, \n",
    "                             recall_score, f1_score)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ESERCIZIO 17.1 â€” Classification Report Completo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Caricamento dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "target_names = data.target_names  # ['malignant', 'benign']\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset: Breast Cancer Wisconsin\")\n",
    "print(f\"   - Campioni: {len(X)}\")\n",
    "print(f\"   - Features: {X.shape[1]}\")\n",
    "print(f\"   - Classi: {target_names}\")\n",
    "print(f\"   - Distribuzione: Maligno={sum(y==0)}, Benigno={sum(y==1)}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 2. Training modello\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"\\nâœ… Random Forest addestrato\")\n",
    "\n",
    "# 3. Classification Report\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# 4. Analisi per classe\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ” ANALISI PER CLASSE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Classe 0 = Malignant (Ã¨ la \"positiva\" dal punto di vista medico!)\n",
    "# Classe 1 = Benign\n",
    "\n",
    "print(\"\\nğŸ“Œ Classe MALIGNO (0):\")\n",
    "prec_mal = tn / (tn + fn)  # Precision per classe 0\n",
    "rec_mal = tn / (tn + fp)   # Recall per classe 0\n",
    "print(f\"   - I falsi negativi (FN) per maligno = {fp}\")\n",
    "print(f\"   - Questi sono tumori maligni classificati come benigni! GRAVE!\")\n",
    "\n",
    "print(\"\\nğŸ“Œ Classe BENIGNO (1):\")\n",
    "prec_ben = tp / (tp + fp)\n",
    "rec_ben = tp / (tp + fn)\n",
    "print(f\"   - I falsi positivi (FP) per benigno = {fn}\")\n",
    "print(f\"   - Questi sono tumori benigni classificati come maligni\")\n",
    "print(f\"   - Meno grave, ma causa ansia inutile\")\n",
    "\n",
    "# 5. Calcolo manuale per verifica\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ§® VERIFICA CALCOLO MANUALE (Classe 1 = Benigno)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "precision_manual = tp / (tp + fp)\n",
    "recall_manual = tp / (tp + fn)\n",
    "f1_manual = 2 * precision_manual * recall_manual / (precision_manual + recall_manual)\n",
    "\n",
    "precision_sklearn = precision_score(y_test, y_pred)\n",
    "recall_sklearn = recall_score(y_test, y_pred)\n",
    "f1_sklearn = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n   {'Metrica':<15} {'Manuale':<12} {'Sklearn':<12} {'Match':<8}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"   {'Precision':<15} {precision_manual:<12.6f} {precision_sklearn:<12.6f} {'âœ…' if abs(precision_manual - precision_sklearn) < 0.0001 else 'âŒ'}\")\n",
    "print(f\"   {'Recall':<15} {recall_manual:<12.6f} {recall_sklearn:<12.6f} {'âœ…' if abs(recall_manual - recall_sklearn) < 0.0001 else 'âŒ'}\")\n",
    "print(f\"   {'F1-Score':<15} {f1_manual:<12.6f} {f1_sklearn:<12.6f} {'âœ…' if abs(f1_manual - f1_sklearn) < 0.0001 else 'âŒ'}\")\n",
    "\n",
    "# Visualizzazione\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Confusion matrix\n",
    "ax1 = axes[0]\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=target_names)\n",
    "disp.plot(ax=ax1, cmap='Blues')\n",
    "ax1.set_title('ğŸ“Š Matrice di Confusione')\n",
    "\n",
    "# Metriche per classe\n",
    "ax2 = axes[1]\n",
    "metriche_per_classe = pd.DataFrame({\n",
    "    'Precision': [precision_score(y_test, y_pred, pos_label=0), \n",
    "                  precision_score(y_test, y_pred, pos_label=1)],\n",
    "    'Recall': [recall_score(y_test, y_pred, pos_label=0), \n",
    "               recall_score(y_test, y_pred, pos_label=1)],\n",
    "    'F1': [f1_score(y_test, y_pred, pos_label=0), \n",
    "           f1_score(y_test, y_pred, pos_label=1)]\n",
    "}, index=target_names)\n",
    "\n",
    "metriche_per_classe.plot(kind='bar', ax=ax2, color=['#3498db', '#2ecc71', '#9b59b6'], \n",
    "                          edgecolor='black', alpha=0.8)\n",
    "ax2.set_title('ğŸ“ˆ Metriche per Classe')\n",
    "ax2.set_ylabel('Valore')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Conclusione\n",
    "print(\"\\nğŸ’¡ Conclusione:\")\n",
    "print(f\"   In questo contesto medico, un FN (maligno â†’ benigno) Ã¨ MOLTO grave!\")\n",
    "print(f\"   FN attuali: {fp} casi di cancro non rilevati\")\n",
    "print(f\"   Potremmo abbassare la soglia per aumentare il Recall sulla classe maligno.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf7ade8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Esercizio 17.2 â€” Confronto Modelli con ROC-AUC\n",
    "\n",
    "**Obiettivo**: Confrontare piÃ¹ modelli usando la curva ROC.\n",
    "\n",
    "**Consegna**:\n",
    "1. Addestra 3 modelli diversi sullo stesso dataset\n",
    "2. Calcola ROC-AUC per ciascuno\n",
    "3. Plotta le 3 curve ROC insieme\n",
    "4. Identifica il modello migliore\n",
    "5. Verifica se il modello migliore per AUC Ã¨ anche il migliore per F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a54742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 17.2 â€” SOLUZIONE COMPLETA\n",
    "# Confronto Modelli con ROC-AUC\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ESERCIZIO 17.2 â€” Confronto Modelli con ROC-AUC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Generazione dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=5,\n",
    "    weights=[0.7, 0.3],  # Leggermente sbilanciato\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset generato:\")\n",
    "print(f\"   - Campioni: {len(X)}\")\n",
    "print(f\"   - Features: {X.shape[1]}\")\n",
    "print(f\"   - Classe 0: {(y==0).sum()}, Classe 1: {(y==1).sum()}\")\n",
    "\n",
    "# 2. Definizione modelli\n",
    "modelli = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# 3. Training e calcolo metriche\n",
    "risultati = []\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for (nome, modello), color in zip(modelli.items(), colors):\n",
    "    # Training\n",
    "    modello.fit(X_train, y_train)\n",
    "    \n",
    "    # ProbabilitÃ \n",
    "    y_proba = modello.predict_proba(X_test)[:, 1]\n",
    "    y_pred = modello.predict(X_test)\n",
    "    \n",
    "    # Metriche\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    risultati.append({\n",
    "        'Modello': nome,\n",
    "        'AUC': auc,\n",
    "        'F1': f1,\n",
    "        'color': color\n",
    "    })\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{nome} (AUC = {auc:.3f})', linewidth=2, color=color)\n",
    "\n",
    "# Linea random\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random (AUC = 0.5)')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ğŸ“ˆ Confronto Curve ROC - 4 Modelli', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Tabella risultati\n",
    "print(\"\\nğŸ“Š RISULTATI:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Modello':<25} {'AUC-ROC':>12} {'F1-Score':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "risultati_df = pd.DataFrame(risultati)\n",
    "risultati_df = risultati_df.sort_values('AUC', ascending=False)\n",
    "\n",
    "for _, row in risultati_df.iterrows():\n",
    "    print(f\"{row['Modello']:<25} {row['AUC']:>12.4f} {row['F1']:>12.4f}\")\n",
    "\n",
    "# 5. Identificazione vincitore\n",
    "best_auc = risultati_df.iloc[0]\n",
    "best_f1 = risultati_df.sort_values('F1', ascending=False).iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ† VINCITORI\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n   Migliore per AUC-ROC: {best_auc['Modello']} ({best_auc['AUC']:.4f})\")\n",
    "print(f\"   Migliore per F1-Score: {best_f1['Modello']} ({best_f1['F1']:.4f})\")\n",
    "\n",
    "if best_auc['Modello'] == best_f1['Modello']:\n",
    "    print(\"\\n   âœ… Lo stesso modello vince per entrambe le metriche!\")\n",
    "else:\n",
    "    print(\"\\n   âš ï¸ Modelli diversi! Questo puÃ² succedere:\")\n",
    "    print(\"   - AUC misura la capacitÃ  di ranking (su tutte le soglie)\")\n",
    "    print(\"   - F1 misura la performance a soglia 0.5\")\n",
    "\n",
    "# Grafico confronto finale\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(risultati_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, risultati_df['AUC'], width, label='AUC-ROC', \n",
    "               color='#3498db', edgecolor='black', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, risultati_df['F1'], width, label='F1-Score', \n",
    "               color='#e74c3c', edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('ğŸ“Š Confronto AUC vs F1 per Modello')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(risultati_df['Modello'], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "            f'{bar.get_height():.3f}', ha='center', fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "            f'{bar.get_height():.3f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcfb8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Esercizio 17.3 â€” Ottimizzazione Soglia per Business Case\n",
    "\n",
    "**Obiettivo**: Trovare la soglia che minimizza il costo totale in uno scenario reale.\n",
    "\n",
    "**Consegna**:\n",
    "1. Simula uno scenario di rilevamento frodi:\n",
    "   - Costo FP (blocco transazione legittima): $20\n",
    "   - Costo FN (frode non rilevata): $500\n",
    "2. Calcola il costo totale per diverse soglie\n",
    "3. Trova la soglia che minimizza il costo\n",
    "4. Confronta con la soglia di default (0.5)\n",
    "5. Visualizza il risparmio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905912eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 17.3 â€” SOLUZIONE COMPLETA\n",
    "# Ottimizzazione Soglia per Business Case\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ESERCIZIO 17.3 â€” Ottimizzazione Soglia per Business Case\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scenario: Rilevamento Frodi\n",
    "print(\"\"\"\n",
    "ğŸ“Œ SCENARIO: Sistema di Rilevamento Frodi\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â€¢ Classe 0: Transazione legittima\n",
    "â€¢ Classe 1: Frode (positivo)\n",
    "\n",
    "COSTI:\n",
    "â€¢ FP (blocco transazione legittima): $20 (fastidio cliente)\n",
    "â€¢ FN (frode non rilevata): $500 (perdita effettiva)\n",
    "\"\"\")\n",
    "\n",
    "COSTO_FP = 20   # Blocco transazione legittima\n",
    "COSTO_FN = 500  # Frode non rilevata\n",
    "\n",
    "# 1. Generazione dataset (frodi = 5%)\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=15,\n",
    "    n_informative=10,\n",
    "    weights=[0.95, 0.05],  # 5% frodi\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"ğŸ“Š Dataset:\")\n",
    "print(f\"   - Transazioni totali: {len(X)}\")\n",
    "print(f\"   - Frodi: {(y==1).sum()} ({(y==1).mean():.1%})\")\n",
    "print(f\"   - Test set: {len(y_test)} transazioni\")\n",
    "\n",
    "# 2. Training modello\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_proba = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nâœ… Modello addestrato\")\n",
    "\n",
    "# 3. Calcolo costo per diverse soglie\n",
    "soglie = np.arange(0.01, 0.99, 0.01)\n",
    "costi = []\n",
    "dettagli = []\n",
    "\n",
    "for soglia in soglie:\n",
    "    y_pred = (y_proba >= soglia).astype(int)\n",
    "    \n",
    "    fp = ((y_pred == 1) & (y_test == 0)).sum()  # Legittime bloccate\n",
    "    fn = ((y_pred == 0) & (y_test == 1)).sum()  # Frodi non rilevate\n",
    "    \n",
    "    costo_totale = fp * COSTO_FP + fn * COSTO_FN\n",
    "    costi.append(costo_totale)\n",
    "    dettagli.append({'soglia': soglia, 'fp': fp, 'fn': fn, 'costo': costo_totale})\n",
    "\n",
    "# 4. Trova soglia ottimale\n",
    "dettagli_df = pd.DataFrame(dettagli)\n",
    "idx_ottimo = np.argmin(costi)\n",
    "soglia_ottima = soglie[idx_ottimo]\n",
    "costo_minimo = costi[idx_ottimo]\n",
    "\n",
    "# Costo a soglia 0.5 (default)\n",
    "idx_default = np.where(soglie >= 0.5)[0][0]\n",
    "costo_default = costi[idx_default]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š RISULTATI OTTIMIZZAZIONE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nğŸ¯ Soglia ottimale: {soglia_ottima:.2f}\")\n",
    "print(f\"   Costo totale: ${costo_minimo:,.0f}\")\n",
    "print(f\"   FP (legit bloccate): {dettagli_df.iloc[idx_ottimo]['fp']:.0f}\")\n",
    "print(f\"   FN (frodi perse): {dettagli_df.iloc[idx_ottimo]['fn']:.0f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Œ Soglia default (0.5):\")\n",
    "print(f\"   Costo totale: ${costo_default:,.0f}\")\n",
    "print(f\"   FP: {dettagli_df.iloc[idx_default]['fp']:.0f}\")\n",
    "print(f\"   FN: {dettagli_df.iloc[idx_default]['fn']:.0f}\")\n",
    "\n",
    "risparmio = costo_default - costo_minimo\n",
    "risparmio_perc = (risparmio / costo_default) * 100\n",
    "\n",
    "print(f\"\\nğŸ’° RISPARMIO con soglia ottimizzata:\")\n",
    "print(f\"   ${risparmio:,.0f} ({risparmio_perc:.1f}%)\")\n",
    "\n",
    "# 5. Visualizzazione\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Costo vs Soglia\n",
    "ax1 = axes[0]\n",
    "ax1.plot(soglie, costi, linewidth=2, color='#e74c3c')\n",
    "ax1.axvline(x=soglia_ottima, color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Ottima: {soglia_ottima:.2f}')\n",
    "ax1.axvline(x=0.5, color='gray', linestyle=':', linewidth=2, \n",
    "            label='Default: 0.50')\n",
    "ax1.scatter([soglia_ottima], [costo_minimo], s=150, color='green', zorder=5)\n",
    "ax1.scatter([0.5], [costo_default], s=150, color='gray', zorder=5)\n",
    "\n",
    "ax1.set_xlabel('Soglia di Classificazione', fontsize=12)\n",
    "ax1.set_ylabel('Costo Totale ($)', fontsize=12)\n",
    "ax1.set_title('ğŸ’° Costo Totale vs Soglia', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Breakdown FP vs FN\n",
    "ax2 = axes[1]\n",
    "ax2.plot(soglie, dettagli_df['fp'] * COSTO_FP, linewidth=2, \n",
    "         label=f'Costo FP (${COSTO_FP}/blocco)', color='#3498db')\n",
    "ax2.plot(soglie, dettagli_df['fn'] * COSTO_FN, linewidth=2, \n",
    "         label=f'Costo FN (${COSTO_FN}/frode)', color='#e74c3c')\n",
    "ax2.axvline(x=soglia_ottima, color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Soglia ottima: {soglia_ottima:.2f}')\n",
    "\n",
    "ax2.set_xlabel('Soglia di Classificazione', fontsize=12)\n",
    "ax2.set_ylabel('Costo ($)', fontsize=12)\n",
    "ax2.set_title('ğŸ“Š Breakdown: Costo FP vs FN', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabella confronto\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“‹ TABELLA CONFRONTO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    Soglia    â”‚   Costo FP   â”‚   Costo FN   â”‚ Costo Totale â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 0.50 (def)   â”‚ ${dettagli_df.iloc[idx_default]['fp'] * COSTO_FP:>10,.0f} â”‚ ${dettagli_df.iloc[idx_default]['fn'] * COSTO_FN:>10,.0f} â”‚ ${costo_default:>10,.0f} â”‚\n",
    "â”‚ {soglia_ottima:.2f} (opt)   â”‚ ${dettagli_df.iloc[idx_ottimo]['fp'] * COSTO_FP:>10,.0f} â”‚ ${dettagli_df.iloc[idx_ottimo]['fn'] * COSTO_FN:>10,.0f} â”‚ ${costo_minimo:>10,.0f} â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ RISPARMIO    â”‚              â”‚              â”‚ ${risparmio:>10,.0f} â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ’¡ Conclusione:\")\n",
    "print(f\"   Abbassando la soglia da 0.50 a {soglia_ottima:.2f}, accettiamo\")\n",
    "print(f\"   piÃ¹ falsi positivi (blocchi errati) ma riduciamo le frodi perse,\")\n",
    "print(f\"   risparmiando ${risparmio:,.0f} complessivamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b25d2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Œ Conclusione e Bignami\n",
    "\n",
    "### âœ… Cosa Abbiamo Imparato\n",
    "\n",
    "1. **L'accuracy non basta** â€” su classi sbilanciate Ã¨ fuorviante\n",
    "2. **Precision vs Recall** â€” trade-off inevitabile, scegli in base al contesto\n",
    "3. **La soglia conta** â€” non usare sempre 0.5, ottimizzala per il business\n",
    "4. **ROC-AUC vs PR-AUC** â€” PR-AUC piÃ¹ informativa su dataset sbilanciati\n",
    "5. **Costi reali** â€” quando FP e FN hanno costi diversi, ottimizza per il costo totale\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ BIGNAMI â€” Metriche di Classificazione\n",
    "\n",
    "| Metrica | Formula | Quando Usarla | Esempio |\n",
    "|---------|---------|---------------|---------|\n",
    "| **Accuracy** | (TP+TN)/(Totale) | Classi bilanciate | Classificazione generi |\n",
    "| **Precision** | TP/(TP+FP) | FP costosi | Spam, raccomandazioni |\n",
    "| **Recall** | TP/(TP+FN) | FN costosi | Malattie, frodi |\n",
    "| **F1-Score** | 2Â·(PÂ·R)/(P+R) | Bilanciare P e R | Generale |\n",
    "| **Specificity** | TN/(TN+FP) | Veri negativi importanti | Test medici |\n",
    "| **ROC-AUC** | Area sotto ROC | Confronto modelli | Benchmark |\n",
    "| **PR-AUC** | Area sotto PR | Classi sbilanciate | Frodi, malattie rare |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Flowchart Decisionale Rapido\n",
    "\n",
    "```\n",
    "Dataset bilanciato? â†’ Accuracy + ROC-AUC\n",
    "          â†“ NO\n",
    "Classi sbilanciate? â†’ PR-AUC + F1 della classe rara\n",
    "          â†“\n",
    "FN piÃ¹ costosi di FP? â†’ Massimizza Recall (abbassa soglia)\n",
    "FP piÃ¹ costosi di FN? â†’ Massimizza Precision (alza soglia)\n",
    "Costi noti? â†’ Minimizza Costo = FPÃ—C_FP + FNÃ—C_FN\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ Codice Rapido di Riferimento\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# Report completo\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "auc = roc_auc_score(y_true, y_proba)\n",
    "\n",
    "# Soglia ottimale (Youden's J)\n",
    "j_scores = tpr - fpr\n",
    "soglia_ottima = thresholds[np.argmax(j_scores)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ Errori Comuni da Evitare\n",
    "\n",
    "| âŒ Errore | âœ… Soluzione |\n",
    "|-----------|-------------|\n",
    "| Usare accuracy su dati sbilanciati | Usa F1, PR-AUC |\n",
    "| Ignorare la soglia | Ottimizzala per il business |\n",
    "| Confrontare ROC su dati sbilanciati | Usa PR curve |\n",
    "| Non calcolare entrambe P e R | Guarda sempre il trade-off |\n",
    "| Non considerare i costi | Quantifica FP e FN in â‚¬ |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Prossimi Passi\n",
    "Nella **Lezione 18** vedremo il **Clustering** â€” algoritmi non supervisionati per scoprire gruppi naturali nei dati!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
