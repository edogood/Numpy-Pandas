{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Titolo e obiettivi\n",
    "Lezione 38: Introduzione ai modelli generativi con un bigramma semplice.\n",
    "\n",
    "---\n",
    "\n",
    "## Mappa concettuale della lezione\n",
    "\n",
    "```\n",
    "MODELLI GENERATIVI - PANORAMICA\n",
    "================================\n",
    "\n",
    "MODELLI DISCRIMINATIVI          MODELLI GENERATIVI\n",
    "==================              ==================\n",
    "P(y|x) - \"data x, che classe?\"  P(x) o P(x|z) - \"genera x\"\n",
    "\n",
    "Input ──► Modello ──► Label     Latent/Context ──► Modello ──► Output\n",
    "                                        \n",
    "Esempio:                        Esempio:\n",
    "- Classificazione               - Generazione testo\n",
    "- Regressione                   - Sintesi immagini\n",
    "- Object detection              - Data augmentation\n",
    "\n",
    "\n",
    "MODELLO AUTOREGRESSIVO (Bigram)\n",
    "================================\n",
    "\n",
    "    P(frase) = P(w₁) × P(w₂|w₁) × P(w₃|w₂) × ... × P(wₙ|wₙ₋₁)\n",
    "\n",
    "    <s> ──► \"apri\" ──► \"un\" ──► \"ticket\" ──► </s>\n",
    "         P(apri|<s>)  P(un|apri) P(ticket|un)  P(</s>|ticket)\n",
    "\n",
    "\n",
    "GENERAZIONE CON TEMPERATURA\n",
    "============================\n",
    "\n",
    "    BASSA TEMPERATURA (T→0)         ALTA TEMPERATURA (T→∞)\n",
    "    ========================         ========================\n",
    "    Sceglie parola piu' probabile   Scelta quasi uniforme\n",
    "    Output deterministico           Output molto casuale\n",
    "    Ripetitivo ma coerente          Creativo ma incoerente\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivi didattici\n",
    "\n",
    "| # | Obiettivo | Livello |\n",
    "|---|-----------|---------|\n",
    "| 1 | Distinguere modelli generativi da discriminativi | Concettuale |\n",
    "| 2 | Implementare modello bigram con conteggi | Operativo |\n",
    "| 3 | Generare testo campionando da distribuzioni | Pratico |\n",
    "| 4 | Controllare creativita' con temperatura | Avanzato |\n",
    "| 5 | Riconoscere limiti dei modelli n-gram | Critico |\n",
    "| 6 | Preparare transizione verso modelli neurali | Prospettiva |\n",
    "\n",
    "---\n",
    "\n",
    "## Concetti chiave\n",
    "\n",
    "> **Modello generativo**: modello che impara la distribuzione P(x) dei dati e puo' generare nuovi esempi campionando da essa.\n",
    "\n",
    "> **Autoregressivo**: genera sequenze un token alla volta, condizionando ogni token sui precedenti: P(x₁, x₂, ..., xₙ) = ∏ P(xᵢ|x₁...xᵢ₋₁).\n",
    "\n",
    "> **Temperatura**: parametro che scala i logits prima del softmax; T<1 sharpening (piu' deterministico), T>1 flattening (piu' casuale).\n",
    "\n",
    "---\n",
    "\n",
    "## Tassonomia modelli generativi\n",
    "\n",
    "```\n",
    "+------------------------+------------------------+\n",
    "|   MODELLO              |   COMPLESSITA'         |\n",
    "+------------------------+------------------------+\n",
    "| Bigram                 | ★☆☆☆☆ (baseline)       |\n",
    "| N-gram (n>2)           | ★★☆☆☆                  |\n",
    "| RNN/LSTM               | ★★★☆☆                  |\n",
    "| Transformer            | ★★★★☆                  |\n",
    "| GPT/LLM                | ★★★★★                  |\n",
    "+------------------------+------------------------+\n",
    "\n",
    "Bigram: memoria di 1 token - questa lezione\n",
    "N-gram: memoria di n-1 token - estensione diretta\n",
    "RNN: memoria \"teoricamente\" infinita - vanishing gradient\n",
    "Transformer: attenzione globale - stato dell'arte\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Cosa useremo\n",
    "- Conteggi di bigrammi con `defaultdict`\n",
    "- Campionamento con `np.random.choice`\n",
    "- Controllo temperatura per diversita'\n",
    "- Corpus di esempio (frasi assistenza clienti)\n",
    "\n",
    "## Prerequisiti\n",
    "- Tokenizzazione base (Lezione 30)\n",
    "- Probabilita' condizionata\n",
    "- Concetto di likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Teoria concettuale\n",
    "- Modello generativo: stima P(token_t | token_{t-1}) da un corpus.\n",
    "- Smoothing/temperatura: controlla diversita' vs ripetizione.\n",
    "- Limiti: nessuna memoria lunga; non gestisce sintassi complessa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Schema mentale / mappa decisionale\n",
    "1. Tokenizza il corpus.\n",
    "2. Conta bigrammi -> probabilita' condizionate.\n",
    "3. Genera testo scegliendo parola successiva in base a P(.|prev) con temperatura.\n",
    "4. Valuta qualitativamente, aggiungi dati per migliorare copertura.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sezione dimostrativa\n",
    "Demo: costruzione modello bigram su corpus assistenza clienti, generazione e osservazioni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus su assistenza clienti\n",
    "corpus = [\n",
    "    \"apri un ticket per assistenza tecnica\",\n",
    "    \"richiedi rimborso per ordine difettoso\",\n",
    "    \"verifica lo stato della spedizione\",\n",
    "    \"contatta il supporto per informazioni\",\n",
    "    \"chiudi il ticket risolto\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizzazione semplice e conteggio bigrammi\n",
    "start = '<s>'\n",
    "end = '</s>'\n",
    "\n",
    "bigrams = defaultdict(Counter)\n",
    "for doc in corpus:\n",
    "    tokens = [start] + doc.split() + [end]\n",
    "    for a, b in zip(tokens, tokens[1:]):\n",
    "        bigrams[a][b] += 1\n",
    "\n",
    "# Calcolo probabilita' condizionate\n",
    "probs = {a: {b: c/sum(cnt.values()) for b, c in cnt.items()} for a, cnt in bigrams.items()}\n",
    "print(\"Esempio probs per '<s>':\", probs[start])\n",
    "assert start in probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generazione autoregressiva con temperatura\n",
    "\n",
    "def sample_next(distrib, temperature=1.0):\n",
    "    words = list(distrib.keys())\n",
    "    logits = np.log(np.array(list(distrib.values())) + 1e-8) / temperature\n",
    "    probs_temp = np.exp(logits) / np.sum(np.exp(logits))\n",
    "    return np.random.choice(words, p=probs_temp)\n",
    "\n",
    "def generate(max_len=15, temperature=1.0):\n",
    "    word = start\n",
    "    out = []\n",
    "    for _ in range(max_len):\n",
    "        next_w = sample_next(probs[word], temperature)\n",
    "        if next_w == end:\n",
    "            break\n",
    "        out.append(next_w)\n",
    "        word = next_w\n",
    "    return ' '.join(out)\n",
    "\n",
    "print(\"Generazione (temp=1.0):\", generate())\n",
    "print(\"Generazione (temp=0.5):\", generate(temperature=0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Osservazioni\n",
    "- Con temperatura bassa il testo e' piu' ripetitivo ma coerente; alta temperatura aumenta diversita' e rumore.\n",
    "- Corpus piccolo limita la copertura e porta a frasi brevi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Esercizi svolti (step-by-step)\n",
    "## Esercizio 38.1 - Probabilita' di una frase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 38.1: calcola P(\"apri un ticket\")\n",
    "phrase = [start, 'apri', 'un', 'ticket', end]\n",
    "prob_phrase = 1.0\n",
    "for a, b in zip(phrase, phrase[1:]):\n",
    "    prob_phrase *= probs.get(a, {}).get(b, 1e-8)\n",
    "print(f\"Probabilita' frase: {prob_phrase:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 38.2 - Temperatura diversa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 38.2: genera con temperature diverse\n",
    "for t in [0.3, 0.7, 1.2]:\n",
    "    print(f\"Temp={t}: {generate(temperature=t)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 38.3 - Aggiungere dati\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 38.3: aggiungi un documento e rigenera\n",
    "corpus_extra = corpus + [\"richiedi assistenza per rimborso spedizione\"]\n",
    "# ricalcolo rapido\n",
    "bigrams2 = defaultdict(Counter)\n",
    "for doc in corpus_extra:\n",
    "    toks = [start] + doc.split() + [end]\n",
    "    for a,b in zip(toks, toks[1:]):\n",
    "        bigrams2[a][b] += 1\n",
    "probs2 = {a:{b:c/sum(cnt.values()) for b,c in cnt.items()} for a,cnt in bigrams2.items()}\n",
    "probs = probs2  # aggiorna il modello\n",
    "print(\"Generazione dopo aggiornamento:\", generate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Conclusione operativa - Bignami Modelli Generativi\n",
    "\n",
    "---\n",
    "\n",
    "## I 5 Take-Home Messages\n",
    "\n",
    "| # | Concetto | Perche' conta |\n",
    "|---|----------|---------------|\n",
    "| 1 | **Generativo vs Discriminativo** | Generativo modella P(x), discriminativo P(y|x) |\n",
    "| 2 | **Autoregressivo = token by token** | Ogni token dipende dai precedenti |\n",
    "| 3 | **Bigram = memoria corta** | Solo 1 token di contesto, limiti evidenti |\n",
    "| 4 | **Temperatura controlla creativita'** | Trade-off coerenza vs diversita' |\n",
    "| 5 | **Smoothing per token mai visti** | Evita probabilita' zero su bigrammi nuovi |\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline generazione testo\n",
    "\n",
    "```\n",
    "TRAINING                           INFERENCE\n",
    "========                           =========\n",
    "\n",
    "Corpus ──► Tokenize ──► Count      Seed ──► Sample ──► Append ──► Loop\n",
    "                          |                    ↑            |\n",
    "                          v                    |            |\n",
    "                      P(w|prev)                └────────────┘\n",
    "                          |                    until </s>\n",
    "                          v\n",
    "                   Normalize probs\n",
    "\n",
    "\n",
    "TEMPERATURA (formula)\n",
    "=====================\n",
    "\n",
    "    P_temp(w) = exp(log P(w) / T) / Σ exp(log P(w') / T)\n",
    "\n",
    "    T = 1.0  →  Probabilita' originali\n",
    "    T < 1.0  →  Sharpening (piu' deterministico)\n",
    "    T > 1.0  →  Flattening (piu' casuale)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Confronto temperature\n",
    "\n",
    "| Temperatura | Effetto | Output tipico |\n",
    "|-------------|---------|---------------|\n",
    "| 0.1 | Quasi greedy | \"apri un ticket per assistenza\" (ripetitivo) |\n",
    "| 0.5 | Bilanciato | \"richiedi rimborso per ordine\" (coerente) |\n",
    "| 1.0 | Originale | \"apri un ticket difettoso\" (variabile) |\n",
    "| 2.0 | Casuale | \"per ticket rimborso un\" (incoerente) |\n",
    "\n",
    "---\n",
    "\n",
    "## Template modello bigram\n",
    "\n",
    "```python\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class BigramModel:\n",
    "    def __init__(self):\n",
    "        self.bigrams = defaultdict(lambda: defaultdict(int))\n",
    "        self.vocab = set()\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "        \"\"\"Corpus: lista di frasi tokenizzate\"\"\"\n",
    "        for sentence in corpus:\n",
    "            tokens = ['<s>'] + sentence.split() + ['</s>']\n",
    "            for i in range(len(tokens) - 1):\n",
    "                self.bigrams[tokens[i]][tokens[i+1]] += 1\n",
    "                self.vocab.update(tokens)\n",
    "    \n",
    "    def get_probs(self, prev_token, temperature=1.0):\n",
    "        \"\"\"Ritorna distribuzione P(next|prev) con temperatura\"\"\"\n",
    "        counts = self.bigrams[prev_token]\n",
    "        if not counts:\n",
    "            return None  # Token mai visto\n",
    "        words = list(counts.keys())\n",
    "        freqs = np.array([counts[w] for w in words], dtype=float)\n",
    "        # Applica temperatura\n",
    "        log_probs = np.log(freqs + 1e-10) / temperature\n",
    "        probs = np.exp(log_probs) / np.sum(np.exp(log_probs))\n",
    "        return dict(zip(words, probs))\n",
    "    \n",
    "    def generate(self, max_len=20, temperature=1.0):\n",
    "        \"\"\"Genera frase autoregressivamente\"\"\"\n",
    "        tokens = ['<s>']\n",
    "        for _ in range(max_len):\n",
    "            probs = self.get_probs(tokens[-1], temperature)\n",
    "            if probs is None:\n",
    "                break\n",
    "            next_token = np.random.choice(list(probs.keys()), p=list(probs.values()))\n",
    "            if next_token == '</s>':\n",
    "                break\n",
    "            tokens.append(next_token)\n",
    "        return ' '.join(tokens[1:])\n",
    "    \n",
    "    def sentence_probability(self, sentence):\n",
    "        \"\"\"Calcola P(frase) = prodotto P(w_i|w_{i-1})\"\"\"\n",
    "        tokens = ['<s>'] + sentence.split() + ['</s>']\n",
    "        prob = 1.0\n",
    "        for i in range(len(tokens) - 1):\n",
    "            probs = self.get_probs(tokens[i])\n",
    "            if probs and tokens[i+1] in probs:\n",
    "                prob *= probs[tokens[i+1]]\n",
    "            else:\n",
    "                return 0.0  # Bigram mai visto\n",
    "        return prob\n",
    "\n",
    "# Uso\n",
    "model = BigramModel()\n",
    "model.fit(corpus)\n",
    "print(model.generate(temperature=0.7))\n",
    "print(f\"P(frase): {model.sentence_probability('apri un ticket'):.6f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Errori comuni e soluzioni\n",
    "\n",
    "| Errore | Conseguenza | Soluzione |\n",
    "|--------|-------------|-----------|\n",
    "| No marker <s>/<\\/s> | Non sa dove iniziare/finire | Aggiungere sempre i marker |\n",
    "| Temperatura = 0 | Divisione per zero | Usare T >= 0.01 |\n",
    "| Corpus piccolo | Molti bigram mancanti | Add-k smoothing o piu' dati |\n",
    "| Loop infiniti | Frase non termina | Max length + check </s> |\n",
    "\n",
    "---\n",
    "\n",
    "## Metodi e concetti chiave\n",
    "\n",
    "| Elemento | Ruolo |\n",
    "|----------|-------|\n",
    "| `defaultdict(int)` | Conteggi bigram |\n",
    "| Probabilita' condizionata | P(next\\|prev) = count(prev,next) / count(prev) |\n",
    "| Temperatura | Controlla entropia della distribuzione |\n",
    "| `np.random.choice` | Campionamento da distribuzione |\n",
    "| Log-probability | Per evitare underflow su frasi lunghe |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Checklist di fine lezione\n",
    "- [ ] Ho tokenizzato e aggiunto marker <s>/<\\s> per inizio/fine.\n",
    "- [ ] Ho calcolato correttamente le probabilita' condizionate bigram.\n",
    "- [ ] Ho testato generazione con temperature diverse.\n",
    "- [ ] Ho aggiunto dati o smoothing se i bigram mancavano.\n",
    "- [ ] Ho valutato qualitativamente le frasi generate.\n",
    "\n",
    "Glossario\n",
    "- Bigram: coppia di parole consecutive.\n",
    "- Temperatura: parametro che calibra la casualita' nel campionamento.\n",
    "- Autoregressivo: genera token successivi basandosi sugli precedenti.\n",
    "- Smoothing: tecnica per gestire probabilita' di eventi non visti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Changelog didattico\n",
    "\n",
    "| Versione | Data | Modifiche |\n",
    "|----------|------|-----------|\n",
    "| 1.0 | 2024-01-XX | Struttura iniziale 8 sezioni |\n",
    "| 2.0 | 2024-12-XX | Espansione completa Modelli Generativi |\n",
    "| 2.1 | - | Confronto generativo vs discriminativo ASCII |\n",
    "| 2.2 | - | Tassonomia modelli (bigram → GPT) |\n",
    "| 2.3 | - | Formula temperatura con effetti visualizzati |\n",
    "| 2.4 | - | Classe BigramModel completa con metodi |\n",
    "| 2.5 | - | Tabella confronto temperature con esempi output |\n",
    "| 2.6 | - | Calcolo probabilita' di frase |\n",
    "\n",
    "---\n",
    "\n",
    "## Note di versione\n",
    "\n",
    "**v2.0 - Espansione didattica completa**\n",
    "- Posizionamento generativo vs discriminativo chiaro\n",
    "- Modello autoregressivo spiegato con formula prodotto\n",
    "- Temperatura come parametro di controllo creativita'\n",
    "- Classe template riutilizzabile con tutti i metodi\n",
    "- Preparazione per modelli piu' complessi (RNN, Transformer)\n",
    "\n",
    "**Dipendenze didattiche**\n",
    "- Richiede: Lezione 30 (tokenizzazione), probabilita' base\n",
    "- Prepara: Comprensione di LLM e generazione neurale\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
