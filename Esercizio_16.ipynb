{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acc6c5f",
   "metadata": {},
   "source": [
    "# üìä Lezione 16 ‚Äî Feature Importance\n",
    "\n",
    "## Obiettivi della lezione\n",
    "\n",
    "In questa lezione imparerai:\n",
    "- üéØ **Cos'√®** la Feature Importance e perch√© √® fondamentale\n",
    "- üìä **MDI** (Mean Decrease Impurity) ‚Äî importanza built-in degli alberi\n",
    "- üîÄ **Permutation Importance** ‚Äî metodo model-agnostic\n",
    "- üìà **SHAP Values** ‚Äî spiegabilit√† avanzata del modello\n",
    "- ‚öñÔ∏è **Confronto** tra i diversi metodi\n",
    "- üéØ **Feature Selection** basata sull'importanza\n",
    "\n",
    "---\n",
    "\n",
    "## üìç Posizione nel Corso\n",
    "\n",
    "```\n",
    "Lezione 14: Alberi Decisionali e Random Forest\n",
    "Lezione 15: Gradient Boosting (XGBoost, LightGBM)\n",
    "üëâ Lezione 16: Feature Importance ‚Üê SEI QUI\n",
    "Lezione 17: Metriche Avanzate di Valutazione\n",
    "Lezione 18: Clustering (K-Means, DBSCAN)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì Perch√© Feature Importance?\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                   PERCH√â CI SERVE?                               ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  1Ô∏è‚É£  INTERPRETABILIT√Ä                                           ‚îÇ\n",
    "‚îÇ      ‚Üí Spiegare PERCH√â il modello fa certe predizioni           ‚îÇ\n",
    "‚îÇ      ‚Üí Comunicare ai stakeholder (business, management)         ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  2Ô∏è‚É£  FEATURE SELECTION                                          ‚îÇ\n",
    "‚îÇ      ‚Üí Eliminare features inutili o ridondanti                  ‚îÇ\n",
    "‚îÇ      ‚Üí Ridurre dimensionalit√† e overfitting                     ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  3Ô∏è‚É£  DEBUG DEL MODELLO                                          ‚îÇ\n",
    "‚îÇ      ‚Üí Verificare che usi features sensate                      ‚îÇ\n",
    "‚îÇ      ‚Üí Identificare data leakage                                ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  4Ô∏è‚É£  DOMAIN KNOWLEDGE                                           ‚îÇ\n",
    "‚îÇ      ‚Üí Scoprire insights sui dati                               ‚îÇ\n",
    "‚îÇ      ‚Üí Validare ipotesi di business                             ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d059f8",
   "metadata": {},
   "source": [
    "# Section 1 ‚Äî I Tre Metodi di Feature Importance\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Overview dei Metodi\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    TRE APPROCCI PRINCIPALI                               ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îÇ  1Ô∏è‚É£  MDI (Mean Decrease Impurity)                                       ‚îÇ\n",
    "‚îÇ      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                        ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Come funziona: Misura quanto ogni feature riduce l'impurit√†      ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Pro: Velocissimo (calcolato durante training)                    ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Contro: Bias verso features con alta cardinalit√†                 ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Dove: .feature_importances_ in sklearn                           ‚îÇ\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îÇ  2Ô∏è‚É£  PERMUTATION IMPORTANCE                                             ‚îÇ\n",
    "‚îÇ      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                         ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Come funziona: Mischia una feature e vede quanto peggiora        ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Pro: Model-agnostic, pi√π affidabile                              ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Contro: Pi√π lento, sensibile a correlazioni                      ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Dove: sklearn.inspection.permutation_importance                  ‚îÇ\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îÇ  3Ô∏è‚É£  SHAP VALUES                                                        ‚îÇ\n",
    "‚îÇ      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                        ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Come funziona: Teoria dei giochi (contributo marginale)          ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Pro: Spiega OGNI predizione, molto accurato                      ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Contro: Computazionalmente costoso                               ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Dove: libreria shap                                              ‚îÇ\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ MDI ‚Äî Mean Decrease Impurity\n",
    "\n",
    "```\n",
    "Come funziona MDI:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "Per ogni nodo dell'albero che usa feature X:\n",
    "   Importance(X) += (n_samples √ó impurity_decrease)\n",
    "\n",
    "Poi viene normalizzato e mediato su tutti gli alberi.\n",
    "\n",
    "Esempio visivo:\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ  Gini=0.5   ‚îÇ\n",
    "                    ‚îÇ  Split: Age ‚îÇ ‚Üê Age contribuisce a ridurre Gini\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚îÇ\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ                         ‚îÇ\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ Gini=0.3  ‚îÇ             ‚îÇ Gini=0.2  ‚îÇ\n",
    "        ‚îÇSplit:Salary‚îÇ            ‚îÇ Pure leaf ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Age importance = 0.5 - (0.3 + 0.2)/2 = 0.25 (pesato per n_samples)\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **PROBLEMA MDI**: Features con molti valori unici (alta cardinalit√†)\n",
    "tendono ad avere importanza gonfiata!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30908cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO: MDI (Mean Decrease Impurity)\n",
    "# Feature Importance built-in degli alberi\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MDI ‚Äî Mean Decrease Impurity\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Caricamento dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "print(f\"\\nüìä Dataset: Breast Cancer Wisconsin\")\n",
    "print(f\"   - Campioni: {len(X)}\")\n",
    "print(f\"   - Features: {X.shape[1]}\")\n",
    "\n",
    "# Training Random Forest\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Random Forest addestrato (100 alberi)\")\n",
    "print(f\"   Accuracy: {rf.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Estrazione MDI importance\n",
    "mdi_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'MDI_Importance': rf.feature_importances_\n",
    "}).sort_values('MDI_Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ Top 10 Features (MDI):\")\n",
    "print(\"-\" * 40)\n",
    "for i, row in mdi_importance.head(10).iterrows():\n",
    "    bar = \"‚ñà\" * int(row['MDI_Importance'] * 50)\n",
    "    print(f\"   {row['Feature']:25} {row['MDI_Importance']:.4f} {bar}\")\n",
    "\n",
    "# Visualizzazione Top 15\n",
    "plt.figure(figsize=(10, 6))\n",
    "top15 = mdi_importance.head(15)\n",
    "colors = plt.cm.Blues(np.linspace(0.3, 0.9, 15))[::-1]\n",
    "plt.barh(top15['Feature'], top15['MDI_Importance'], color=colors, edgecolor='black')\n",
    "plt.xlabel('MDI Importance')\n",
    "plt.title('üìä Feature Importance (MDI) - Top 15')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Nota: MDI √® veloce ma pu√≤ essere biased verso features con\")\n",
    "print(\"   alta cardinalit√† (molti valori unici).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392e07a",
   "metadata": {},
   "source": [
    "# Section 2 ‚Äî Permutation Importance\n",
    "\n",
    "---\n",
    "\n",
    "## üîÄ Come Funziona\n",
    "\n",
    "```\n",
    "IDEA: Se una feature √® importante, mischiarla (shuffling) \n",
    "      dovrebbe peggiorare le predizioni.\n",
    "\n",
    "ALGORITMO:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "1. Calcola score baseline sul test set\n",
    "2. Per ogni feature X:\n",
    "   a. Mischia casualmente i valori di X\n",
    "   b. Calcola nuovo score\n",
    "   c. Importance(X) = baseline_score - shuffled_score\n",
    "3. Ripeti n volte per avere una stima robusta\n",
    "\n",
    "ESEMPIO:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                    Accuracy\n",
    "Baseline:           0.95\n",
    "Shuffle \"Age\":      0.70  ‚Üí Importance = 0.95 - 0.70 = 0.25 ‚¨ÜÔ∏è IMPORTANTE!\n",
    "Shuffle \"Color\":    0.94  ‚Üí Importance = 0.95 - 0.94 = 0.01 ‚¨áÔ∏è poco importante\n",
    "```\n",
    "\n",
    "## ‚úÖ Vantaggi della Permutation Importance\n",
    "\n",
    "| Vantaggio | Descrizione |\n",
    "|-----------|-------------|\n",
    "| **Model-agnostic** | Funziona con QUALSIASI modello |\n",
    "| **Affidabile** | Meno bias rispetto a MDI |\n",
    "| **Post-hoc** | Calcolata dopo il training, su dati reali |\n",
    "| **Interpretabile** | \"Quanto peggiora se perdo questa info?\" |\n",
    "\n",
    "## ‚ö†Ô∏è Attenzione\n",
    "\n",
    "- Features correlate possono entrambe sembrare poco importanti\n",
    "- Pi√π lenta di MDI (richiede multiple predizioni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO: Permutation Importance\n",
    "# Metodo model-agnostic e affidabile\n",
    "# ============================================\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PERMUTATION IMPORTANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Usiamo lo stesso modello RF di prima\n",
    "print(f\"\\nüìä Calcolando Permutation Importance su test set...\")\n",
    "print(f\"   (10 ripetizioni per robustezza)\")\n",
    "\n",
    "start_time = time.time()\n",
    "perm_importance = permutation_importance(\n",
    "    rf, X_test, y_test, \n",
    "    n_repeats=10,           # Numero di shuffle per feature\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "tempo = time.time() - start_time\n",
    "print(f\"   ‚úÖ Completato in {tempo:.2f} secondi\")\n",
    "\n",
    "# Creiamo DataFrame con mean e std\n",
    "perm_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Perm_Importance': perm_importance.importances_mean,\n",
    "    'Std': perm_importance.importances_std\n",
    "}).sort_values('Perm_Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ Top 10 Features (Permutation):\")\n",
    "print(\"-\" * 50)\n",
    "for i, row in perm_df.head(10).iterrows():\n",
    "    bar = \"‚ñà\" * int(row['Perm_Importance'] * 100)\n",
    "    print(f\"   {row['Feature']:25} {row['Perm_Importance']:.4f} ¬± {row['Std']:.4f} {bar}\")\n",
    "\n",
    "# Visualizzazione con error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "top15_perm = perm_df.head(15)\n",
    "colors = plt.cm.Greens(np.linspace(0.3, 0.9, 15))[::-1]\n",
    "plt.barh(top15_perm['Feature'], top15_perm['Perm_Importance'], \n",
    "         xerr=top15_perm['Std'], color=colors, edgecolor='black', capsize=3)\n",
    "plt.xlabel('Permutation Importance (decrease in accuracy)')\n",
    "plt.title('üîÄ Permutation Importance - Top 15 (con deviazione std)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Le barre di errore mostrano la variabilit√† tra le 10 ripetizioni.\")\n",
    "print(\"   Features con alta std potrebbero essere meno stabili.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d112668",
   "metadata": {},
   "source": [
    "# Section 3 ‚Äî Confronto MDI vs Permutation\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Quando i Due Metodi Divergono\n",
    "\n",
    "```\n",
    "SCENARIO PROBLEMATICO: Features ad Alta Cardinalit√†\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "Esempio: Feature \"ID Cliente\" (unico per ogni riga)\n",
    "\n",
    "MDI dice: \"ID √® MOLTO importante!\" üò±\n",
    "   ‚Üí Perch√© crea split perfetti (ogni ID = una foglia)\n",
    "   ‚Üí Ma non generalizza! √à overfitting puro.\n",
    "\n",
    "Permutation dice: \"ID √® inutile\" ‚úÖ\n",
    "   ‚Üí Perch√© su dati nuovi, shuffle di ID non cambia nulla\n",
    "   ‚Üí Correttamente identifica che ID non ha potere predittivo.\n",
    "```\n",
    "\n",
    "## üìä Regola Pratica\n",
    "\n",
    "| Situazione | Metodo Consigliato |\n",
    "|------------|-------------------|\n",
    "| Quick check, dati puliti | MDI (.feature_importances_) |\n",
    "| Report finale, decisioni business | Permutation Importance |\n",
    "| Features con cardinalit√† diverse | Permutation Importance |\n",
    "| Modelli non-tree (SVM, NN) | Permutation Importance |\n",
    "| Spiegare singole predizioni | SHAP (prossima sezione) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO: Confronto MDI vs Permutation Importance\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFRONTO: MDI vs PERMUTATION IMPORTANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Merge dei due DataFrame\n",
    "confronto = mdi_importance.merge(\n",
    "    perm_df[['Feature', 'Perm_Importance']], \n",
    "    on='Feature'\n",
    ")\n",
    "\n",
    "# Calcolo ranking\n",
    "confronto['Rank_MDI'] = range(1, len(confronto) + 1)\n",
    "confronto = confronto.sort_values('Perm_Importance', ascending=False)\n",
    "confronto['Rank_Perm'] = range(1, len(confronto) + 1)\n",
    "confronto['Rank_Diff'] = abs(confronto['Rank_MDI'] - confronto['Rank_Perm'])\n",
    "\n",
    "# Ordiniamo per MDI per il confronto\n",
    "confronto = confronto.sort_values('MDI_Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Confronto Top 15 Features:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Feature':25} {'MDI':>8} {'Rank':>5} {'Perm':>8} {'Rank':>5} {'Œî Rank':>7}\")\n",
    "print(\"-\" * 70)\n",
    "for _, row in confronto.head(15).iterrows():\n",
    "    delta = int(row['Rank_Diff'])\n",
    "    emoji = \"‚úÖ\" if delta <= 2 else \"‚ö†Ô∏è\" if delta <= 5 else \"‚ùå\"\n",
    "    print(f\"{row['Feature']:25} {row['MDI_Importance']:>8.4f} {int(row['Rank_MDI']):>5} \"\n",
    "          f\"{row['Perm_Importance']:>8.4f} {int(row['Rank_Perm']):>5} {delta:>5} {emoji}\")\n",
    "\n",
    "# Visualizzazione scatter plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter: MDI vs Permutation\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(confronto['MDI_Importance'], confronto['Perm_Importance'], \n",
    "            alpha=0.7, s=80, c='steelblue', edgecolor='black')\n",
    "\n",
    "# Linea di correlazione perfetta\n",
    "max_val = max(confronto['MDI_Importance'].max(), confronto['Perm_Importance'].max())\n",
    "ax1.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Correlazione perfetta')\n",
    "\n",
    "# Annotazioni per top 5\n",
    "for _, row in confronto.head(5).iterrows():\n",
    "    ax1.annotate(row['Feature'].split()[0], \n",
    "                 (row['MDI_Importance'], row['Perm_Importance']),\n",
    "                 fontsize=9, alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('MDI Importance')\n",
    "ax1.set_ylabel('Permutation Importance')\n",
    "ax1.set_title('üìä Correlazione tra MDI e Permutation')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Confronto ranking side by side\n",
    "ax2 = axes[1]\n",
    "top10_mdi = confronto.nsmallest(10, 'Rank_MDI')\n",
    "y_pos = np.arange(10)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.barh(y_pos - width/2, top10_mdi['Rank_MDI'], width, \n",
    "                  label='Rank MDI', color='#3498db', alpha=0.8)\n",
    "bars2 = ax2.barh(y_pos + width/2, top10_mdi['Rank_Perm'], width, \n",
    "                  label='Rank Permutation', color='#2ecc71', alpha=0.8)\n",
    "\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(top10_mdi['Feature'])\n",
    "ax2.set_xlabel('Ranking (1 = pi√π importante)')\n",
    "ax2.set_title('üìä Confronto Ranking Top 10')\n",
    "ax2.legend()\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcolo correlazione\n",
    "from scipy.stats import spearmanr\n",
    "corr, pval = spearmanr(confronto['MDI_Importance'], confronto['Perm_Importance'])\n",
    "print(f\"\\nüìà Correlazione di Spearman (ranking): {corr:.4f}\")\n",
    "print(f\"   p-value: {pval:.2e}\")\n",
    "\n",
    "if corr > 0.8:\n",
    "    print(\"   ‚úÖ Alta correlazione: i due metodi concordano!\")\n",
    "elif corr > 0.5:\n",
    "    print(\"   ‚ö†Ô∏è Correlazione moderata: alcune discrepanze\")\n",
    "else:\n",
    "    print(\"   ‚ùå Bassa correlazione: investigare le differenze!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e9170",
   "metadata": {},
   "source": [
    "# Section 4 ‚Äî SHAP Values (Introduzione)\n",
    "\n",
    "---\n",
    "\n",
    "## üéÆ La Teoria dei Giochi Applicata al ML\n",
    "\n",
    "**SHAP** = **SH**apley **A**dditive ex**P**lanations\n",
    "\n",
    "```\n",
    "IDEA CENTRALE:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "SHAP risponde alla domanda:\n",
    "\"Quanto ha contribuito OGNI feature alla predizione di QUESTO campione?\"\n",
    "\n",
    "√à come dividere equamente le vincite tra giocatori di una squadra,\n",
    "in base al loro contributo marginale.\n",
    "\n",
    "ESEMPIO:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Predizione: Probabilit√† cancro = 85%\n",
    "Media dataset: 50%\n",
    "\n",
    "SHAP spiega i +35%:\n",
    "   worst concave points: +20%  (il pi√π influente per QUESTO paziente)\n",
    "   mean radius:          +10%\n",
    "   worst perimeter:       +5%\n",
    "   altri:                  0%\n",
    "\n",
    "‚Üí Possiamo dire PERCH√â il modello ha dato questa predizione!\n",
    "```\n",
    "\n",
    "## üìä Tipi di Visualizzazioni SHAP\n",
    "\n",
    "| Tipo | Cosa Mostra |\n",
    "|------|-------------|\n",
    "| **Summary Plot** | Importanza globale + distribuzione impatto |\n",
    "| **Beeswarm Plot** | Ogni punto = un campione |\n",
    "| **Waterfall Plot** | Spiegazione di UNA predizione |\n",
    "| **Dependence Plot** | Relazione feature-SHAP (come partial dependence) |\n",
    "\n",
    "## ‚ö†Ô∏è Nota\n",
    "\n",
    "SHAP richiede l'installazione della libreria: `pip install shap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9713caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO: SHAP Values\n",
    "# Spiegabilit√† avanzata del modello\n",
    "# ============================================\n",
    "\n",
    "# Nota: Richiede 'pip install shap'\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_DISPONIBILE = True\n",
    "except ImportError:\n",
    "    SHAP_DISPONIBILE = False\n",
    "    print(\"‚ö†Ô∏è SHAP non installato. Esegui: pip install shap\")\n",
    "\n",
    "if SHAP_DISPONIBILE:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SHAP VALUES ‚Äî Spiegabilit√† Avanzata\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Creiamo uno SHAP explainer per il nostro Random Forest\n",
    "    print(\"\\nüìä Creando SHAP explainer...\")\n",
    "    explainer = shap.TreeExplainer(rf)\n",
    "    \n",
    "    # Calcoliamo SHAP values per il test set\n",
    "    print(\"   Calcolando SHAP values (potrebbe richiedere qualche secondo)...\")\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    print(\"   ‚úÖ Completato!\")\n",
    "    \n",
    "    # Per classificazione binaria, prendiamo i SHAP della classe positiva\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_vals = shap_values[1]  # Classe positiva\n",
    "    else:\n",
    "        shap_vals = shap_values\n",
    "    \n",
    "    # 1. Summary Plot - Importanza globale\n",
    "    print(\"\\nüìà Summary Plot: mostra importanza E direzione dell'effetto\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_vals, X_test, feature_names=X.columns, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Bar plot - Importanza media assoluta\n",
    "    print(\"\\nüìä Bar Plot: importanza media (valore assoluto)\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_vals, X_test, feature_names=X.columns, \n",
    "                      plot_type=\"bar\", show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Waterfall per singola predizione\n",
    "    print(\"\\nüîç Waterfall Plot: spiegazione di UNA predizione specifica\")\n",
    "    print(\"   (Esempio: primo paziente del test set)\")\n",
    "    \n",
    "    # Creiamo Explanation object per il primo campione\n",
    "    sample_idx = 0\n",
    "    expected_value = explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.waterfall_plot(\n",
    "        shap.Explanation(\n",
    "            values=shap_vals[sample_idx],\n",
    "            base_values=expected_value,\n",
    "            data=X_test.iloc[sample_idx].values,\n",
    "            feature_names=X.columns.tolist()\n",
    "        ),\n",
    "        show=False\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Interpretazione\n",
    "    pred_proba = rf.predict_proba(X_test.iloc[[sample_idx]])[0][1]\n",
    "    pred_class = rf.predict(X_test.iloc[[sample_idx]])[0]\n",
    "    true_class = y_test.iloc[sample_idx] if hasattr(y_test, 'iloc') else y_test[sample_idx]\n",
    "    \n",
    "    print(f\"\\nüìã Analisi paziente #{sample_idx}:\")\n",
    "    print(f\"   - Predizione: {'Benigno' if pred_class == 1 else 'Maligno'} (prob: {pred_proba:.2%})\")\n",
    "    print(f\"   - Classe vera: {'Benigno' if true_class == 1 else 'Maligno'}\")\n",
    "    print(f\"   - Il waterfall mostra COME si arriva a questa predizione\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Per vedere SHAP in azione, installa la libreria:\")\n",
    "    print(\"   pip install shap\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45717b3d",
   "metadata": {},
   "source": [
    "# Section 5 ‚Äî Feature Selection con Importance\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Usare Feature Importance per Selezionare Features\n",
    "\n",
    "```\n",
    "WORKFLOW DI FEATURE SELECTION:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "1. Addestra modello con TUTTE le features\n",
    "2. Calcola Permutation Importance\n",
    "3. Rimuovi features con importanza ‚âà 0 o negativa\n",
    "4. Riaddestra e confronta performance\n",
    "\n",
    "BENEFICI:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚úÖ Modello pi√π semplice e interpretabile\n",
    "‚úÖ Meno overfitting (meno parametri)\n",
    "‚úÖ Training/inference pi√π veloci\n",
    "‚úÖ Meno dati da raccogliere in produzione\n",
    "```\n",
    "\n",
    "## ‚ö†Ô∏è Attenzione alle Features Correlate\n",
    "\n",
    "```\n",
    "PROBLEMA:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Se A e B sono altamente correlate:\n",
    "   - Shuffling di A: B \"compensa\", score non peggiora molto\n",
    "   - Shuffling di B: A \"compensa\", score non peggiora molto\n",
    "   ‚Üí Entrambe sembrano poco importanti, ma insieme sono cruciali!\n",
    "\n",
    "SOLUZIONE:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "1. Rimuovere una delle due PRIMA di calcolare importance\n",
    "2. Usare tecniche di clustering per features\n",
    "3. Considerare gruppi di features correlate insieme\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f83bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO: Feature Selection basata su Importance\n",
    "# ============================================\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE SELECTION CON IMPORTANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Dataset originale: {X_train.shape[1]} features\")\n",
    "\n",
    "# Metodo 1: Soglia basata su importanza media\n",
    "mean_importance = perm_df['Perm_Importance'].mean()\n",
    "features_sopra_media = perm_df[perm_df['Perm_Importance'] > mean_importance]['Feature'].tolist()\n",
    "\n",
    "print(f\"\\nüìå Metodo 1: Soglia = media importance ({mean_importance:.4f})\")\n",
    "print(f\"   Features selezionate: {len(features_sopra_media)}\")\n",
    "\n",
    "# Metodo 2: Top K features\n",
    "K = 10\n",
    "top_k_features = perm_df.head(K)['Feature'].tolist()\n",
    "print(f\"\\nüìå Metodo 2: Top {K} features\")\n",
    "print(f\"   Features selezionate: {top_k_features[:5]}...\")\n",
    "\n",
    "# Metodo 3: SelectFromModel di sklearn (usa MDI)\n",
    "selector = SelectFromModel(rf, threshold='median', prefit=True)\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "print(f\"\\nüìå Metodo 3: SelectFromModel (threshold='median')\")\n",
    "print(f\"   Features selezionate: {X_train_selected.shape[1]}\")\n",
    "\n",
    "# Confronto performance: tutte le features vs top K\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONFRONTO PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Modello con tutte le features (gi√† trainato)\n",
    "acc_full = rf.score(X_test, y_test)\n",
    "\n",
    "# Modello con top K features\n",
    "rf_topk = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_topk.fit(X_train[top_k_features], y_train)\n",
    "acc_topk = rf_topk.score(X_test[top_k_features], y_test)\n",
    "\n",
    "# Modello con SelectFromModel\n",
    "rf_selected = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "acc_selected = rf_selected.score(X_test_selected, y_test)\n",
    "\n",
    "print(\"\\nüìä Risultati:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   {'Modello':<30} {'Features':>10} {'Accuracy':>10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   {'Tutte le features':<30} {X.shape[1]:>10} {acc_full:>10.4f}\")\n",
    "print(f\"   {'Top 10 (Permutation)':<30} {K:>10} {acc_topk:>10.4f}\")\n",
    "print(f\"   {'SelectFromModel (median)':<30} {X_train_selected.shape[1]:>10} {acc_selected:>10.4f}\")\n",
    "\n",
    "# Calcolo riduzione\n",
    "riduzione_topk = (1 - K / X.shape[1]) * 100\n",
    "perdita_topk = (acc_full - acc_topk) * 100\n",
    "\n",
    "print(f\"\\nüí° Insight:\")\n",
    "print(f\"   Con Top 10 features (-{riduzione_topk:.0f}% features):\")\n",
    "if perdita_topk > 0:\n",
    "    print(f\"   ‚Üí Perdita accuracy: {perdita_topk:.2f}%\")\n",
    "else:\n",
    "    print(f\"   ‚Üí Guadagno accuracy: {-perdita_topk:.2f}%!\")\n",
    "print(f\"   ‚Üí Trade-off accettabile? Dipende dal contesto!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5243d40",
   "metadata": {},
   "source": [
    "# Section 6 ‚Äî Schema Mentale\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Quale Metodo di Feature Importance Usare?\n",
    "\n",
    "```\n",
    "                    SCEGLI IL METODO GIUSTO\n",
    "                    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "            ‚îÇ   Hai bisogno di spiegare       ‚îÇ\n",
    "            ‚îÇ   SINGOLE predizioni?           ‚îÇ\n",
    "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ                               ‚îÇ\n",
    "              ‚ñº                               ‚ñº\n",
    "            [ S√¨ ]                          [ No ]\n",
    "              ‚îÇ                               ‚îÇ\n",
    "              ‚ñº                               ‚ñº\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  SHAP   ‚îÇ             ‚îÇ Il modello √®       ‚îÇ\n",
    "         ‚îÇ Values  ‚îÇ             ‚îÇ tree-based?         ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                            ‚îÇ\n",
    "                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                          ‚îÇ                                   ‚îÇ\n",
    "                          ‚ñº                                   ‚ñº\n",
    "                        [ S√¨ ]                              [ No ]\n",
    "                          ‚îÇ                                   ‚îÇ\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚ñº\n",
    "              ‚îÇ                       ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚ñº                       ‚ñº              ‚îÇ   PERMUTATION    ‚îÇ\n",
    "         Quick check?           Report finale?       ‚îÇ   IMPORTANCE     ‚îÇ\n",
    "              ‚îÇ                       ‚îÇ              ‚îÇ (model-agnostic) ‚îÇ\n",
    "              ‚ñº                       ‚ñº              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ   MDI   ‚îÇ          ‚îÇ   PERMUTATION    ‚îÇ\n",
    "         ‚îÇ (fast)  ‚îÇ          ‚îÇ   IMPORTANCE     ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## üìã Tabella Riassuntiva\n",
    "\n",
    "| Metodo | Velocit√† | Affidabilit√† | Model-Agnostic | Spiega Singole Pred. |\n",
    "|--------|----------|--------------|----------------|---------------------|\n",
    "| MDI | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | ‚ùå | ‚ùå |\n",
    "| Permutation | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚ùå |\n",
    "| SHAP | ‚ö° | ‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚úÖ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d5e75",
   "metadata": {},
   "source": [
    "# Section 7 ‚Äî Esercizi Svolti\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Esercizio 16.1 ‚Äî Confronto Importance con XGBoost\n",
    "\n",
    "**Obiettivo**: Calcolare e confrontare MDI e Permutation Importance su XGBoost.\n",
    "\n",
    "**Consegna**:\n",
    "1. Addestra un modello XGBoost sul dataset California Housing\n",
    "2. Calcola MDI importance (built-in XGBoost)\n",
    "3. Calcola Permutation Importance\n",
    "4. Confronta i due ranking\n",
    "5. Identifica le top 5 features secondo entrambi i metodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a27585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 16.1 ‚Äî SOLUZIONE COMPLETA\n",
    "# Confronto Importance con XGBoost\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ESERCIZIO 16.1 ‚Äî Feature Importance con XGBoost\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Caricamento dataset\n",
    "california = fetch_california_housing()\n",
    "X = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "y = california.target\n",
    "\n",
    "print(f\"\\nüìä Dataset: California Housing\")\n",
    "print(f\"   - Campioni: {len(X)}\")\n",
    "print(f\"   - Features: {list(X.columns)}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Training XGBoost\n",
    "print(\"\\nüèãÔ∏è Training XGBoost...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    verbosity=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"   ‚úÖ XGBoost addestrato\")\n",
    "print(f\"   R¬≤ Score: {xgb_model.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# 3. MDI Importance (built-in XGBoost)\n",
    "mdi_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'MDI': xgb_model.feature_importances_\n",
    "}).sort_values('MDI', ascending=False)\n",
    "mdi_importance['Rank_MDI'] = range(1, len(mdi_importance) + 1)\n",
    "\n",
    "print(\"\\nüìä MDI Importance (built-in XGBoost):\")\n",
    "print(\"-\" * 40)\n",
    "for _, row in mdi_importance.iterrows():\n",
    "    bar = \"‚ñà\" * int(row['MDI'] * 30)\n",
    "    print(f\"   {row['Feature']:12} {row['MDI']:.4f} {bar}\")\n",
    "\n",
    "# 4. Permutation Importance\n",
    "print(\"\\nüîÄ Calcolando Permutation Importance...\")\n",
    "perm_result = permutation_importance(\n",
    "    xgb_model, X_test, y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "perm_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Perm': perm_result.importances_mean,\n",
    "    'Std': perm_result.importances_std\n",
    "}).sort_values('Perm', ascending=False)\n",
    "perm_importance['Rank_Perm'] = range(1, len(perm_importance) + 1)\n",
    "\n",
    "print(\"\\nüìä Permutation Importance:\")\n",
    "print(\"-\" * 40)\n",
    "for _, row in perm_importance.iterrows():\n",
    "    bar = \"‚ñà\" * int(row['Perm'] * 5)\n",
    "    print(f\"   {row['Feature']:12} {row['Perm']:.4f} ¬± {row['Std']:.4f} {bar}\")\n",
    "\n",
    "# 5. Confronto ranking\n",
    "confronto = mdi_importance.merge(perm_importance[['Feature', 'Perm', 'Rank_Perm']], on='Feature')\n",
    "confronto = confronto.sort_values('MDI', ascending=False)\n",
    "\n",
    "# Visualizzazione\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# MDI\n",
    "ax1 = axes[0]\n",
    "colors_mdi = plt.cm.Blues(np.linspace(0.3, 0.9, len(mdi_importance)))[::-1]\n",
    "ax1.barh(mdi_importance['Feature'], mdi_importance['MDI'], color=colors_mdi, edgecolor='black')\n",
    "ax1.set_xlabel('MDI Importance')\n",
    "ax1.set_title('üìä MDI (XGBoost built-in)')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Permutation\n",
    "ax2 = axes[1]\n",
    "perm_sorted = perm_importance.sort_values('Perm', ascending=True)\n",
    "colors_perm = plt.cm.Greens(np.linspace(0.3, 0.9, len(perm_sorted)))\n",
    "ax2.barh(perm_sorted['Feature'], perm_sorted['Perm'], \n",
    "         xerr=perm_sorted['Std'], color=colors_perm, edgecolor='black', capsize=3)\n",
    "ax2.set_xlabel('Permutation Importance')\n",
    "ax2.set_title('üîÄ Permutation Importance')\n",
    "\n",
    "# Confronto ranking\n",
    "ax3 = axes[2]\n",
    "y_pos = np.arange(len(confronto))\n",
    "width = 0.35\n",
    "ax3.barh(y_pos - width/2, confronto['Rank_MDI'], width, label='Rank MDI', color='#3498db')\n",
    "ax3.barh(y_pos + width/2, confronto['Rank_Perm'], width, label='Rank Perm', color='#2ecc71')\n",
    "ax3.set_yticks(y_pos)\n",
    "ax3.set_yticklabels(confronto['Feature'])\n",
    "ax3.set_xlabel('Ranking (1 = pi√π importante)')\n",
    "ax3.set_title('üìä Confronto Ranking')\n",
    "ax3.legend()\n",
    "ax3.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Risultato finale\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üèÜ TOP 5 FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n   Secondo MDI:         Secondo Permutation:\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(5):\n",
    "    mdi_feat = mdi_importance.iloc[i]['Feature']\n",
    "    perm_feat = perm_importance.iloc[i]['Feature']\n",
    "    match = \"‚úÖ\" if mdi_feat == perm_feat else \"‚ö†Ô∏è\"\n",
    "    print(f\"   {i+1}. {mdi_feat:12}    {i+1}. {perm_feat:12} {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8f9cc1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Esercizio 16.2 ‚Äî Il Problema delle Features Correlate\n",
    "\n",
    "**Obiettivo**: Dimostrare come le features correlate influenzano la Permutation Importance.\n",
    "\n",
    "**Consegna**:\n",
    "1. Crea un dataset sintetico con 2 features altamente correlate (r > 0.9)\n",
    "2. Addestra un modello e calcola Permutation Importance\n",
    "3. Rimuovi una delle due features correlate\n",
    "4. Ricalcola l'importance\n",
    "5. Confronta i risultati e spiega il fenomeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 16.2 ‚Äî SOLUZIONE COMPLETA\n",
    "# Il Problema delle Features Correlate\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ESERCIZIO 16.2 ‚Äî Features Correlate e Permutation Importance\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Creazione dataset con features correlate\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Feature base (informativa)\n",
    "feature_A = np.random.randn(n_samples)\n",
    "\n",
    "# Feature B: quasi identica ad A (correlazione ~0.95)\n",
    "noise = np.random.randn(n_samples) * 0.3\n",
    "feature_B = feature_A + noise\n",
    "\n",
    "# Feature C: indipendente, anche informativa\n",
    "feature_C = np.random.randn(n_samples)\n",
    "\n",
    "# Feature D: rumore puro\n",
    "feature_D = np.random.randn(n_samples)\n",
    "\n",
    "# Target: dipende da A (o B, sono quasi uguali) e C\n",
    "y = ((feature_A + feature_C) > 0).astype(int)\n",
    "\n",
    "# Dataset completo\n",
    "X_full = pd.DataFrame({\n",
    "    'Feature_A': feature_A,\n",
    "    'Feature_B': feature_B,\n",
    "    'Feature_C': feature_C,\n",
    "    'Feature_D': feature_D\n",
    "})\n",
    "\n",
    "# Calcolo correlazione\n",
    "corr_AB = np.corrcoef(feature_A, feature_B)[0, 1]\n",
    "print(f\"\\nüìä Dataset creato:\")\n",
    "print(f\"   - Feature_A: informativa (base)\")\n",
    "print(f\"   - Feature_B: copia rumorosa di A (corr = {corr_AB:.4f})\")\n",
    "print(f\"   - Feature_C: informativa, indipendente\")\n",
    "print(f\"   - Feature_D: rumore puro\")\n",
    "\n",
    "# 2. Training con tutte le features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_full = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_full.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nüèãÔ∏è Modello con TUTTE le features:\")\n",
    "print(f\"   Accuracy: {rf_full.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Permutation Importance con tutte le features\n",
    "perm_full = permutation_importance(rf_full, X_test, y_test, n_repeats=30, random_state=42, n_jobs=-1)\n",
    "perm_full_df = pd.DataFrame({\n",
    "    'Feature': X_full.columns,\n",
    "    'Importance': perm_full.importances_mean,\n",
    "    'Std': perm_full.importances_std\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Permutation Importance (TUTTE le features):\")\n",
    "print(\"-\" * 50)\n",
    "for _, row in perm_full_df.iterrows():\n",
    "    bar = \"‚ñà\" * int(row['Importance'] * 20)\n",
    "    print(f\"   {row['Feature']:12} {row['Importance']:.4f} ¬± {row['Std']:.4f} {bar}\")\n",
    "\n",
    "# 3. Rimuoviamo Feature_B (la copia) e ritestiamo\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìå ORA RIMUOVIAMO Feature_B (la copia correlata)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_reduced = X_full.drop('Feature_B', axis=1)\n",
    "X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(\n",
    "    X_reduced, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "rf_reduced = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_reduced.fit(X_train_red, y_train_red)\n",
    "\n",
    "print(f\"\\nüèãÔ∏è Modello SENZA Feature_B:\")\n",
    "print(f\"   Accuracy: {rf_reduced.score(X_test_red, y_test_red):.4f}\")\n",
    "\n",
    "# Permutation Importance senza B\n",
    "perm_red = permutation_importance(rf_reduced, X_test_red, y_test_red, n_repeats=30, random_state=42, n_jobs=-1)\n",
    "perm_red_df = pd.DataFrame({\n",
    "    'Feature': X_reduced.columns,\n",
    "    'Importance': perm_red.importances_mean,\n",
    "    'Std': perm_red.importances_std\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Permutation Importance (SENZA Feature_B):\")\n",
    "print(\"-\" * 50)\n",
    "for _, row in perm_red_df.iterrows():\n",
    "    bar = \"‚ñà\" * int(row['Importance'] * 20)\n",
    "    print(f\"   {row['Feature']:12} {row['Importance']:.4f} ¬± {row['Std']:.4f} {bar}\")\n",
    "\n",
    "# 4. Visualizzazione confronto\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Con tutte le features\n",
    "ax1 = axes[0]\n",
    "colors = ['#e74c3c' if 'A' in f or 'B' in f else '#3498db' for f in perm_full_df['Feature']]\n",
    "ax1.barh(perm_full_df['Feature'], perm_full_df['Importance'], \n",
    "         xerr=perm_full_df['Std'], color=colors, edgecolor='black', capsize=3)\n",
    "ax1.set_xlabel('Permutation Importance')\n",
    "ax1.set_title('üìä CON Feature_B (correlata ad A)')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Senza B\n",
    "ax2 = axes[1]\n",
    "colors_red = ['#e74c3c' if 'A' in f else '#3498db' for f in perm_red_df['Feature']]\n",
    "ax2.barh(perm_red_df['Feature'], perm_red_df['Importance'], \n",
    "         xerr=perm_red_df['Std'], color=colors_red, edgecolor='black', capsize=3)\n",
    "ax2.set_xlabel('Permutation Importance')\n",
    "ax2.set_title('üìä SENZA Feature_B')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Spiegazione\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° SPIEGAZIONE DEL FENOMENO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "imp_A_full = perm_full_df[perm_full_df['Feature'] == 'Feature_A']['Importance'].values[0]\n",
    "imp_B_full = perm_full_df[perm_full_df['Feature'] == 'Feature_B']['Importance'].values[0]\n",
    "imp_A_red = perm_red_df[perm_red_df['Feature'] == 'Feature_A']['Importance'].values[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "üîç Cosa √® successo:\n",
    "\n",
    "   CON B presente:\n",
    "   - Feature_A importance: {imp_A_full:.4f}\n",
    "   - Feature_B importance: {imp_B_full:.4f}\n",
    "   ‚Üí Entrambe sembrano meno importanti perch√© si \"compensano\"!\n",
    "   \n",
    "   SENZA B:\n",
    "   - Feature_A importance: {imp_A_red:.4f}\n",
    "   ‚Üí Ora A mostra la sua vera importanza!\n",
    "\n",
    "üìå LEZIONE APPRESA:\n",
    "   Quando shuffliamo A, B \"copre\" l'informazione persa (sono correlate).\n",
    "   Quando shuffliamo B, A \"copre\" l'informazione persa.\n",
    "   Risultato: entrambe sembrano meno importanti di quanto siano realmente!\n",
    "   \n",
    "‚ö†Ô∏è SOLUZIONE PRATICA:\n",
    "   1. Prima di calcolare importance, rimuovi features molto correlate\n",
    "   2. Oppure usa tecniche come clustering di features\n",
    "   3. Oppure interpreta l'importance a livello di GRUPPO di features\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae24de8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Esercizio 16.3 ‚Äî Feature Selection Automatica\n",
    "\n",
    "**Obiettivo**: Automatizzare la selezione delle features basandosi sull'importance.\n",
    "\n",
    "**Consegna**:\n",
    "1. Carica un dataset con molte features (es. digits o altro)\n",
    "2. Trova il numero ottimale di features tramite validazione\n",
    "3. Crea una curva accuracy vs numero features\n",
    "4. Identifica il \"gomito\" dove aggiungere features non aiuta pi√π\n",
    "5. Addestra il modello finale con le features selezionate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 16.3 ‚Äî SOLUZIONE COMPLETA\n",
    "# Feature Selection Automatica\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ESERCIZIO 16.3 ‚Äî Feature Selection Automatica\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Caricamento dataset digits (64 features = pixel 8x8)\n",
    "digits = load_digits()\n",
    "X = pd.DataFrame(digits.data, columns=[f'pixel_{i}' for i in range(64)])\n",
    "y = digits.target\n",
    "\n",
    "print(f\"\\nüìä Dataset: Digits (cifre scritte a mano)\")\n",
    "print(f\"   - Campioni: {len(X)}\")\n",
    "print(f\"   - Features: {X.shape[1]} (pixel 8x8)\")\n",
    "print(f\"   - Classi: 0-9\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Calcolo importance per ranking\n",
    "print(\"\\nüîÄ Calcolando Permutation Importance per ranking features...\")\n",
    "rf_base = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "rf_base.fit(X_train, y_train)\n",
    "\n",
    "perm_result = permutation_importance(rf_base, X_test, y_test, n_repeats=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "feature_ranking = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': perm_result.importances_mean\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"   ‚úÖ Features ordinate per importanza\")\n",
    "\n",
    "# 3. Curva accuracy vs numero features\n",
    "print(\"\\nüìà Costruendo curva Accuracy vs Numero Features...\")\n",
    "\n",
    "n_features_to_test = [1, 2, 3, 5, 8, 10, 15, 20, 25, 30, 40, 50, 64]\n",
    "results = []\n",
    "\n",
    "for n_feat in n_features_to_test:\n",
    "    # Selezioniamo le top n features\n",
    "    top_features = feature_ranking.head(n_feat)['Feature'].tolist()\n",
    "    \n",
    "    # Training con cross-validation\n",
    "    rf_temp = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "    cv_scores = cross_val_score(rf_temp, X_train[top_features], y_train, cv=5)\n",
    "    \n",
    "    results.append({\n",
    "        'n_features': n_feat,\n",
    "        'mean_accuracy': cv_scores.mean(),\n",
    "        'std_accuracy': cv_scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"   {n_feat:2} features ‚Üí Accuracy: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 4. Visualizzazione e identificazione gomito\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Curva accuracy\n",
    "ax1 = axes[0]\n",
    "ax1.errorbar(results_df['n_features'], results_df['mean_accuracy'], \n",
    "             yerr=results_df['std_accuracy'], marker='o', linewidth=2, \n",
    "             capsize=4, color='#3498db', markersize=8)\n",
    "ax1.set_xlabel('Numero di Features')\n",
    "ax1.set_ylabel('CV Accuracy')\n",
    "ax1.set_title('üìà Accuracy vs Numero Features')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Identificazione gomito (dove il guadagno marginale diventa <1%)\n",
    "marginal_gains = results_df['mean_accuracy'].diff()\n",
    "results_df['marginal_gain'] = marginal_gains\n",
    "\n",
    "# Trova dove il guadagno marginale scende sotto 0.005 (0.5%)\n",
    "threshold = 0.005\n",
    "gomito_idx = None\n",
    "for i, gain in enumerate(marginal_gains[1:], 1):\n",
    "    if gain < threshold:\n",
    "        gomito_idx = i\n",
    "        break\n",
    "\n",
    "if gomito_idx is None:\n",
    "    gomito_idx = len(results_df) - 1\n",
    "\n",
    "gomito_n_feat = results_df.iloc[gomito_idx]['n_features']\n",
    "gomito_accuracy = results_df.iloc[gomito_idx]['mean_accuracy']\n",
    "\n",
    "ax1.axvline(x=gomito_n_feat, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Gomito: {int(gomito_n_feat)} features')\n",
    "ax1.legend()\n",
    "\n",
    "# Guadagno marginale\n",
    "ax2 = axes[1]\n",
    "ax2.bar(results_df['n_features'][1:], marginal_gains[1:] * 100, \n",
    "        color='#2ecc71', edgecolor='black', alpha=0.8)\n",
    "ax2.axhline(y=threshold*100, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Soglia: {threshold*100}%')\n",
    "ax2.set_xlabel('Numero di Features')\n",
    "ax2.set_ylabel('Guadagno Marginale (%)')\n",
    "ax2.set_title('üìä Guadagno Marginale per Feature Aggiunta')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Modello finale\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üèÜ MODELLO FINALE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scegliamo un punto ragionevole (es. 20 features)\n",
    "n_optimal = 20  # Buon trade-off basato sulla curva\n",
    "optimal_features = feature_ranking.head(n_optimal)['Feature'].tolist()\n",
    "\n",
    "rf_final = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_final.fit(X_train[optimal_features], y_train)\n",
    "\n",
    "acc_final = rf_final.score(X_test[optimal_features], y_test)\n",
    "\n",
    "# Confronto con tutte le features\n",
    "rf_all = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_all.fit(X_train, y_train)\n",
    "acc_all = rf_all.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nüìä Confronto:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   {'Modello':<30} {'Features':>10} {'Accuracy':>10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   {'Tutte le features':<30} {64:>10} {acc_all:>10.4f}\")\n",
    "print(f\"   {'Top 20 features':<30} {n_optimal:>10} {acc_final:>10.4f}\")\n",
    "\n",
    "perdita = (acc_all - acc_final) * 100\n",
    "risparmio = (1 - n_optimal/64) * 100\n",
    "\n",
    "print(f\"\\nüí° Risultato:\")\n",
    "print(f\"   - Risparmio features: {risparmio:.1f}%\")\n",
    "print(f\"   - Perdita accuracy: {perdita:.2f}%\")\n",
    "\n",
    "if perdita < 1:\n",
    "    print(f\"   ‚úÖ Ottimo trade-off! Meno features, stessa performance.\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Perdita significativa. Considerare pi√π features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3cfc5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìö Conclusione ‚Äî Lezione 16\n",
    "\n",
    "## üéØ Cosa abbiamo imparato\n",
    "\n",
    "In questa lezione abbiamo esplorato i metodi per capire **quali features sono importanti** per il modello:\n",
    "\n",
    "### Concetti Chiave:\n",
    "\n",
    "| Concetto | Descrizione |\n",
    "|----------|-------------|\n",
    "| **Feature Importance** | Misura del contributo di ogni feature alle predizioni |\n",
    "| **MDI** | Mean Decrease Impurity, built-in negli alberi, veloce ma biased |\n",
    "| **Permutation Importance** | Shuffle e misura peggioramento, model-agnostic |\n",
    "| **SHAP** | Teoria dei giochi, spiega ogni singola predizione |\n",
    "| **Feature Selection** | Rimuovere features inutili per semplificare il modello |\n",
    "| **Features Correlate** | Possono mascherare la loro vera importanza |\n",
    "\n",
    "---\n",
    "\n",
    "## üìù BIGNAMI ‚Äî Feature Importance\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    FEATURE IMPORTANCE                            ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  MDI (Mean Decrease Impurity)                                   ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ model.feature_importances_                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Veloce (calcolato durante training)                          ‚îÇ\n",
    "‚îÇ  ‚Ä¢ ‚ö†Ô∏è Bias verso alta cardinalit√†                               ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  PERMUTATION IMPORTANCE                                         ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                         ‚îÇ\n",
    "‚îÇ  ‚Ä¢ from sklearn.inspection import permutation_importance        ‚îÇ\n",
    "‚îÇ  ‚Ä¢ perm_importance(model, X_test, y_test, n_repeats=10)        ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Model-agnostic, pi√π affidabile                               ‚îÇ\n",
    "‚îÇ  ‚Ä¢ ‚ö†Ô∏è Lento, problemi con features correlate                   ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îÇ  SHAP VALUES                                                    ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                    ‚îÇ\n",
    "‚îÇ  ‚Ä¢ import shap; explainer = shap.TreeExplainer(model)          ‚îÇ\n",
    "‚îÇ  ‚Ä¢ shap_values = explainer.shap_values(X)                       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Spiega OGNI predizione singolarmente                        ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  QUANDO USARE COSA:                                             ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Quick check           ‚Üí MDI                                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Report finale         ‚Üí Permutation Importance               ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Spiegare predizioni   ‚Üí SHAP                                 ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Modelli non-tree      ‚Üí Permutation o SHAP                   ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  FEATURE SELECTION:                                             ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ SelectFromModel(model, threshold='median')                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Top K features (manuale)                                     ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Curva accuracy vs n_features ‚Üí trova il \"gomito\"            ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  ‚ö†Ô∏è ATTENZIONE FEATURES CORRELATE:                              ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                              ‚îÇ\n",
    "‚îÇ  Se A e B sono correlate:                                       ‚îÇ\n",
    "‚îÇ     ‚Üí Shuffle A: B compensa ‚Üí A sembra poco importante          ‚îÇ\n",
    "‚îÇ     ‚Üí Shuffle B: A compensa ‚Üí B sembra poco importante          ‚îÇ\n",
    "‚îÇ  SOLUZIONE: Rimuovere duplicati prima di calcolare importance   ‚îÇ\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Prossima Lezione\n",
    "\n",
    "**Lezione 17**: Metriche Avanzate di Valutazione\n",
    "- Precision, Recall, F1-Score\n",
    "- Matrice di confusione\n",
    "- Curve ROC e AUC\n",
    "- Curva Precision-Recall\n",
    "- Soglia di classificazione ottimale"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
