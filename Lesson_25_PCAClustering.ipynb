{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d870580",
   "metadata": {},
   "source": [
    "# 1) Titolo e obiettivi\n",
    "Lezione 25: PCA e clustering insieme per ridurre la dimensionalita' e rendere piu' robusti i raggruppamenti.\n",
    "- Obiettivi: applicare PCA prima del clustering, scegliere il numero di componenti, valutare l'impatto su qualita' e velocita', leggere i loadings per interpretare i cluster.\n",
    "- Cosa useremo: dataset Digits, Wine, Iris e dati sintetici; StandardScaler, PCA, KMeans, DBSCAN, AgglomerativeClustering.\n",
    "- Prerequisiti: basi di scalatura, PCA, metriche di clustering (silhouette, ARI), conoscenza di KMeans e DBSCAN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9349b",
   "metadata": {},
   "source": [
    "# 2) Teoria concettuale\n",
    "## 2.1 Perche' combinare PCA e clustering\n",
    "- PCA comprime l'informazione, riduce il rumore e rende le distanze piu' significative in spazi ad alta dimensionalita'.\n",
    "- Clustering su poche componenti e' piu' veloce e meno sensibile alla curse of dimensionality.\n",
    "- Per KMeans: PCA rende le varianze piu' omogenee e facilita centroidi stabili; per DBSCAN riduce la sparizione di densita' utile.\n",
    "- Se i cluster sono anisotropi, PCA li rende piu' sferici e quindi piu' facili da separare con algoritmi basati su distanza euclidea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8ed35",
   "metadata": {},
   "source": [
    "## 2.2 Quante componenti usare\n",
    "- Criterio di varianza cumulata (es. 0.80, 0.90, 0.95) come compromesso tra perdita di informazione e rumore.\n",
    "- Scree plot: osservare il gomito dove i benefici marginali calano.\n",
    "- Verifica empirica: confrontare silhouette/ARI e i tempi di fit tra soglie diverse.\n",
    "- Loadings: ogni componente e' una combinazione lineare di feature originali; servono a interpretare cosa differenzia i cluster.\n",
    "- Se servono visualizzazioni, conservare almeno le prime 2-3 componenti anche se la soglia e' piu' alta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b452e5",
   "metadata": {},
   "source": [
    "## 2.3 Quando non usare PCA prima del clustering\n",
    "- Feature gia' semantiche e poche (<15) con scale coerenti: PCA potrebbe offuscare l'interpretabilita'.\n",
    "- Cluster separati da feature rare/importanti: PCA puo' diluire la separazione eliminando varianza specifica.\n",
    "- Se DBSCAN lavora gia' bene nello spazio originale (densita' evidente), testare prima senza PCA.\n",
    "- Regola pratica: confrontare sempre baseline senza PCA e versioni con PCA per verificare che la qualita' non peggiori.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d8b6f",
   "metadata": {},
   "source": [
    "# 3) Schema mentale / mappa decisionale\n",
    "Workflow di riferimento: load -> clean -> scale -> (opzionale) PCA -> cluster -> valuta -> interpreta.\n",
    "Decision map testuale:\n",
    "1. Scala i dati se le feature non hanno la stessa scala.\n",
    "2. Esegui un clustering senza PCA per avere una baseline di silhouette/ARI e tempi.\n",
    "3. Prova PCA con soglie 0.80/0.90/0.95 di varianza, confronta metriche e tempi.\n",
    "4. Se usi DBSCAN, valuta anche combinazioni di eps/min_samples nello spazio PCA.\n",
    "5. Interpreta i cluster guardando le componenti principali e i loadings piu' forti.\n",
    "Micro-checklist: niente NaN, forme coerenti dopo PCA, numero cluster atteso, presenza di rumore (-1) se DBSCAN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e911b",
   "metadata": {},
   "source": [
    "# 4) Sezione dimostrativa\n",
    "Panoramica delle demo:\n",
    "- Demo 1: Digits con PCA al 90% di varianza + KMeans, metriche e grafici.\n",
    "- Demo 2: Confronto con/senza PCA su Digits (tempi e qualita').\n",
    "- Demo 3: Pipeline compatta PCA + KMeans su Iris.\n",
    "- Demo 4: PCA + DBSCAN per individuare outlier.\n",
    "- Demo 5: Visualizzazione 3D con PCA su Wine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9196ab",
   "metadata": {},
   "source": [
    "## Demo 1 - PCA 90% + KMeans su Digits\n",
    "Perche': mostrare che ridurre le 64 feature di Digits a poche componenti mantiene la struttura e migliora stabilita'.\n",
    "Metodi usati (input -> output):\n",
    "- `StandardScaler`: input array (n_samples x n_features), output array scalato; errore tipico: valori NaN o colonne costanti.\n",
    "- `PCA(n_components=0.90)`: riduce alle componenti che spiegano il 90% della varianza; errore tipico: n_components troppo alto rispetto ai campioni.\n",
    "- `KMeans(n_clusters=10)`: richiede dati numerici e n_clusters noto; errore tipico: dati non scalati o cluster non sferici.\n",
    "Checkpoint attesi: nessun NaN, forma X (1797, 64), forma X_pca (1797, ~20 componenti), varianza cumulata >= 0.90, silhouette > 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3989b351",
   "metadata": {},
   "source": [
    "### Checkpoint visivo per Demo 1\n",
    "- Scree plot decrescente: il gomito dovrebbe apparire dopo ~15-25 componenti.\n",
    "- Scatter PC1 vs PC2: punti a gruppi con confini ragionevolmente separati; se tutto sovrapposto, controllare scaler o n_clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: PCA (90% varianza) + KMeans su Digits\n",
    "# Intenzione: ridurre dimensionalita', verificare varianza mantenuta, eseguire clustering stabile e valutare silhouette/ARI.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.close('all')\n",
    "\n",
    "# Caricamento dati\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "print(f\"Forma originale X: {X_digits.shape}, y: {y_digits.shape}\")\n",
    "assert X_digits.shape[0] == y_digits.shape[0], \"Numero di campioni incoerente\"\n",
    "assert not np.isnan(X_digits).any(), \"Sono presenti NaN nel dataset\"\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_digits)\n",
    "print(f\"Forma dopo scaling: {X_scaled.shape}\")\n",
    "\n",
    "# PCA al 90% di varianza spiegata\n",
    "pca = PCA(n_components=0.90, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "var_cum = pca.explained_variance_ratio_.sum()\n",
    "print(f\"Componenti tenute: {X_pca.shape[1]}, Varianza cumulata: {var_cum:.3f}\")\n",
    "assert var_cum >= 0.90, \"La varianza cumulata e' inferiore alla soglia desiderata\"\n",
    "\n",
    "# KMeans sui dati ridotti\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_pca)\n",
    "ari = adjusted_rand_score(y_digits, labels)\n",
    "sil = silhouette_score(X_pca, labels)\n",
    "print(f\"ARI: {ari:.3f}, Silhouette: {sil:.3f}\")\n",
    "\n",
    "# Scree plot e scatter PC1-PC2\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(np.arange(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
    "axes[0].set_title('Scree plot - varianza per componente')\n",
    "axes[0].set_xlabel('Componente')\n",
    "axes[0].set_ylabel('Quota di varianza')\n",
    "axes[0].axhline(0.05, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "scatter = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', s=12, alpha=0.7)\n",
    "axes[1].set_title('KMeans su PC1-PC2')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "plt.colorbar(scatter, ax=axes[1], label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0e256",
   "metadata": {},
   "source": [
    "## Demo 2 - Confronto con e senza PCA\n",
    "Perche': quantificare il compromesso tra tempo di esecuzione e qualita' del clustering.\n",
    "Metodi: `Pipeline` con `StandardScaler`, PCA opzionale, `KMeans` fisso a 10 cluster.\n",
    "Checkpoint: tempo con PCA inferiore o simile, silhouette/ARI non peggiorate; se peggiorano molto, ridurre meno la dimensionalita'.\n",
    "Errori comuni: dimenticare lo scaler (cluster distorti), PCA con troppe poche componenti (silhouette bassa), silhouette non calcolabile se tutti i punti finiscono nello stesso cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ffc32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Confronto con/senza PCA su Digits\n",
    "# Intenzione: misurare l'effetto di PCA su tempi e metriche di clustering con KMeans.\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "scenarios = [\n",
    "    (\"Senza PCA\", None),\n",
    "    (\"PCA var 0.90\", 0.90),\n",
    "    (\"PCA 50 componenti\", 50),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, n_comp in scenarios:\n",
    "    steps = [(\"scaler\", StandardScaler())]\n",
    "    if n_comp is not None:\n",
    "        steps.append((\"pca\", PCA(n_components=n_comp, random_state=42)))\n",
    "    steps.append((\"kmeans\", KMeans(n_clusters=10, random_state=42, n_init=10)))\n",
    "    pipe = Pipeline(steps)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    labels = pipe.fit_predict(X_digits)\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Trasformazioni per valutare silhouette nello spazio usato dal modello\n",
    "    if n_comp is None:\n",
    "        X_used = pipe.named_steps[\"scaler\"].transform(X_digits)\n",
    "    else:\n",
    "        X_scaled_tmp = pipe.named_steps[\"scaler\"].transform(X_digits)\n",
    "        X_used = pipe.named_steps[\"pca\"].transform(X_scaled_tmp)\n",
    "\n",
    "    assert X_used.shape[0] == X_digits.shape[0], \"Numero di campioni alterato\"\n",
    "    ari = adjusted_rand_score(y_digits, labels)\n",
    "    sil = silhouette_score(X_used, labels)\n",
    "    results.append({\"scenario\": name, \"caratteristiche_usate\": X_used.shape[1], \"silhouette\": sil, \"ari\": ari, \"tempo_s\": elapsed})\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "print(res_df)\n",
    "\n",
    "best_idx = res_df['silhouette'].idxmax()\n",
    "print(\"Miglior silhouette:\")\n",
    "print(res_df.loc[[best_idx]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1509613",
   "metadata": {},
   "source": [
    "## Demo 3 - Pipeline compatta su Iris\n",
    "Perche': costruire una pipeline riproducibile che includa scaler, PCA e KMeans in un unico oggetto.\n",
    "Metodi: `Pipeline`, `StandardScaler`, `PCA(n_components=2)`, `KMeans(n_clusters=3)`.\n",
    "Checkpoint: nessun NaN, forma X_iris (150, 4), forma X_pca (150, 2), silhouette > 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139417db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3: Pipeline compatta su Iris\n",
    "# Intenzione: costruire una pipeline scalatore -> PCA -> KMeans e valutarla in 2D.\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "print(f\"Forma X_iris: {X_iris.shape}, y_iris: {y_iris.shape}\")\n",
    "assert not np.isnan(X_iris).any(), \"NaN trovati in Iris\"\n",
    "assert X_iris.shape == (150, 4), \"Forma inattesa per Iris\"\n",
    "\n",
    "pipe_iris = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=2, random_state=42)),\n",
    "    (\"kmeans\", KMeans(n_clusters=3, random_state=42, n_init=10)),\n",
    "])\n",
    "\n",
    "labels_iris = pipe_iris.fit_predict(X_iris)\n",
    "X_iris_pca = pipe_iris.named_steps[\"pca\"].transform(pipe_iris.named_steps[\"scaler\"].transform(X_iris))\n",
    "sil_iris = silhouette_score(X_iris_pca, labels_iris)\n",
    "print(f\"Forma dopo PCA: {X_iris_pca.shape}, Silhouette: {sil_iris:.3f}\")\n",
    "assert X_iris_pca.shape[1] == 2, \"La PCA non ha prodotto 2 componenti\"\n",
    "assert sil_iris > 0, \"Silhouette non positiva; rivedere n_clusters o n_components\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "scatter = ax.scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=labels_iris, cmap='viridis', s=30, alpha=0.8)\n",
    "ax.set_title('Iris: KMeans su PCA 2D')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003ad48",
   "metadata": {},
   "source": [
    "## Demo 4 - PCA + DBSCAN per outlier\n",
    "Perche': usare PCA per compattare i dati e facilitare DBSCAN nell'individuare aree di bassa densita' (outlier).\n",
    "Metodi: `make_blobs` per dati sintetici, aggiunta di rumore uniforme, `DBSCAN` su spazio PCA.\n",
    "Checkpoint: esistenza di etichette -1 (rumore), almeno 2 cluster non rumorosi, silhouette calcolata sui soli punti non rumorosi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 4: PCA + DBSCAN per outlier detection\n",
    "# Intenzione: usare PCA per compattare dati sintetici e permettere a DBSCAN di evidenziare il rumore.\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X_blobs, _ = make_blobs(n_samples=500, centers=4, n_features=6, cluster_std=1.2, random_state=42)\n",
    "noise = np.random.uniform(low=-8, high=8, size=(20, 6))\n",
    "X_mix = np.vstack([X_blobs, noise])\n",
    "print(f\"Forma dati con rumore: {X_mix.shape}\")\n",
    "assert not np.isnan(X_mix).any(), \"NaN nei dati sintetici\"\n",
    "\n",
    "pipe_dbscan = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=2, random_state=42)),\n",
    "])\n",
    "X_mix_pca = pipe_dbscan.fit_transform(X_mix)\n",
    "print(f\"Forma dopo PCA: {X_mix_pca.shape}\")\n",
    "assert X_mix_pca.shape[0] == X_mix.shape[0], \"Numero di campioni alterato dalla pipeline\"\n",
    "\n",
    "labels_dbscan = DBSCAN(eps=0.9, min_samples=5).fit_predict(X_mix_pca)\n",
    "unique_labels = np.unique(labels_dbscan)\n",
    "print(f\"Etichette trovate: {unique_labels}\")\n",
    "\n",
    "mask = labels_dbscan != -1\n",
    "if mask.sum() > 0 and np.unique(labels_dbscan[mask]).size > 1:\n",
    "    sil_db = silhouette_score(X_mix_pca[mask], labels_dbscan[mask])\n",
    "else:\n",
    "    sil_db = np.nan\n",
    "print(f\"Silhouette (solo cluster non rumore): {sil_db}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "scatter = ax.scatter(X_mix_pca[:, 0], X_mix_pca[:, 1], c=labels_dbscan, cmap='tab10', s=20, alpha=0.8)\n",
    "ax.set_title('DBSCAN su PCA 2D (rumore = -1)')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "plt.colorbar(scatter, ax=ax, label='Etichetta DBSCAN')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f34352",
   "metadata": {},
   "source": [
    "## Demo 5 - PCA 3D su Wine\n",
    "Perche': visualizzare i cluster su tre componenti e valutare quanta varianza manteniamo.\n",
    "Metodi: `PCA(n_components=3)` e `KMeans(n_clusters=3)`.\n",
    "Checkpoint: varianza cumulata > 0.60, silhouette > 0, grafico 3D con cluster separabili.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ce5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 5: PCA 3D + KMeans su Wine\n",
    "# Intenzione: visualizzare cluster Wine su 3 componenti principali e valutarne la separazione.\n",
    "from mpl_toolkits.mplot3d import Axes3D  # necessaria per il projection='3d'\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "X_wine, y_wine = load_wine(return_X_y=True)\n",
    "print(f\"Forma X_wine: {X_wine.shape}, y_wine: {y_wine.shape}\")\n",
    "assert not np.isnan(X_wine).any(), \"NaN trovati nel dataset Wine\"\n",
    "\n",
    "pipe_wine = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=3, random_state=42)),\n",
    "    (\"kmeans\", KMeans(n_clusters=3, random_state=42, n_init=10)),\n",
    "])\n",
    "labels_wine = pipe_wine.fit_predict(X_wine)\n",
    "X_wine_pca = pipe_wine.named_steps[\"pca\"].transform(pipe_wine.named_steps[\"scaler\"].transform(X_wine))\n",
    "var_wine = pipe_wine.named_steps[\"pca\"].explained_variance_ratio_.sum()\n",
    "sil_wine = silhouette_score(X_wine_pca, labels_wine)\n",
    "print(f\"Forma dopo PCA: {X_wine_pca.shape}, Varianza cumulata: {var_wine:.3f}, Silhouette: {sil_wine:.3f}\")\n",
    "assert var_wine >= 0.60, \"Varianza cumulata troppo bassa: aumenta n_components\"\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_wine_pca[:, 0], X_wine_pca[:, 1], X_wine_pca[:, 2], c=labels_wine, cmap='tab10', s=30, alpha=0.8)\n",
    "ax.set_title('Wine: KMeans su PCA 3D')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "fig.colorbar(scatter, ax=ax, label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20069db1",
   "metadata": {},
   "source": [
    "# 5) Esercizi svolti (passo-passo)\n",
    "## Esercizio 25.1 - Customer segmentation sintetica\n",
    "Obiettivo: generare un dataset sintetico 1000x20, ridurre al 90% di varianza con PCA e clusterizzare in 5 gruppi con KMeans.\n",
    "Perche': esercitarsi con un flusso end-to-end includendo controlli su forma, NaN e metriche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a38700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 25.1: Customer segmentation sintetica\n",
    "# Step-by-step: genera dati, scala, PCA al 90%, KMeans a 5 cluster, valuta silhouette/ARI.\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_synth, y_synth_true = make_blobs(n_samples=1000, centers=5, n_features=20, cluster_std=2.0, random_state=42)\n",
    "print(f\"Forma dati sintetici: {X_synth.shape}\")\n",
    "assert not np.isnan(X_synth).any(), \"NaN nei dati sintetici\"\n",
    "\n",
    "pipe_synth = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=0.90, random_state=42)),\n",
    "    (\"kmeans\", KMeans(n_clusters=5, random_state=42, n_init=10)),\n",
    "])\n",
    "\n",
    "labels_synth = pipe_synth.fit_predict(X_synth)\n",
    "X_synth_pca = pipe_synth.named_steps[\"pca\"].transform(pipe_synth.named_steps[\"scaler\"].transform(X_synth))\n",
    "var_synth = pipe_synth.named_steps[\"pca\"].explained_variance_ratio_.sum()\n",
    "sil_synth = silhouette_score(X_synth_pca, labels_synth)\n",
    "ari_synth = adjusted_rand_score(y_synth_true, labels_synth)\n",
    "print(f\"Componenti tenute: {X_synth_pca.shape[1]}, Varianza: {var_synth:.3f}, Silhouette: {sil_synth:.3f}, ARI: {ari_synth:.3f}\")\n",
    "\n",
    "# Sanity check: tutti i cluster presenti\n",
    "unique_labels = np.unique(labels_synth)\n",
    "print(f\"Cluster trovati: {unique_labels}\")\n",
    "assert len(unique_labels) == 5, \"Non sono stati trovati 5 cluster\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642d6d1",
   "metadata": {},
   "source": [
    "## Esercizio 25.2 - Confronto algoritmi su Digits\n",
    "Obiettivo: confrontare KMeans, AgglomerativeClustering e DBSCAN sullo spazio PCA (80% di varianza) del dataset Digits.\n",
    "Perche': mostrare come PCA impatta algoritmi diversi e come leggere ARI/silhouette, gestendo il rumore di DBSCAN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 25.2: Confronto algoritmi su Digits (PCA 80%)\n",
    "# Intenzione: confrontare KMeans, Agglomerative e DBSCAN dopo PCA, valutando silhouette e ARI.\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "scaler_80 = StandardScaler()\n",
    "pca_80 = PCA(n_components=0.80, random_state=42)\n",
    "X_digits_pca80 = pca_80.fit_transform(scaler_80.fit_transform(X_digits))\n",
    "print(f\"Forma PCA 80%: {X_digits_pca80.shape}, Varianza: {pca_80.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "models = [\n",
    "    (\"KMeans\", KMeans(n_clusters=10, random_state=42, n_init=10)),\n",
    "    (\"Agglomerative\", AgglomerativeClustering(n_clusters=10)),\n",
    "    (\"DBSCAN\", DBSCAN(eps=3.0, min_samples=5)),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for name, model in models:\n",
    "    labels = model.fit_predict(X_digits_pca80)\n",
    "    mask = labels != -1\n",
    "    if mask.sum() > 0 and np.unique(labels[mask]).size > 1:\n",
    "        sil = silhouette_score(X_digits_pca80[mask], labels[mask])\n",
    "    else:\n",
    "        sil = np.nan\n",
    "    ari = adjusted_rand_score(y_digits, labels)\n",
    "    rows.append({\"modello\": name, \"silhouette\": sil, \"ari\": ari, \"etichette_trovate\": np.unique(labels).size})\n",
    "\n",
    "res_models = pd.DataFrame(rows)\n",
    "print(res_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efa991",
   "metadata": {},
   "source": [
    "## Esercizio 25.3 - Mini tuning PCA + DBSCAN\n",
    "Obiettivo: cercare combinazioni di (n_components, eps, min_samples) che migliorano silhouette su Digits ridotto con PCA.\n",
    "Perche': illustrare un micro-grid manuale senza nuove librerie, con controlli di forma e cluster presenti.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f84a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 25.3: Mini tuning PCA + DBSCAN su Digits\n",
    "# Intenzione: esplorare manualmente combinazioni di n_components, eps e min_samples per migliorare silhouette.\n",
    "param_grid = {\n",
    "    \"n_components\": [0.70, 0.80, 0.90],\n",
    "    \"eps\": [2.5, 3.0, 3.5],\n",
    "    \"min_samples\": [3, 5],\n",
    "}\n",
    "\n",
    "best = None\n",
    "candidates = []\n",
    "for n_comp in param_grid[\"n_components\"]:\n",
    "    pca_tune = PCA(n_components=n_comp, random_state=42)\n",
    "    X_tune = pca_tune.fit_transform(scaler.fit_transform(X_digits))\n",
    "    for eps in param_grid[\"eps\"]:\n",
    "        for ms in param_grid[\"min_samples\"]:\n",
    "            labels = DBSCAN(eps=eps, min_samples=ms).fit_predict(X_tune)\n",
    "            mask = labels != -1\n",
    "            if mask.sum() < 2 or np.unique(labels[mask]).size < 2:\n",
    "                continue\n",
    "            sil = silhouette_score(X_tune[mask], labels[mask])\n",
    "            ari = adjusted_rand_score(y_digits, labels)\n",
    "            row = {\"n_components\": n_comp, \"eps\": eps, \"min_samples\": ms, \"silhouette\": sil, \"ari\": ari, \"etichette_trovate\": np.unique(labels).size}\n",
    "            candidates.append(row)\n",
    "            if best is None or sil > best[\"silhouette\"]:\n",
    "                best = row\n",
    "\n",
    "if candidates:\n",
    "    df_tuning = pd.DataFrame(candidates).sort_values(by=\"silhouette\", ascending=False)\n",
    "    print(\"Top 5 combinazioni per silhouette:\")\n",
    "    print(df_tuning.head())\n",
    "    print(\"Migliore combinazione:\")\n",
    "    print(best)\n",
    "else:\n",
    "    print(\"Nessuna combinazione ha prodotto almeno due cluster non rumorosi.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d436c5",
   "metadata": {},
   "source": [
    "# 6) Conclusione operativa\n",
    "Risultati operativi:\n",
    "- PCA prima del clustering riduce tempi e spesso migliora silhouette/ARI mantenendo la struttura (Digits e Wine).\n",
    "- Confrontare sempre baseline senza PCA con soglie diverse di varianza per scegliere n_components.\n",
    "- DBSCAN beneficia di PCA per rumore e densita', ma richiede tuning di eps/min_samples dopo la riduzione.\n",
    "\n",
    "Metodi spiegati (cosa fa, input/output, quando usarlo):\n",
    "- `StandardScaler`: centra e scala colonne numeriche; input matrice n_samples x n_features, output stessa forma; usalo prima di PCA/KMeans.\n",
    "- `PCA`: riduce dimensionalita' preservando varianza; input dati scalati, output n_samples x n_components; evita se vuoi interpretare feature originali senza mescolarle.\n",
    "- `KMeans`: clustering per centroidi; input numerico, output etichette; funziona bene con cluster sferici e dati scalati.\n",
    "- `AgglomerativeClustering`: clustering gerarchico bottom-up; input numerico, output etichette; utile se vuoi dendrogrammi o cluster non sferici.\n",
    "- `DBSCAN`: clustering basato su densita' con etichetta -1 per rumore; input numerico, output etichette; funziona bene con cluster di forma arbitraria, sensibile a eps/min_samples.\n",
    "- `silhouette_score`: misura coesione/separazione dei cluster (range -1..1); richiede almeno 2 cluster; non usare se tutti i punti stanno in un cluster unico.\n",
    "- `adjusted_rand_score`: confronta etichette previste con etichette vere; input due vettori di lunghezza n_samples; output da -1 a 1.\n",
    "\n",
    "Errori comuni e debug rapido:\n",
    "- Silhouette negativa o molto bassa -> probabile n_components troppo basso o n_clusters sbagliato; prova piu' componenti o rivedi il numero di cluster.\n",
    "- Tutti i punti in un solo cluster con DBSCAN -> eps troppo piccolo o min_samples troppo alto; allarga eps o riduci min_samples.\n",
    "- Varianza cumulata sotto la soglia richiesta -> controlla scaler o aumenta leggermente n_components.\n",
    "- ARI molto basso rispetto alla baseline senza PCA -> prova una soglia di varianza piu' alta o evita PCA se le feature sono gia' informative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea097e",
   "metadata": {},
   "source": [
    "# 7) Checklist di fine lezione\n",
    "- [ ] Ho scalato i dati prima di PCA e clustering.\n",
    "- [ ] Ho scelto `n_components` confrontando varianza e metriche (silhouette/ARI).\n",
    "- [ ] Ho verificato forme coerenti dopo PCA e che non ci siano NaN.\n",
    "- [ ] Ho controllato tempi di fit con e senza PCA.\n",
    "- [ ] Ho interpretato i cluster guardando le componenti e i loadings piu' pesanti.\n",
    "- [ ] Ho gestito il rumore di DBSCAN verificando la presenza di etichette -1.\n",
    "\n",
    "Glossario (termini usati nel notebook):\n",
    "- PCA: tecnica di riduzione dimensionale basata su varianza massima.\n",
    "- Principal component (PC): nuova variabile lineare che massimizza la varianza.\n",
    "- Varianza spiegata: quota di varianza coperta dalle componenti scelte.\n",
    "- Scree plot: grafico della varianza spiegata per componente.\n",
    "- Loadings: pesi delle feature originali su ogni componente.\n",
    "- n_components: numero di componenti mantenute o soglia di varianza.\n",
    "- Silhouette score: metrica di coesione/separazione dei cluster.\n",
    "- Adjusted Rand Index (ARI): accordo tra etichette previste e vere.\n",
    "- KMeans: clustering per centroidi con n_clusters fissato.\n",
    "- DBSCAN: clustering per densita' con etichetta -1 per il rumore.\n",
    "- AgglomerativeClustering: clustering gerarchico bottom-up.\n",
    "- Scaling: trasformazione per avere media 0 e deviazione standard 1 per ogni feature.\n",
    "- Punti rumore: punti etichettati -1 da DBSCAN.\n",
    "- eps: raggio di vicinanza usato da DBSCAN.\n",
    "- min_samples: numero minimo di vicini per formare un core point in DBSCAN.\n",
    "- Curse of dimensionality: fenomeno di distanze meno informative in spazi ad alta dimensione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Changelog didattico\n",
    "- Ristrutturata la lezione nelle 8 sezioni richieste con titoli in italiano.\n",
    "- Aggiunte spiegazioni operative prima di ogni blocco di codice e checklist di checkpoint attesi.\n",
    "- Inseriti controlli di forma, NaN, varianza cumulata, silhouette e ARI con assert dove rilevante.\n",
    "- Approfondite le demo con confronti PCA vs no PCA e visualizzazioni PCA 2D/3D.\n",
    "- Resi gli esercizi guidati passo-passo con note su errori comuni e interpretazione dei risultati.\n",
    "- Aggiunti metodi spiegati, errori comuni, checklist e glossario interno alla lezione.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}