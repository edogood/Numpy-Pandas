{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20a2e51",
   "metadata": {},
   "source": [
    "# Lezione 33 â€” Named Entity Recognition (NER)\n",
    "\n",
    "## BLOCCO 4: Artificial Intelligence & NLP\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivi di Apprendimento\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "1. **Definire** cosa sono le Named Entities e perchÃ© estrarle\n",
    "2. **Classificare** le tipologie di entitÃ  standard (PER, ORG, LOC, DATE, ecc.)\n",
    "3. **Implementare** un sistema NER rule-based con regex\n",
    "4. **Comprendere** le architetture ML per NER (BIO tagging)\n",
    "5. **Applicare** NER per estrarre informazioni strutturate da testo\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisiti\n",
    "\n",
    "- Lezione 30: Rappresentazione del testo (tokenizzazione)\n",
    "- Lezione 32: Sentiment Analysis (classificazione testo)\n",
    "- Nozioni base di espressioni regolari (regex)\n",
    "\n",
    "---\n",
    "\n",
    "## Indice\n",
    "\n",
    "1. Teoria â€” Cos'Ã¨ il NER\n",
    "2. Tipologie di EntitÃ \n",
    "3. Schema Mentale\n",
    "4. Notebook Dimostrativo\n",
    "5. Esercizi Svolti\n",
    "6. Conclusione e Bignami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed817ad9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Teoria â€” Cos'Ã¨ il Named Entity Recognition\n",
    "\n",
    "## 1.1 Definizione Formale\n",
    "\n",
    "Il **Named Entity Recognition (NER)** Ã¨ il task di NLP che consiste nell'identificare e classificare porzioni di testo che rappresentano entitÃ  del mondo reale.\n",
    "\n",
    "```\n",
    "Input: \"Mario Rossi lavora presso Microsoft a Milano dal 2020.\"\n",
    "\n",
    "Output:\n",
    "  \"Mario Rossi\"  â†’ PERSONA (PER)\n",
    "  \"Microsoft\"    â†’ ORGANIZZAZIONE (ORG)\n",
    "  \"Milano\"       â†’ LUOGO (LOC)\n",
    "  \"2020\"         â†’ DATA (DATE)\n",
    "```\n",
    "\n",
    "## 1.2 PerchÃ© il NER Ã¨ Importante\n",
    "\n",
    "| Applicazione | Uso del NER | Valore Business |\n",
    "|--------------|-------------|-----------------|\n",
    "| Information Extraction | Estrarre fatti da testo | Knowledge base automatiche |\n",
    "| Customer Service | Identificare prodotti/servizi menzionati | Routing automatico |\n",
    "| Compliance | Rilevare PII (Personally Identifiable Info) | GDPR, privacy |\n",
    "| Finance | Estrarre ticker, aziende, date | Trading automatico |\n",
    "| Healthcare | Identificare farmaci, sintomi, malattie | Cartelle cliniche |\n",
    "\n",
    "## 1.3 Il NER come Sequence Labeling\n",
    "\n",
    "A differenza della classificazione (1 label per documento), il NER assegna **1 label per ogni token**:\n",
    "\n",
    "```\n",
    "Token:    [Mario] [Rossi] [lavora] [presso] [Microsoft] [a] [Milano]\n",
    "Labels:   [B-PER] [I-PER] [O]      [O]      [B-ORG]     [O] [B-LOC]\n",
    "```\n",
    "\n",
    "Questo Ã¨ chiamato **sequence labeling** o **token classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a227a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 Tipologie di EntitÃ  Standard\n",
    "\n",
    "Le entitÃ  piÃ¹ comuni seguite negli standard (CoNLL, OntoNotes):\n",
    "\n",
    "| Tipo | Sigla | Esempi |\n",
    "|------|-------|--------|\n",
    "| **Persona** | PER | Mario Rossi, Angela Merkel |\n",
    "| **Organizzazione** | ORG | Microsoft, ONU, Ferrari |\n",
    "| **Luogo** | LOC | Milano, Italia, Monte Bianco |\n",
    "| **Geopolitico** | GPE | Italia (come stato), Roma (come comune) |\n",
    "| **Data** | DATE | 25 dicembre, 2024, lunedÃ¬ |\n",
    "| **Tempo** | TIME | 14:30, mezzogiorno |\n",
    "| **Denaro** | MONEY | â‚¬100, 1 milione di euro |\n",
    "| **Percentuale** | PERCENT | 5%, dieci per cento |\n",
    "| **Prodotto** | PRODUCT | iPhone, Fiat 500 |\n",
    "| **Evento** | EVENT | Olimpiadi, G20 |\n",
    "\n",
    "## 1.5 Schema BIO (Begin-Inside-Outside)\n",
    "\n",
    "Il formato piÃ¹ usato per annotare NER:\n",
    "\n",
    "| Tag | Significato |\n",
    "|-----|-------------|\n",
    "| **B-XXX** | Begin: primo token di un'entitÃ  di tipo XXX |\n",
    "| **I-XXX** | Inside: token successivi della stessa entitÃ  |\n",
    "| **O** | Outside: non fa parte di nessuna entitÃ  |\n",
    "\n",
    "**Esempio:**\n",
    "\n",
    "```\n",
    "Testo:  \"La Ferrari ha vinto a Monza\"\n",
    "        La    Ferrari   ha   vinto   a    Monza\n",
    "Tags:   O     B-ORG     O    O       O    B-LOC\n",
    "\n",
    "Testo:  \"Il CEO di Amazon Web Services\"\n",
    "        Il   CEO   di   Amazon   Web      Services\n",
    "Tags:   O    O     O    B-ORG    I-ORG    I-ORG\n",
    "```\n",
    "\n",
    "**PerchÃ© B e I separati?**\n",
    "Per distinguere entitÃ  adiacenti:\n",
    "```\n",
    "\"New York Times cita New York Post\"\n",
    " B-ORG I-ORG I-ORG  O    B-ORG I-ORG I-ORG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fef394",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.6 Approcci al NER\n",
    "\n",
    "### Approccio 1: Rule-Based (Pattern Matching)\n",
    "\n",
    "Usa regex e regole per identificare entitÃ :\n",
    "\n",
    "```python\n",
    "# Esempio: date in formato italiano\n",
    "import re\n",
    "pattern_data = r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}'\n",
    "# Match: \"25/12/2024\", \"1-1-99\"\n",
    "\n",
    "# Esempio: email\n",
    "pattern_email = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "```\n",
    "\n",
    "**Pro:** Precisione alta per pattern noti, interpretabile\n",
    "**Contro:** Scarsa recall su varianti, non generalizza\n",
    "\n",
    "### Approccio 2: Machine Learning (Feature-based)\n",
    "\n",
    "Features estratte per ogni token:\n",
    "- Il token stesso\n",
    "- Prefisso/Suffisso\n",
    "- Ãˆ maiuscolo?\n",
    "- Ãˆ un numero?\n",
    "- POS tag (verbo, sostantivo...)\n",
    "- Token precedente/successivo\n",
    "\n",
    "Classificatori: CRF (Conditional Random Fields), HMM, SVM\n",
    "\n",
    "### Approccio 3: Deep Learning\n",
    "\n",
    "- LSTM bidirezionali\n",
    "- Transformers (BERT, RoBERTa)\n",
    "- Fine-tuning su dati annotati\n",
    "\n",
    "**Pro:** State-of-the-art performance\n",
    "**Contro:** Richiede GPU, dati, competenze\n",
    "\n",
    "## 1.7 Metriche per NER\n",
    "\n",
    "**Entity-level** (la piÃ¹ usata):\n",
    "- Un'entitÃ  Ã¨ corretta se: tipo E span sono entrambi corretti\n",
    "\n",
    "$$Precision = \\frac{\\text{EntitÃ  predette correttamente}}{\\text{Tutte le entitÃ  predette}}$$\n",
    "\n",
    "$$Recall = \\frac{\\text{EntitÃ  predette correttamente}}{\\text{Tutte le entitÃ  reali}}$$\n",
    "\n",
    "$$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3dfb6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Schema Mentale â€” Mappa Logica\n",
    "\n",
    "## Pipeline NER\n",
    "\n",
    "```\n",
    "                        NER PIPELINE\n",
    "                        \n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      INPUT: Testo Raw                       â”‚\n",
    "â”‚     \"Il CEO di Apple, Tim Cook, visiterÃ  Roma il 15/3\"     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    TOKENIZZAZIONE                           â”‚\n",
    "â”‚   [\"Il\", \"CEO\", \"di\", \"Apple\", \",\", \"Tim\", \"Cook\", \",\",    â”‚\n",
    "â”‚    \"visiterÃ \", \"Roma\", \"il\", \"15/3\"]                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  FEATURE EXTRACTION                         â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚   Token  â”‚ IsUpper â”‚ IsDigit â”‚ Prefix â”‚ Suffix â”‚ Shape    â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\n",
    "â”‚   Apple  â”‚  True   â”‚  False  â”‚  App   â”‚  ple   â”‚  Xxxxx   â”‚\n",
    "â”‚   Tim    â”‚  True   â”‚  False  â”‚  Tim   â”‚  im    â”‚  Xxx     â”‚\n",
    "â”‚   15/3   â”‚  False  â”‚  True   â”‚  15/   â”‚  /3    â”‚  dd/d    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    CLASSIFICAZIONE                          â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚   Rule-Based          ML (CRF)           Deep Learning     â”‚\n",
    "â”‚   (regex, lookup)     (features)         (BERT, LSTM)      â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚   â†’ Per casi noti     â†’ Bilanciato       â†’ Massima acc.    â”‚\n",
    "â”‚   â†’ Interpretabile    â†’ Moderate res.    â†’ Richiede GPU    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     SPAN EXTRACTION                         â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚   Token    â”‚  BIO Tag   â”‚  EntitÃ  Estratta                 â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚\n",
    "â”‚   Apple    â”‚  B-ORG     â”‚  \"Apple\" (ORG)                   â”‚\n",
    "â”‚   Tim      â”‚  B-PER     â”‚  \"Tim Cook\" (PER)                â”‚\n",
    "â”‚   Cook     â”‚  I-PER     â”‚                                  â”‚\n",
    "â”‚   Roma     â”‚  B-LOC     â”‚  \"Roma\" (LOC)                    â”‚\n",
    "â”‚   15/3     â”‚  B-DATE    â”‚  \"15/3\" (DATE)                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     OUTPUT STRUTTURATO                      â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚   [                                                         â”‚\n",
    "â”‚     {\"text\": \"Apple\", \"type\": \"ORG\", \"start\": 11, \"end\": 16}â”‚\n",
    "â”‚     {\"text\": \"Tim Cook\", \"type\": \"PER\", \"start\": 18, \"end\": 26}\n",
    "â”‚     {\"text\": \"Roma\", \"type\": \"LOC\", \"start\": 37, \"end\": 41} â”‚\n",
    "â”‚     {\"text\": \"15/3\", \"type\": \"DATE\", \"start\": 46, \"end\": 50}â”‚\n",
    "â”‚   ]                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Scelta dell'Approccio\n",
    "\n",
    "```\n",
    "                    QUALE APPROCCIO?\n",
    "                    \n",
    "              EntitÃ  con pattern fisso?\n",
    "              (email, date, codici)\n",
    "                     â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚ SÃ¬                    â”‚ No\n",
    "         â–¼                       â–¼\n",
    "    REGEX/RULES            Dati annotati?\n",
    "                                 â”‚\n",
    "                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                     â”‚ SÃ¬                    â”‚ No\n",
    "                     â–¼                       â–¼\n",
    "               Quanti dati?           Usa modello\n",
    "                     â”‚                pre-trained\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    (spaCy, HF)\n",
    "         â”‚ <5000              â”‚ >50000\n",
    "         â–¼                       â–¼\n",
    "       CRF                 Fine-tune BERT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e49ba72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Notebook Dimostrativo\n",
    "\n",
    "## Setup e Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEMO 1: NER Rule-Based con Regex\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import namedtuple\n",
    "\n",
    "# Definiamo una struttura per le entitÃ  estratte\n",
    "Entity = namedtuple('Entity', ['text', 'type', 'start', 'end'])\n",
    "\n",
    "class RuleBasedNER:\n",
    "    \"\"\"\n",
    "    Sistema NER basato su regole (regex).\n",
    "    Ideale per entitÃ  con pattern prevedibili.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Dizionario di pattern regex per tipo di entitÃ \n",
    "        self.patterns = {\n",
    "            # Date italiane: 25/12/2024, 25-12-24, 25 dicembre 2024\n",
    "            'DATE': [\n",
    "                r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b',\n",
    "                r'\\b\\d{1,2}\\s+(gennaio|febbraio|marzo|aprile|maggio|giugno|'\n",
    "                r'luglio|agosto|settembre|ottobre|novembre|dicembre)\\s+\\d{4}\\b',\n",
    "            ],\n",
    "            \n",
    "            # Email\n",
    "            'EMAIL': [\n",
    "                r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b',\n",
    "            ],\n",
    "            \n",
    "            # Telefoni italiani\n",
    "            'PHONE': [\n",
    "                r'\\b(?:\\+39\\s?)?(?:0\\d{1,4}|\\d{3})[\\s.-]?\\d{6,7}\\b',\n",
    "                r'\\b(?:\\+39\\s?)?3\\d{2}[\\s.-]?\\d{6,7}\\b',  # Cellulari\n",
    "            ],\n",
    "            \n",
    "            # Codice fiscale italiano\n",
    "            'CF': [\n",
    "                r'\\b[A-Z]{6}\\d{2}[A-Z]\\d{2}[A-Z]\\d{3}[A-Z]\\b',\n",
    "            ],\n",
    "            \n",
    "            # Importi monetari\n",
    "            'MONEY': [\n",
    "                r'â‚¬\\s?\\d+(?:[.,]\\d{2})?(?:\\s?(?:mila|milioni|miliardi))?',\n",
    "                r'\\d+(?:[.,]\\d{2})?\\s?â‚¬',\n",
    "                r'\\d+(?:[.,]\\d{2})?\\s?euro\\b',\n",
    "            ],\n",
    "            \n",
    "            # Percentuali\n",
    "            'PERCENT': [\n",
    "                r'\\d+(?:[.,]\\d+)?\\s?%',\n",
    "                r'\\d+(?:[.,]\\d+)?\\s?per\\s?cento',\n",
    "            ],\n",
    "        }\n",
    "    \n",
    "    def extract(self, text: str) -> List[Entity]:\n",
    "        \"\"\"\n",
    "        Estrae entitÃ  dal testo usando i pattern definiti.\n",
    "        \n",
    "        Args:\n",
    "            text: testo da analizzare\n",
    "            \n",
    "        Returns:\n",
    "            Lista di Entity trovate\n",
    "        \"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        for entity_type, patterns in self.patterns.items():\n",
    "            for pattern in patterns:\n",
    "                # Trova tutti i match\n",
    "                for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "                    entity = Entity(\n",
    "                        text=match.group(),\n",
    "                        type=entity_type,\n",
    "                        start=match.start(),\n",
    "                        end=match.end()\n",
    "                    )\n",
    "                    entities.append(entity)\n",
    "        \n",
    "        # Ordina per posizione\n",
    "        entities.sort(key=lambda x: x.start)\n",
    "        \n",
    "        # Rimuovi overlap (mantieni il piÃ¹ lungo)\n",
    "        entities = self._remove_overlaps(entities)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _remove_overlaps(self, entities: List[Entity]) -> List[Entity]:\n",
    "        \"\"\"Rimuove entitÃ  sovrapposte, mantenendo la piÃ¹ lunga.\"\"\"\n",
    "        if not entities:\n",
    "            return []\n",
    "        \n",
    "        result = [entities[0]]\n",
    "        for entity in entities[1:]:\n",
    "            last = result[-1]\n",
    "            # Se non c'Ã¨ overlap, aggiungi\n",
    "            if entity.start >= last.end:\n",
    "                result.append(entity)\n",
    "            # Se c'Ã¨ overlap, tieni il piÃ¹ lungo\n",
    "            elif len(entity.text) > len(last.text):\n",
    "                result[-1] = entity\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Test del sistema\n",
    "ner = RuleBasedNER()\n",
    "\n",
    "testi_test = [\n",
    "    \"Contattami al 333-1234567 o via email mario.rossi@gmail.com\",\n",
    "    \"Il fatturato Ã¨ cresciuto del 15% raggiungendo â‚¬2,5 milioni\",\n",
    "    \"La riunione Ã¨ fissata per il 25/12/2024 alle 15:30\",\n",
    "    \"Codice fiscale: RSSMRA80A01H501X\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEMO 1: NER RULE-BASED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for testo in testi_test:\n",
    "    print(f\"\\nTesto: '{testo}'\")\n",
    "    entities = ner.extract(testo)\n",
    "    if entities:\n",
    "        print(\"  EntitÃ  trovate:\")\n",
    "        for e in entities:\n",
    "            print(f\"    [{e.type}] '{e.text}' (pos: {e.start}-{e.end})\")\n",
    "    else:\n",
    "        print(\"  Nessuna entitÃ  trovata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEMO 2: NER con Lookup Tables (Gazetteers)\n",
    "# ============================================================\n",
    "# Per nomi propri e organizzazioni, usiamo liste predefinite\n",
    "\n",
    "class GazetteerNER:\n",
    "    \"\"\"\n",
    "    NER basato su dizionari (gazetteers).\n",
    "    Efficace per entitÃ  note a priori.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Gazetteers: liste di entitÃ  note\n",
    "        self.gazetteers = {\n",
    "            'PER': [\n",
    "                # Nomi italiani comuni\n",
    "                'mario', 'luigi', 'giuseppe', 'giovanni', 'antonio',\n",
    "                'francesco', 'alessandro', 'andrea', 'marco', 'luca',\n",
    "                'maria', 'anna', 'giulia', 'francesca', 'sara',\n",
    "                # Cognomi comuni\n",
    "                'rossi', 'russo', 'ferrari', 'esposito', 'bianchi',\n",
    "                'romano', 'colombo', 'ricci', 'marino', 'greco',\n",
    "            ],\n",
    "            'ORG': [\n",
    "                'apple', 'microsoft', 'google', 'amazon', 'facebook', 'meta',\n",
    "                'ferrari', 'fiat', 'eni', 'enel', 'unicredit', 'intesa',\n",
    "                'juventus', 'inter', 'milan', 'roma', 'napoli',\n",
    "                'onu', 'nato', 'ue', 'unione europea', 'bce',\n",
    "            ],\n",
    "            'LOC': [\n",
    "                'roma', 'milano', 'napoli', 'torino', 'firenze', 'bologna',\n",
    "                'venezia', 'genova', 'palermo', 'bari', 'catania',\n",
    "                'italia', 'francia', 'germania', 'spagna', 'usa',\n",
    "                'europa', 'asia', 'africa', 'america',\n",
    "            ],\n",
    "        }\n",
    "        \n",
    "        # Crea lookup set per ricerca veloce (lowercase)\n",
    "        self.lookup = {\n",
    "            entity_type: set(name.lower() for name in names)\n",
    "            for entity_type, names in self.gazetteers.items()\n",
    "        }\n",
    "    \n",
    "    def extract(self, text: str) -> List[Entity]:\n",
    "        \"\"\"Estrae entitÃ  usando i gazetteers.\"\"\"\n",
    "        entities = []\n",
    "        words = text.split()\n",
    "        \n",
    "        # Cerca singole parole\n",
    "        for i, word in enumerate(words):\n",
    "            word_clean = re.sub(r'[^\\w]', '', word.lower())\n",
    "            \n",
    "            for entity_type, lookup_set in self.lookup.items():\n",
    "                if word_clean in lookup_set:\n",
    "                    # Trova posizione nel testo originale\n",
    "                    start = text.lower().find(word_clean)\n",
    "                    if start != -1:\n",
    "                        entities.append(Entity(\n",
    "                            text=word.strip('.,!?'),\n",
    "                            type=entity_type,\n",
    "                            start=start,\n",
    "                            end=start + len(word_clean)\n",
    "                        ))\n",
    "        \n",
    "        # Cerca multi-word (bigram)\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = f\"{words[i]} {words[i+1]}\".lower()\n",
    "            bigram_clean = re.sub(r'[^\\w\\s]', '', bigram)\n",
    "            \n",
    "            for entity_type, lookup_set in self.lookup.items():\n",
    "                if bigram_clean in lookup_set:\n",
    "                    start = text.lower().find(bigram_clean)\n",
    "                    if start != -1:\n",
    "                        entities.append(Entity(\n",
    "                            text=f\"{words[i]} {words[i+1]}\".strip('.,!?'),\n",
    "                            type=entity_type,\n",
    "                            start=start,\n",
    "                            end=start + len(bigram_clean)\n",
    "                        ))\n",
    "        \n",
    "        return sorted(set(entities), key=lambda x: x.start)\n",
    "\n",
    "# Test\n",
    "gazetteer_ner = GazetteerNER()\n",
    "\n",
    "testi = [\n",
    "    \"Mario Rossi lavora a Milano per Unicredit\",\n",
    "    \"La Juventus ha battuto il Milan a Torino\",\n",
    "    \"Apple e Microsoft dominano il mercato tech\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEMO 2: NER CON GAZETTEERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for testo in testi:\n",
    "    print(f\"\\nTesto: '{testo}'\")\n",
    "    entities = gazetteer_ner.extract(testo)\n",
    "    if entities:\n",
    "        print(\"  EntitÃ  trovate:\")\n",
    "        for e in entities:\n",
    "            print(f\"    [{e.type}] '{e.text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0747433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEMO 3: Formato BIO e Conversione\n",
    "# ============================================================\n",
    "# Comprendere il formato BIO usato nell'annotazione NER\n",
    "\n",
    "def text_to_bio(text: str, entities: List[Tuple[int, int, str]]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Converte testo con entitÃ  annotate in formato BIO.\n",
    "    \n",
    "    Args:\n",
    "        text: testo originale\n",
    "        entities: lista di (start, end, type)\n",
    "        \n",
    "    Returns:\n",
    "        Lista di (token, bio_tag)\n",
    "    \"\"\"\n",
    "    # Tokenizzazione semplice\n",
    "    tokens = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    for match in re.finditer(r'\\S+', text):\n",
    "        tokens.append({\n",
    "            'text': match.group(),\n",
    "            'start': match.start(),\n",
    "            'end': match.end()\n",
    "        })\n",
    "    \n",
    "    # Assegna tag BIO\n",
    "    bio_tags = []\n",
    "    for token in tokens:\n",
    "        tag = 'O'  # Default: Outside\n",
    "        \n",
    "        for ent_start, ent_end, ent_type in entities:\n",
    "            # Token inizia dentro l'entitÃ \n",
    "            if ent_start <= token['start'] < ent_end:\n",
    "                # Ãˆ il primo token dell'entitÃ ?\n",
    "                if token['start'] == ent_start:\n",
    "                    tag = f'B-{ent_type}'\n",
    "                else:\n",
    "                    tag = f'I-{ent_type}'\n",
    "                break\n",
    "        \n",
    "        bio_tags.append((token['text'], tag))\n",
    "    \n",
    "    return bio_tags\n",
    "\n",
    "def bio_to_entities(bio_tags: List[Tuple[str, str]]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Converte sequenza BIO in entitÃ  strutturate.\n",
    "    \n",
    "    Args:\n",
    "        bio_tags: lista di (token, tag)\n",
    "        \n",
    "    Returns:\n",
    "        Lista di {text, type}\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for token, tag in bio_tags:\n",
    "        if tag.startswith('B-'):\n",
    "            # Salva entitÃ  precedente se esiste\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            # Inizia nuova entitÃ \n",
    "            current_entity = {\n",
    "                'text': token,\n",
    "                'type': tag[2:]  # Rimuove \"B-\"\n",
    "            }\n",
    "        elif tag.startswith('I-') and current_entity:\n",
    "            # Continua entitÃ  corrente\n",
    "            current_entity['text'] += ' ' + token\n",
    "        else:\n",
    "            # O tag: chiudi entitÃ  se aperta\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "    \n",
    "    # Non dimenticare l'ultima\n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Esempio: testo â†’ BIO\n",
    "testo = \"Mario Rossi lavora presso Microsoft a Milano\"\n",
    "# Annotazioni: (start, end, type)\n",
    "annotations = [\n",
    "    (0, 11, 'PER'),    # \"Mario Rossi\"\n",
    "    (28, 37, 'ORG'),   # \"Microsoft\"\n",
    "    (40, 46, 'LOC'),   # \"Milano\"\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEMO 3: FORMATO BIO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTesto: '{testo}'\")\n",
    "\n",
    "# Converti in BIO\n",
    "bio = text_to_bio(testo, annotations)\n",
    "print(f\"\\nFormato BIO:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'Token':<15} {'Tag':<10}\")\n",
    "print(\"-\" * 30)\n",
    "for token, tag in bio:\n",
    "    print(f\"{token:<15} {tag:<10}\")\n",
    "\n",
    "# Ricostruisci da BIO\n",
    "print(f\"\\nRicostruzione da BIO:\")\n",
    "reconstructed = bio_to_entities(bio)\n",
    "for ent in reconstructed:\n",
    "    print(f\"  [{ent['type']}] '{ent['text']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6266af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEMO 4: Feature Engineering per NER ML-based\n",
    "# ============================================================\n",
    "# Quali feature estraiamo da ogni token per un modello ML\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_token_features(token: str, prev_token: str = None, \n",
    "                           next_token: str = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Estrae feature da un singolo token per NER.\n",
    "    \n",
    "    Args:\n",
    "        token: il token corrente\n",
    "        prev_token: token precedente (contesto)\n",
    "        next_token: token successivo (contesto)\n",
    "        \n",
    "    Returns:\n",
    "        Dizionario di feature\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        # Feature sul token stesso\n",
    "        'token': token.lower(),\n",
    "        'len': len(token),\n",
    "        \n",
    "        # Caratteristiche morfologiche\n",
    "        'is_upper': token.isupper(),            # \"NATO\"\n",
    "        'is_title': token.istitle(),            # \"Mario\"\n",
    "        'is_lower': token.islower(),            # \"ciao\"\n",
    "        'is_digit': token.isdigit(),            # \"2024\"\n",
    "        'is_alpha': token.isalpha(),            # \"ciao\"\n",
    "        'is_alnum': token.isalnum(),            # \"test123\"\n",
    "        \n",
    "        # Prefissi e suffissi (utili per nomi/verbi)\n",
    "        'prefix_2': token[:2].lower() if len(token) >= 2 else token.lower(),\n",
    "        'prefix_3': token[:3].lower() if len(token) >= 3 else token.lower(),\n",
    "        'suffix_2': token[-2:].lower() if len(token) >= 2 else token.lower(),\n",
    "        'suffix_3': token[-3:].lower() if len(token) >= 3 else token.lower(),\n",
    "        \n",
    "        # Shape: pattern del token (X=maiuscola, x=minuscola, d=digit)\n",
    "        'shape': get_token_shape(token),\n",
    "        \n",
    "        # Contiene caratteri speciali?\n",
    "        'has_hyphen': '-' in token,\n",
    "        'has_digit': any(c.isdigit() for c in token),\n",
    "        'has_punct': any(not c.isalnum() for c in token),\n",
    "    }\n",
    "    \n",
    "    # Feature di contesto\n",
    "    if prev_token:\n",
    "        features['prev_token'] = prev_token.lower()\n",
    "        features['prev_is_title'] = prev_token.istitle()\n",
    "    else:\n",
    "        features['prev_token'] = '<START>'\n",
    "        features['prev_is_title'] = False\n",
    "    \n",
    "    if next_token:\n",
    "        features['next_token'] = next_token.lower()\n",
    "        features['next_is_title'] = next_token.istitle()\n",
    "    else:\n",
    "        features['next_token'] = '<END>'\n",
    "        features['next_is_title'] = False\n",
    "    \n",
    "    return features\n",
    "\n",
    "def get_token_shape(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera la 'shape' di un token.\n",
    "    Esempio: \"McDonald's\" â†’ \"XxXxxxx'x\"\n",
    "    \"\"\"\n",
    "    shape = []\n",
    "    for c in token:\n",
    "        if c.isupper():\n",
    "            shape.append('X')\n",
    "        elif c.islower():\n",
    "            shape.append('x')\n",
    "        elif c.isdigit():\n",
    "            shape.append('d')\n",
    "        else:\n",
    "            shape.append(c)\n",
    "    \n",
    "    # Comprimi sequenze ripetute\n",
    "    if len(shape) > 0:\n",
    "        compressed = [shape[0]]\n",
    "        for s in shape[1:]:\n",
    "            if s != compressed[-1] or s not in 'Xxd':\n",
    "                compressed.append(s)\n",
    "        return ''.join(compressed)\n",
    "    return ''\n",
    "\n",
    "# Dimostra su alcuni token\n",
    "tokens_demo = [\"Mario\", \"ROSSI\", \"lavora\", \"25/12/2024\", \"Microsoft\", \"â‚¬100\"]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEMO 4: FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, token in enumerate(tokens_demo):\n",
    "    prev_t = tokens_demo[i-1] if i > 0 else None\n",
    "    next_t = tokens_demo[i+1] if i < len(tokens_demo)-1 else None\n",
    "    \n",
    "    features = extract_token_features(token, prev_t, next_t)\n",
    "    \n",
    "    print(f\"\\nToken: '{token}'\")\n",
    "    print(f\"  Shape: {features['shape']}\")\n",
    "    print(f\"  is_title: {features['is_title']}, is_upper: {features['is_upper']}\")\n",
    "    print(f\"  prefix_3: {features['prefix_3']}, suffix_3: {features['suffix_3']}\")\n",
    "    print(f\"  Context: prev='{features['prev_token']}', next='{features['next_token']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67908ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEMO 5: Sistema NER Ibrido (Regex + Gazetteer + Heuristics)\n",
    "# ============================================================\n",
    "# Combiniamo le tecniche per un sistema piÃ¹ robusto\n",
    "\n",
    "class HybridNER:\n",
    "    \"\"\"\n",
    "    Sistema NER che combina:\n",
    "    1. Pattern regex per entitÃ  strutturate\n",
    "    2. Gazetteers per nomi noti\n",
    "    3. Heuristics per pattern linguistici\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rule_ner = RuleBasedNER()\n",
    "        self.gazetteer_ner = GazetteerNER()\n",
    "    \n",
    "    def extract(self, text: str) -> List[Entity]:\n",
    "        \"\"\"Combina tutti gli approcci.\"\"\"\n",
    "        all_entities = []\n",
    "        \n",
    "        # 1. EntitÃ  da regex (date, email, money, etc.)\n",
    "        regex_entities = self.rule_ner.extract(text)\n",
    "        all_entities.extend(regex_entities)\n",
    "        \n",
    "        # 2. EntitÃ  da gazetteer (PER, ORG, LOC)\n",
    "        gazetteer_entities = self.gazetteer_ner.extract(text)\n",
    "        all_entities.extend(gazetteer_entities)\n",
    "        \n",
    "        # 3. Heuristics: pattern capitalized words non ancora catturati\n",
    "        heuristic_entities = self._extract_heuristic(text, all_entities)\n",
    "        all_entities.extend(heuristic_entities)\n",
    "        \n",
    "        # Rimuovi duplicati e overlap\n",
    "        return self._resolve_conflicts(all_entities)\n",
    "    \n",
    "    def _extract_heuristic(self, text: str, existing: List[Entity]) -> List[Entity]:\n",
    "        \"\"\"\n",
    "        Euristica: sequenze di parole con iniziale maiuscola\n",
    "        potrebbero essere nomi propri non nel gazetteer.\n",
    "        \"\"\"\n",
    "        entities = []\n",
    "        existing_spans = {(e.start, e.end) for e in existing}\n",
    "        \n",
    "        # Pattern: 2+ parole consecutive che iniziano con maiuscola\n",
    "        pattern = r'\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\\b'\n",
    "        \n",
    "        for match in re.finditer(pattern, text):\n",
    "            # Salta se giÃ  catturato\n",
    "            if (match.start(), match.end()) not in existing_spans:\n",
    "                # Euristica: probabile nome proprio\n",
    "                entities.append(Entity(\n",
    "                    text=match.group(),\n",
    "                    type='PER?',  # Incerto\n",
    "                    start=match.start(),\n",
    "                    end=match.end()\n",
    "                ))\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _resolve_conflicts(self, entities: List[Entity]) -> List[Entity]:\n",
    "        \"\"\"Risolve conflitti tra entitÃ  sovrapposte.\"\"\"\n",
    "        if not entities:\n",
    "            return []\n",
    "        \n",
    "        # Ordina per posizione\n",
    "        sorted_ents = sorted(entities, key=lambda x: (x.start, -len(x.text)))\n",
    "        \n",
    "        result = []\n",
    "        for ent in sorted_ents:\n",
    "            # Controlla overlap con entitÃ  giÃ  accettate\n",
    "            overlaps = False\n",
    "            for accepted in result:\n",
    "                if not (ent.end <= accepted.start or ent.start >= accepted.end):\n",
    "                    overlaps = True\n",
    "                    break\n",
    "            \n",
    "            if not overlaps:\n",
    "                result.append(ent)\n",
    "        \n",
    "        return sorted(result, key=lambda x: x.start)\n",
    "\n",
    "# Test del sistema ibrido\n",
    "hybrid_ner = HybridNER()\n",
    "\n",
    "testo_complesso = \"\"\"\n",
    "Il CEO di Apple, Tim Cook, ha annunciato che l'azienda raggiungerÃ \n",
    "â‚¬10 miliardi di fatturato in Italia entro il 2025. \n",
    "La conferenza stampa si terrÃ  a Milano il 25/03/2025.\n",
    "Per informazioni: info@apple.com o 02-1234567.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEMO 5: SISTEMA NER IBRIDO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTesto:\\n{testo_complesso.strip()}\")\n",
    "\n",
    "entities = hybrid_ner.extract(testo_complesso)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ENTITÃ€ ESTRATTE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Tipo':<10} {'Testo':<30} {'Posizione':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for e in entities:\n",
    "    print(f\"{e.type:<10} '{e.text}'\".ljust(42) + f\" ({e.start}-{e.end})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45daaa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEMO 6: Valutazione Metriche NER\n",
    "# ============================================================\n",
    "# Come calcolare Precision, Recall, F1 per NER\n",
    "\n",
    "def evaluate_ner(predictions: List[Entity], \n",
    "                 ground_truth: List[Entity],\n",
    "                 strict: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Valuta le performance di un sistema NER.\n",
    "    \n",
    "    Args:\n",
    "        predictions: entitÃ  predette\n",
    "        ground_truth: entitÃ  corrette (gold standard)\n",
    "        strict: se True, richiede match esatto di span E tipo\n",
    "                se False, richiede solo overlap di span\n",
    "    \n",
    "    Returns:\n",
    "        Dizionario con P, R, F1 per tipo\n",
    "    \"\"\"\n",
    "    # Converti in set di tuple per confronto\n",
    "    if strict:\n",
    "        pred_set = {(e.text.lower(), e.type, e.start, e.end) for e in predictions}\n",
    "        gold_set = {(e.text.lower(), e.type, e.start, e.end) for e in ground_truth}\n",
    "    else:\n",
    "        # Match rilassato: solo tipo deve coincidere\n",
    "        pred_set = {(e.type, e.text.lower()) for e in predictions}\n",
    "        gold_set = {(e.type, e.text.lower()) for e in ground_truth}\n",
    "    \n",
    "    # True Positives: predetti correttamente\n",
    "    tp = len(pred_set & gold_set)\n",
    "    \n",
    "    # False Positives: predetti ma non nel gold\n",
    "    fp = len(pred_set - gold_set)\n",
    "    \n",
    "    # False Negatives: nel gold ma non predetti\n",
    "    fn = len(gold_set - pred_set)\n",
    "    \n",
    "    # Metriche\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1\n",
    "    }\n",
    "\n",
    "# Esempio di valutazione\n",
    "# Ground truth (annotazioni umane)\n",
    "gold_entities = [\n",
    "    Entity('Apple', 'ORG', 12, 17),\n",
    "    Entity('Tim Cook', 'PER', 19, 27),\n",
    "    Entity('â‚¬10 miliardi', 'MONEY', 72, 84),\n",
    "    Entity('Italia', 'LOC', 103, 109),\n",
    "    Entity('2025', 'DATE', 120, 124),\n",
    "    Entity('Milano', 'LOC', 157, 163),\n",
    "    Entity('25/03/2025', 'DATE', 167, 177),\n",
    "    Entity('info@apple.com', 'EMAIL', 198, 212),\n",
    "    Entity('02-1234567', 'PHONE', 215, 225),\n",
    "]\n",
    "\n",
    "# Predizioni del nostro sistema (dalla demo precedente)\n",
    "pred_entities = entities  # Dal demo 5\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEMO 6: VALUTAZIONE NER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Valutazione stretta\n",
    "strict_metrics = evaluate_ner(pred_entities, gold_entities, strict=True)\n",
    "print(\"\\nğŸ“Š Valutazione STRICT (span + tipo esatti):\")\n",
    "print(f\"  True Positives: {strict_metrics['TP']}\")\n",
    "print(f\"  False Positives: {strict_metrics['FP']}\")\n",
    "print(f\"  False Negatives: {strict_metrics['FN']}\")\n",
    "print(f\"  Precision: {strict_metrics['Precision']:.2%}\")\n",
    "print(f\"  Recall: {strict_metrics['Recall']:.2%}\")\n",
    "print(f\"  F1-Score: {strict_metrics['F1']:.2%}\")\n",
    "\n",
    "# Valutazione rilassata\n",
    "relaxed_metrics = evaluate_ner(pred_entities, gold_entities, strict=False)\n",
    "print(f\"\\nğŸ“Š Valutazione RELAXED (solo tipo corretto):\")\n",
    "print(f\"  Precision: {relaxed_metrics['Precision']:.2%}\")\n",
    "print(f\"  Recall: {relaxed_metrics['Recall']:.2%}\")\n",
    "print(f\"  F1-Score: {relaxed_metrics['F1']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105cf467",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Esercizi Svolti\n",
    "\n",
    "## Esercizio 1: Estrazione Dati Strutturati da Testo\n",
    "\n",
    "**Obiettivo:** Dato un testo non strutturato, estrarre automaticamente informazioni strutturate in formato JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd60c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESERCIZIO 1 - SOLUZIONE\n",
    "# Estrazione Dati Strutturati da Testo\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "\n",
    "class BusinessCardExtractor:\n",
    "    \"\"\"\n",
    "    Estrae informazioni strutturate da un biglietto da visita\n",
    "    o testo contenente dati di contatto.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Pattern per vari tipi di informazioni\n",
    "        self.patterns = {\n",
    "            'email': r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b',\n",
    "            'telefono': r'(?:\\+39\\s?)?(?:0\\d{1,4}|\\d{3})[\\s.-]?\\d{6,7}',\n",
    "            'cellulare': r'(?:\\+39\\s?)?3\\d{2}[\\s.-]?\\d{6,7}',\n",
    "            'piva': r'\\b[0-9]{11}\\b',\n",
    "            'cf': r'\\b[A-Z]{6}\\d{2}[A-Z]\\d{2}[A-Z]\\d{3}[A-Z]\\b',\n",
    "            'cap': r'\\b\\d{5}\\b',\n",
    "            'indirizzo': r'(?:Via|Viale|Corso|Piazza|P\\.zza)\\s+[A-Za-zÃ€-Ã¿\\s]+,?\\s*\\d+',\n",
    "        }\n",
    "        \n",
    "        # Titoli che indicano ruoli\n",
    "        self.titoli = [\n",
    "            'CEO', 'CTO', 'CFO', 'COO', 'CMO',\n",
    "            'Direttore', 'Manager', 'Responsabile',\n",
    "            'Ingegnere', 'Ing.', 'Dott.', 'Dr.',\n",
    "            'Avvocato', 'Avv.', 'Architetto', 'Arch.'\n",
    "        ]\n",
    "    \n",
    "    def extract(self, testo: str) -> Dict:\n",
    "        \"\"\"Estrae tutte le informazioni disponibili.\"\"\"\n",
    "        risultato = {\n",
    "            'nome': None,\n",
    "            'ruolo': None,\n",
    "            'azienda': None,\n",
    "            'email': [],\n",
    "            'telefono': [],\n",
    "            'cellulare': [],\n",
    "            'indirizzo': None,\n",
    "            'piva': None,\n",
    "            'cf': None,\n",
    "        }\n",
    "        \n",
    "        # Estrai con regex\n",
    "        for campo, pattern in self.patterns.items():\n",
    "            matches = re.findall(pattern, testo, re.IGNORECASE)\n",
    "            if matches:\n",
    "                if campo in ['email', 'telefono', 'cellulare']:\n",
    "                    risultato[campo] = matches\n",
    "                else:\n",
    "                    risultato[campo] = matches[0]\n",
    "        \n",
    "        # Estrai ruolo (cerca titoli)\n",
    "        for titolo in self.titoli:\n",
    "            if titolo.lower() in testo.lower():\n",
    "                # Cerca contesto attorno al titolo\n",
    "                pattern = rf'{titolo}\\s*[:\\-]?\\s*\\w+(?:\\s+\\w+)?'\n",
    "                match = re.search(pattern, testo, re.IGNORECASE)\n",
    "                if match:\n",
    "                    risultato['ruolo'] = match.group().strip()\n",
    "                    break\n",
    "        \n",
    "        # Estrai nome: prima riga o sequenza titolata\n",
    "        righe = testo.strip().split('\\n')\n",
    "        if righe:\n",
    "            prima_riga = righe[0].strip()\n",
    "            # Se Ã¨ corta e titlecase, probabilmente Ã¨ il nome\n",
    "            if len(prima_riga) < 50 and prima_riga.istitle():\n",
    "                risultato['nome'] = prima_riga\n",
    "        \n",
    "        return risultato\n",
    "\n",
    "# Test\n",
    "extractor = BusinessCardExtractor()\n",
    "\n",
    "biglietti = [\n",
    "    \"\"\"\n",
    "    Marco Bianchi\n",
    "    CEO - TechCorp Italia\n",
    "    Via Roma 123, 20100 Milano\n",
    "    Tel: 02-12345678\n",
    "    Cell: 333-1234567\n",
    "    Email: m.bianchi@techcorp.it\n",
    "    P.IVA: 12345678901\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Dott.ssa Giulia Verdi\n",
    "    Responsabile Marketing\n",
    "    Innovare S.r.l.\n",
    "    Corso Vittorio Emanuele 45\n",
    "    10121 Torino\n",
    "    giulia.verdi@innovare.com\n",
    "    +39 011 9876543\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ESERCIZIO 1: ESTRAZIONE DATI STRUTTURATI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, biglietto in enumerate(biglietti, 1):\n",
    "    print(f\"\\nğŸ“‡ Biglietto {i}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    dati = extractor.extract(biglietto)\n",
    "    \n",
    "    # Output formattato\n",
    "    for chiave, valore in dati.items():\n",
    "        if valore:\n",
    "            print(f\"  {chiave.capitalize()}: {valore}\")\n",
    "    \n",
    "    # JSON output\n",
    "    print(f\"\\n  ğŸ“‹ JSON:\")\n",
    "    print(f\"  {json.dumps(dati, indent=4, ensure_ascii=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd99aec6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Esercizio 2: PII Detection (Privacy)\n",
    "\n",
    "**Obiettivo:** Creare un sistema che identifica e maschera dati personali sensibili (PII - Personally Identifiable Information) per conformitÃ  GDPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd739b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESERCIZIO 2 - SOLUZIONE\n",
    "# PII Detection e Masking per GDPR\n",
    "# ============================================================\n",
    "\n",
    "class PIIDetector:\n",
    "    \"\"\"\n",
    "    Sistema per identificare e mascherare dati personali\n",
    "    secondo le normative sulla privacy (GDPR).\n",
    "    \n",
    "    PII Types:\n",
    "    - Nomi e cognomi\n",
    "    - Email\n",
    "    - Numeri di telefono\n",
    "    - Codici fiscali\n",
    "    - IBAN\n",
    "    - Carte di credito\n",
    "    - Indirizzi IP\n",
    "    - Date di nascita\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            'EMAIL': {\n",
    "                'regex': r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b',\n",
    "                'mask': '[EMAIL]'\n",
    "            },\n",
    "            'TELEFONO': {\n",
    "                'regex': r'(?:\\+39\\s?)?(?:0\\d{1,4}|\\d{3})[\\s.-]?\\d{6,7}',\n",
    "                'mask': '[TELEFONO]'\n",
    "            },\n",
    "            'CODICE_FISCALE': {\n",
    "                'regex': r'\\b[A-Z]{6}\\d{2}[A-Z]\\d{2}[A-Z]\\d{3}[A-Z]\\b',\n",
    "                'mask': '[CODICE_FISCALE]'\n",
    "            },\n",
    "            'IBAN': {\n",
    "                'regex': r'\\b[A-Z]{2}\\d{2}[A-Z]\\d{22}\\b|\\bIT\\d{2}[A-Z]\\d{10}[A-Z0-9]{12}\\b',\n",
    "                'mask': '[IBAN]'\n",
    "            },\n",
    "            'CARTA_CREDITO': {\n",
    "                'regex': r'\\b(?:\\d{4}[\\s-]?){3}\\d{4}\\b',\n",
    "                'mask': '[CARTA_CREDITO]'\n",
    "            },\n",
    "            'IP_ADDRESS': {\n",
    "                'regex': r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',\n",
    "                'mask': '[IP_ADDRESS]'\n",
    "            },\n",
    "            'DATA_NASCITA': {\n",
    "                'regex': r'\\b\\d{1,2}[/-]\\d{1,2}[/-](?:19|20)\\d{2}\\b',\n",
    "                'mask': '[DATA_NASCITA]'\n",
    "            },\n",
    "        }\n",
    "    \n",
    "    def detect(self, testo: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Identifica tutti i PII nel testo.\n",
    "        \n",
    "        Returns:\n",
    "            Lista di {tipo, valore, posizione}\n",
    "        \"\"\"\n",
    "        trovati = []\n",
    "        \n",
    "        for pii_type, config in self.patterns.items():\n",
    "            for match in re.finditer(config['regex'], testo):\n",
    "                trovati.append({\n",
    "                    'tipo': pii_type,\n",
    "                    'valore': match.group(),\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end()\n",
    "                })\n",
    "        \n",
    "        return sorted(trovati, key=lambda x: x['start'])\n",
    "    \n",
    "    def mask(self, testo: str) -> str:\n",
    "        \"\"\"\n",
    "        Maschera tutti i PII trovati nel testo.\n",
    "        \n",
    "        Returns:\n",
    "            Testo con PII mascherati\n",
    "        \"\"\"\n",
    "        # Ordina i match dal fondo per non alterare le posizioni\n",
    "        matches = self.detect(testo)\n",
    "        matches_sorted = sorted(matches, key=lambda x: x['start'], reverse=True)\n",
    "        \n",
    "        masked = testo\n",
    "        for match in matches_sorted:\n",
    "            pii_type = match['tipo']\n",
    "            mask_text = self.patterns[pii_type]['mask']\n",
    "            masked = masked[:match['start']] + mask_text + masked[match['end']:]\n",
    "        \n",
    "        return masked\n",
    "    \n",
    "    def report(self, testo: str) -> Dict:\n",
    "        \"\"\"Genera un report sui PII trovati.\"\"\"\n",
    "        trovati = self.detect(testo)\n",
    "        \n",
    "        report = {\n",
    "            'totale_pii': len(trovati),\n",
    "            'per_tipo': {},\n",
    "            'rischio': 'ALTO' if len(trovati) > 3 else 'MEDIO' if len(trovati) > 0 else 'BASSO'\n",
    "        }\n",
    "        \n",
    "        for pii in trovati:\n",
    "            tipo = pii['tipo']\n",
    "            if tipo not in report['per_tipo']:\n",
    "                report['per_tipo'][tipo] = []\n",
    "            report['per_tipo'][tipo].append(pii['valore'])\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Test\n",
    "detector = PIIDetector()\n",
    "\n",
    "documento = \"\"\"\n",
    "Gentile Mario Rossi (CF: RSSMRA80A01H501X),\n",
    "\n",
    "Confermiamo la sua prenotazione per il 25/12/1980 (data di nascita verificata).\n",
    "Il suo ordine verrÃ  spedito all'indirizzo registrato.\n",
    "\n",
    "Dati di fatturazione:\n",
    "- Email: mario.rossi@gmail.com\n",
    "- Telefono: 333-1234567\n",
    "- IBAN: IT60X0542811101000000123456\n",
    "- Carta: 4532-1234-5678-9012\n",
    "\n",
    "Accesso registrato da IP: 192.168.1.100\n",
    "\n",
    "Cordiali saluti,\n",
    "Il Team\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ESERCIZIO 2: PII DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Detect\n",
    "print(\"\\nğŸ“‹ DOCUMENTO ORIGINALE:\")\n",
    "print(\"-\" * 40)\n",
    "print(documento)\n",
    "\n",
    "# Report\n",
    "report = detector.report(documento)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“Š REPORT PII\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Totale PII trovati: {report['totale_pii']}\")\n",
    "print(f\"Livello di rischio: {report['rischio']}\")\n",
    "print(f\"\\nPer tipo:\")\n",
    "for tipo, valori in report['per_tipo'].items():\n",
    "    print(f\"  {tipo}: {len(valori)} occorrenze\")\n",
    "    for v in valori:\n",
    "        print(f\"    - {v}\")\n",
    "\n",
    "# Masked version\n",
    "masked = detector.mask(documento)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ”’ DOCUMENTO MASCHERATO (GDPR-compliant)\")\n",
    "print(\"=\"*60)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a39fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Esercizio 3: News Analyzer con NER\n",
    "\n",
    "**Obiettivo:** Analizzare un corpus di notizie estraendo automaticamente le entitÃ  piÃ¹ menzionate per tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d131b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESERCIZIO 3 - SOLUZIONE\n",
    "# News Analyzer con NER\n",
    "# ============================================================\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "class NewsAnalyzer:\n",
    "    \"\"\"\n",
    "    Analizza un corpus di notizie per estrarre:\n",
    "    - EntitÃ  piÃ¹ menzionate\n",
    "    - Co-occorrenze tra entitÃ \n",
    "    - Trend per data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ner = HybridNER()  # Dal demo precedente\n",
    "        self.corpus_entities = []\n",
    "    \n",
    "    def process_news(self, notizie: List[Dict]) -> None:\n",
    "        \"\"\"\n",
    "        Processa un corpus di notizie.\n",
    "        \n",
    "        Args:\n",
    "            notizie: Lista di {titolo, testo, data}\n",
    "        \"\"\"\n",
    "        self.corpus_entities = []\n",
    "        \n",
    "        for news in notizie:\n",
    "            # Combina titolo e testo\n",
    "            full_text = f\"{news.get('titolo', '')} {news.get('testo', '')}\"\n",
    "            \n",
    "            # Estrai entitÃ \n",
    "            entities = self.ner.extract(full_text)\n",
    "            \n",
    "            self.corpus_entities.append({\n",
    "                'news_id': news.get('id', len(self.corpus_entities)),\n",
    "                'data': news.get('data'),\n",
    "                'titolo': news.get('titolo'),\n",
    "                'entities': entities\n",
    "            })\n",
    "    \n",
    "    def get_top_entities(self, n: int = 10, entity_type: str = None) -> pd.DataFrame:\n",
    "        \"\"\"Restituisce le entitÃ  piÃ¹ frequenti.\"\"\"\n",
    "        all_entities = []\n",
    "        \n",
    "        for doc in self.corpus_entities:\n",
    "            for e in doc['entities']:\n",
    "                if entity_type is None or e.type == entity_type:\n",
    "                    all_entities.append(e.text.lower())\n",
    "        \n",
    "        counter = Counter(all_entities)\n",
    "        top = counter.most_common(n)\n",
    "        \n",
    "        return pd.DataFrame(top, columns=['EntitÃ ', 'Frequenza'])\n",
    "    \n",
    "    def get_entity_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Riassunto per tipo di entitÃ .\"\"\"\n",
    "        type_counts = Counter()\n",
    "        \n",
    "        for doc in self.corpus_entities:\n",
    "            for e in doc['entities']:\n",
    "                type_counts[e.type] += 1\n",
    "        \n",
    "        df = pd.DataFrame(type_counts.items(), columns=['Tipo', 'Conteggio'])\n",
    "        return df.sort_values('Conteggio', ascending=False)\n",
    "    \n",
    "    def find_cooccurrences(self, entity_type1: str, entity_type2: str) -> List[Tuple]:\n",
    "        \"\"\"Trova entitÃ  che appaiono insieme nello stesso documento.\"\"\"\n",
    "        cooccurrences = []\n",
    "        \n",
    "        for doc in self.corpus_entities:\n",
    "            ent1_list = [e for e in doc['entities'] if e.type == entity_type1]\n",
    "            ent2_list = [e for e in doc['entities'] if e.type == entity_type2]\n",
    "            \n",
    "            for e1 in ent1_list:\n",
    "                for e2 in ent2_list:\n",
    "                    cooccurrences.append((e1.text, e2.text))\n",
    "        \n",
    "        return Counter(cooccurrences).most_common(10)\n",
    "\n",
    "# Dataset di notizie simulate\n",
    "notizie = [\n",
    "    {\n",
    "        'id': 1,\n",
    "        'titolo': 'Apple annuncia nuovo iPhone a Milano',\n",
    "        'testo': 'Il CEO Tim Cook ha presentato l\\'iPhone 15 al â‚¬1000. '\n",
    "                 'L\\'evento si Ã¨ tenuto il 25/09/2024.',\n",
    "        'data': '2024-09-25'\n",
    "    },\n",
    "    {\n",
    "        'id': 2,\n",
    "        'titolo': 'Ferrari trionfa a Monza',\n",
    "        'testo': 'La Ferrari ha vinto il Gran Premio d\\'Italia. '\n",
    "                 'Charles Leclerc festeggia a Milano.',\n",
    "        'data': '2024-09-01'\n",
    "    },\n",
    "    {\n",
    "        'id': 3,\n",
    "        'titolo': 'Unicredit e Intesa: nuova collaborazione',\n",
    "        'testo': 'Le due banche italiane hanno firmato un accordo a Roma. '\n",
    "                 'L\\'investimento supera â‚¬500 milioni.',\n",
    "        'data': '2024-10-15'\n",
    "    },\n",
    "    {\n",
    "        'id': 4,\n",
    "        'titolo': 'Google apre nuovo hub a Torino',\n",
    "        'testo': 'Google investe in Italia con un centro ricerca. '\n",
    "                 'Previste 200 assunzioni entro il 2025.',\n",
    "        'data': '2024-10-20'\n",
    "    },\n",
    "]\n",
    "\n",
    "# Analisi\n",
    "analyzer = NewsAnalyzer()\n",
    "analyzer.process_news(notizie)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ESERCIZIO 3: NEWS ANALYZER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Summary per tipo\n",
    "print(\"\\nğŸ“Š ENTITÃ€ PER TIPO:\")\n",
    "print(\"-\" * 40)\n",
    "print(analyzer.get_entity_summary().to_string(index=False))\n",
    "\n",
    "# Top entitÃ \n",
    "print(f\"\\nğŸ“ˆ TOP 10 ENTITÃ€ (tutti i tipi):\")\n",
    "print(\"-\" * 40)\n",
    "print(analyzer.get_top_entities(10).to_string(index=False))\n",
    "\n",
    "# Top locations\n",
    "print(f\"\\nğŸ“ TOP LOCATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "print(analyzer.get_top_entities(5, 'LOC').to_string(index=False))\n",
    "\n",
    "# Top organizations\n",
    "print(f\"\\nğŸ¢ TOP ORGANIZATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "print(analyzer.get_top_entities(5, 'ORG').to_string(index=False))\n",
    "\n",
    "# Dettaglio per notizia\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“° DETTAGLIO PER NOTIZIA\")\n",
    "print(\"=\"*60)\n",
    "for doc in analyzer.corpus_entities:\n",
    "    print(f\"\\n[{doc['data']}] {doc['titolo']}\")\n",
    "    for e in doc['entities']:\n",
    "        print(f\"  â””â”€ [{e.type}] {e.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7d9eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Conclusione Operativa\n",
    "\n",
    "## Cosa Abbiamo Imparato\n",
    "\n",
    "| Concetto | Definizione | Applicazione |\n",
    "|----------|-------------|--------------|\n",
    "| **NER** | Identificazione entitÃ  nominate | Information Extraction |\n",
    "| **BIO Tagging** | Schema di annotazione sequenziale | Training modelli NER |\n",
    "| **Gazetteers** | Dizionari di entitÃ  note | Lookup veloce |\n",
    "| **Rule-based NER** | Pattern regex per entitÃ  | EntitÃ  strutturate |\n",
    "| **Feature Engineering** | Caratteristiche token | NER ML-based |\n",
    "\n",
    "## Quando Usare Quale Approccio\n",
    "\n",
    "| Tipo EntitÃ  | Approccio Consigliato | PerchÃ© |\n",
    "|-------------|----------------------|--------|\n",
    "| Date, email, numeri | **Regex** | Pattern fissi e prevedibili |\n",
    "| Nomi aziende note | **Gazetteer** | Lista finita e stabile |\n",
    "| Nomi persone generici | **ML/DL** | VariabilitÃ  alta |\n",
    "| Dominio specifico | **Fine-tuning** | Lessico specializzato |\n",
    "\n",
    "## Workflow Operativo\n",
    "\n",
    "```python\n",
    "# Template NER production\n",
    "\n",
    "# 1. Per entitÃ  strutturate â†’ Regex\n",
    "import re\n",
    "emails = re.findall(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', text)\n",
    "\n",
    "# 2. Per entitÃ  note â†’ Lookup\n",
    "known_orgs = {'apple', 'google', 'microsoft'}\n",
    "found = [w for w in tokens if w.lower() in known_orgs]\n",
    "\n",
    "# 3. Per NER completo â†’ Librerie\n",
    "# spaCy (locale, veloce)\n",
    "import spacy\n",
    "nlp = spacy.load(\"it_core_news_lg\")\n",
    "doc = nlp(text)\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# HuggingFace (cloud, accurato)\n",
    "from transformers import pipeline\n",
    "ner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "```\n",
    "\n",
    "## Errori Comuni da Evitare\n",
    "\n",
    "1. **Regex troppo greedy**: pattern che matchano troppo\n",
    "2. **Case sensitivity**: dimenticare `.lower()` nei lookup\n",
    "3. **Overlap non gestito**: entitÃ  sovrapposte non risolte\n",
    "4. **Valutazione token-level**: usare entity-level per NER\n",
    "5. **Ignorare contesto**: guardare solo il token, non i vicini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349b099",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Bignami â€” Scheda di Riferimento Rapido\n",
    "\n",
    "## Tipi di EntitÃ  Standard\n",
    "\n",
    "| Sigla | Significato | Esempi |\n",
    "|-------|-------------|--------|\n",
    "| PER | Persona | Mario Rossi, Angela Merkel |\n",
    "| ORG | Organizzazione | Apple, ONU, Juventus |\n",
    "| LOC | Luogo | Milano, Monte Bianco |\n",
    "| GPE | EntitÃ  geopolitica | Italia, Roma (comune) |\n",
    "| DATE | Data | 25/12/2024, lunedÃ¬ |\n",
    "| MONEY | Importo | â‚¬100, 1 milione |\n",
    "| PERCENT | Percentuale | 5%, dieci per cento |\n",
    "\n",
    "## Schema BIO\n",
    "\n",
    "```\n",
    "B-XXX = Begin: primo token entitÃ  tipo XXX\n",
    "I-XXX = Inside: token successivi stessa entitÃ \n",
    "O     = Outside: non Ã¨ un'entitÃ \n",
    "```\n",
    "\n",
    "## Pattern Regex Comuni\n",
    "\n",
    "```python\n",
    "# Email\n",
    "r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b'\n",
    "\n",
    "# Telefono italiano\n",
    "r'(?:\\+39\\s?)?(?:0\\d{1,4}|\\d{3})[\\s.-]?\\d{6,7}'\n",
    "\n",
    "# Codice fiscale\n",
    "r'\\b[A-Z]{6}\\d{2}[A-Z]\\d{2}[A-Z]\\d{3}[A-Z]\\b'\n",
    "\n",
    "# Data italiana\n",
    "r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b'\n",
    "\n",
    "# Importo euro\n",
    "r'â‚¬\\s?\\d+(?:[.,]\\d{2})?|\n",
    "\\d+(?:[.,]\\d{2})?\\s?â‚¬'\n",
    "```\n",
    "\n",
    "## Metriche NER\n",
    "\n",
    "```python\n",
    "# Entity-level evaluation\n",
    "Precision = TP / (TP + FP)  # Dei predetti, quanti corretti\n",
    "Recall = TP / (TP + FN)     # Dei gold, quanti trovati\n",
    "F1 = 2 * P * R / (P + R)    # Media armonica\n",
    "```\n",
    "\n",
    "## Checklist Pre-Deploy\n",
    "\n",
    "```\n",
    "â–¡ Pattern testati su edge cases\n",
    "â–¡ Gazetteers aggiornati\n",
    "â–¡ Overlap resolution implementato\n",
    "â–¡ Valutazione entity-level (non token)\n",
    "â–¡ Test su dati reali del dominio\n",
    "â–¡ Performance accettabile (>80% F1)\n",
    "```\n",
    "\n",
    "---\n",
    "*Fine Lezione 33 â€” Named Entity Recognition (NER)*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
