{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b56c9e",
   "metadata": {},
   "source": [
    "# Lezione 20 - K-Means Clustering\n\n## Sezione 1 - Titolo e obiettivi\n\n---\n\n## Obiettivi della Lezione\n\nAl termine di questa lezione sarai in grado di:\n\n1. **Comprendere** la geometria del clustering K-Means.\n2. **Spiegare** il ruolo dei centroidi e come vengono aggiornati.\n3. **Calcolare** la distanza euclidea e capire perche e fondamentale.\n4. **Riconoscere** le assunzioni forti del modello e i loro limiti.\n5. **Applicare** K-Means correttamente con sklearn.\n\n---\n\n## Perche questa lezione e importante\n\nK-Means e l'algoritmo di clustering piu usato al mondo: e semplice, veloce e spesso efficace.\nQuesta semplicita nasconde **assunzioni forti** che, se ignorate, portano a risultati sbagliati:\n- Assume che i cluster siano **sferici**.\n- Assume che abbiano **dimensioni simili**.\n- E **sensibile** ai valori iniziali e agli outlier.\n\nCapire come funziona davvero K-Means ti permette di:\n- Usarlo quando e appropriato.\n- Evitarlo quando non lo e.\n- Interpretare correttamente i risultati.\n\n---\n\n## Ruolo nel percorso\n\n| Lezione | Argomento |\n|---------|-----------|\n| 19 | Introduzione all'Unsupervised Learning |\n| **20** | **K-Means Clustering (sei qui)** |\n| 21 | Scelta del numero di cluster (Elbow, Silhouette) |\n| 22 | Clustering Gerarchico |\n| 23 | DBSCAN |\n\nQuesta lezione introduce il primo algoritmo concreto di clustering. Nelle lezioni successive vedremo come scegliere K e algoritmi alternativi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sezione 2 - Teoria profonda\n",
    "Richiamo dei concetti di K-Means: geometria, distanza, funzione obiettivo, assunzioni e inizializzazione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da452241",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 1 - Teoria Concettuale\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Cos'e il clustering\n",
    "Il clustering raggruppa osservazioni simili tra loro senza conoscere a priori quanti gruppi esistono o quali siano. Serve a scoprire struttura nei dati a partire dalle sole feature, senza target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ded85",
   "metadata": {},
   "source": [
    "---\n\n## 1.3 La distanza euclidea\n\nK-Means usa la **distanza euclidea** per misurare quanto un punto e lontano da un centroide.\n\n### Formula in 2 dimensioni\n\nPer due punti $A = (x_1, y_1)$ e $B = (x_2, y_2)$:\n\n$$d(A, B) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$$\n\nE il teorema di Pitagora: la distanza in linea retta.\n\n### Formula generale (n dimensioni)\n\nPer due punti con $n$ feature:\n\n$$d(A, B) = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}$$\n\n### Esempio numerico\n\nPunto A: (2, 3, 1)  \nPunto B: (5, 7, 2)\n\n$$d(A, B) = \\sqrt{(5-2)^2 + (7-3)^2 + (2-1)^2} = \\sqrt{9 + 16 + 1} = \\sqrt{26} \\approx 5.1$$\n\n---\n\n### Perche la distanza euclidea e importante\n\nK-Means assegna ogni punto al centroide **piu vicino** in termini di distanza euclidea.\n\nQuesto ha conseguenze importanti:\n1. **Scale diverse = problemi**: se una feature ha range 0-100 e un'altra 0-100.000, la seconda domina.\n2. **Forma sferica implicita**: la distanza euclidea definisce \"sfere\" attorno ai centroidi.\n3. **Outlier problematici**: punti molto lontani distorcono i centroidi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3815e6",
   "metadata": {},
   "source": [
    "---\n\n## 1.4 L'algoritmo K-Means passo per passo\n\nL'algoritmo e iterativo e segue questi passi:\n\n### Passo 0: Inizializzazione\nScegli K punti iniziali come centroidi (random o con metodo k-means++).\n\n### Passo 1: Assegnazione\nPer ogni punto, calcola la distanza da tutti i K centroidi.\nAssegna il punto al centroide piu vicino.\n\n### Passo 2: Aggiornamento\nRicalcola ogni centroide come la media dei punti assegnati a quel cluster.\n\n### Passo 3: Convergenza\nSe i centroidi non cambiano (o cambiano meno di una soglia), STOP.\nAltrimenti, torna al Passo 1.\n\n---\n\n### Visualizzazione dell'algoritmo\n\n```\nIterazione 0:           (centroidi iniziali random)\n                         (punti da clusterizzare)\n\nIterazione 1:  Assegna ogni punto al centroide piu vicino\n               Ricalcola i centroidi\n\nIterazione 2:  Riassegna, ricalcola...\n\n...\n\nConvergenza:   I centroidi non si muovono piu -> FINE\n```\n\n---\n\n### La funzione obiettivo (Inertia)\n\nK-Means minimizza l'**inertia** (o Within-Cluster Sum of Squares, WCSS):\n\n$$\\text{Inertia} = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} ||x_i - \\mu_k||^2$$\n\nIn parole: la somma delle distanze al quadrato di ogni punto dal suo centroide.\n\n**Meno inertia = cluster piu compatti.**\n\nAttenzione: l'inertia diminuisce SEMPRE all'aumentare di K (caso limite: K = n punti -> inertia = 0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4216da",
   "metadata": {},
   "source": [
    "---\n\n## 1.5 Le assunzioni forti di K-Means\n\nK-Means funziona bene **solo se** i dati rispettano certe condizioni. Se queste assunzioni sono violate, i risultati saranno sbagliati.\n\n---\n\n### Assunzione 1: Cluster sferici (isotropici)\n\nK-Means assume che i cluster abbiano **forma sferica** (o iper-sferica in n dimensioni).\n\nQuesto perche usa la distanza euclidea, che definisce \"cerchi\" di equidistanza attorno ai centroidi.\n\n**Problema:** Se i cluster hanno forma allungata, a \"banana\" o irregolare, K-Means li taglia male.\n\n---\n\n### Assunzione 2: Cluster di dimensioni simili\n\nK-Means tende ad assegnare lo stesso numero di punti a ogni cluster.\n\n**Problema:** Se un cluster ha 1000 punti e un altro ne ha 50, K-Means potrebbe \"rubare\" punti dal cluster grande per bilanciare.\n\n---\n\n### Assunzione 3: Varianza simile tra cluster\n\nK-Means assume che i cluster abbiano **dispersione simile** attorno al centroide.\n\n**Problema:** Se un cluster e molto compatto e un altro molto disperso, il confine sara sbagliato.\n\n---\n\n### Assunzione 4: Assenza di outlier significativi\n\nI centroidi sono **medie**, quindi molto sensibili agli outlier.\n\n**Problema:** Un singolo punto anomalo puo spostare significativamente un centroide.\n\n---\n\n### Tabella riassuntiva\n\n| Assunzione | Cosa assume K-Means | Cosa succede se violata |\n|------------|---------------------|-------------------------|\n| Forma | Cluster sferici | Cluster tagliati male |\n| Dimensione | Cluster bilanciati | Punti assegnati al cluster sbagliato |\n| Varianza | Dispersione simile | Confini distorti |\n| Outlier | Nessun outlier | Centroidi distorti |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28272b0f",
   "metadata": {},
   "source": [
    "---\n\n## 1.6 Il problema dell'inizializzazione\n\nK-Means e un algoritmo **greedy**: trova un minimo locale, non globale.\n\nIl risultato dipende da **dove partono i centroidi iniziali**.\n\n### Il problema\n\n```\nInizializzazione A:                Converge a soluzione X\nInizializzazione B:                Converge a soluzione Y (diversa!)\n```\n\nCon centroidi iniziali diversi, K-Means puo convergere a soluzioni diverse.\n\n### La soluzione: k-means++\n\nL'algoritmo **k-means++** (default in sklearn) sceglie i centroidi iniziali in modo intelligente:\n\n1. Scegli il primo centroide random.\n2. Per i successivi, scegli punti **lontani** dai centroidi gia scelti.\n3. La probabilita di scegliere un punto e proporzionale alla sua distanza dal centroide piu vicino.\n\nQuesto riduce la probabilita di inizializzazioni sfortunate.\n\n### n_init: eseguire piu volte\n\nSklearn esegue K-Means **n_init** volte con inizializzazioni diverse e tiene la soluzione con inertia minore.\n\n```python\nKMeans(n_clusters=3, n_init=10, random_state=42)\n```\n\nSignifica: esegui 10 volte, tieni il risultato migliore.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2838adb",
   "metadata": {},
   "source": [
    "---\n\n# Parte 2 - Schema Mentale e Mappa Logica\n\n---\n\n## 2.1 Quando usare K-Means\n\n### Situazioni ideali per K-Means\n\n| Condizione | Perche K-Means funziona |\n|------------|-------------------------|\n| **Cluster sferici** | La distanza euclidea li cattura bene |\n| **Cluster bilanciati** | Nessun cluster viene \"schiacciato\" |\n| **Varianza simile** | I confini saranno corretti |\n| **Dati scalati** | Tutte le feature contribuiscono equamente |\n| **K noto o stimabile** | Puoi usare Elbow/Silhouette (Lezione 21) |\n| **Molti dati** | K-Means e veloce anche su milioni di punti |\n\n### Segnali che K-Means e appropriato\n\n- Scatter plot 2D mostra gruppi \"rotondi\" e separati.\n- Le feature hanno scale simili (o le hai scalate).\n- Non ci sono outlier evidenti.\n- Il business suggerisce un numero ragionevole di segmenti.\n\n---\n\n## 2.2 Quando NON usare K-Means\n\n### Situazioni problematiche\n\n| Condizione | Perche K-Means fallisce | Alternativa |\n|------------|-------------------------|-------------|\n| **Cluster allungati** | Li taglia male | DBSCAN, Spectral |\n| **Cluster di dimensioni diverse** | Ruba punti | DBSCAN, GMM |\n| **Outlier** | Distorcono i centroidi | DBSCAN, rimuovi outlier |\n| **K ignoto** | Risultato arbitrario | Hierarchical per esplorare |\n| **Forma arbitraria** | Assume sfericita | DBSCAN |\n\n### Segnali che K-Means e inappropriato\n\n- Scatter plot mostra forme \"a banana\" o irregolari.\n- Un gruppo e molto piu grande degli altri.\n- Ci sono punti isolati lontani da tutto.\n- Non hai idea di quanti cluster cercare.\n\n---\n\n## 2.3 Checklist pre-K-Means\n\nPrima di applicare K-Means:\n\n- [ ] **Scaling**: hai applicato StandardScaler.\n- [ ] **Outlier**: hai verificato e gestito i punti anomali.\n- [ ] **K**: hai un'ipotesi su quanti cluster cercare.\n- [ ] **Forma**: i dati sembrano avere gruppi \"rotondi\".\n- [ ] **n_init**: stai usando n_init >= 10.\n- [ ] **random_state**: hai fissato il seed per riproducibilita.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sezione 3 - Schema mentale e decision map\n",
    "Quando usare K-Means, quando evitarlo e la checklist pre-applicazione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3ce8f",
   "metadata": {},
   "source": [
    "---\n\n# Parte 3 - Notebook Dimostrativo\n\n---\n\n## Demo 1: K-Means base per vedere l'algoritmo in azione\n\nGeneriamo dati con 3 gruppi ben separati e vediamo come K-Means li trova.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sezione 4 - Notebook dimostrativo\n",
    "Demo progressive per vedere K-Means, le iterazioni, i limiti e l'effetto degli outlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 1)\nScenario ideale per leggere inertia, silhouette e centroidi su dati sferici scalati.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 1: K-Means base\n",
    "# Intento: vedere K-Means in azione su dati ideali\n",
    "# Assunzioni: 3 cluster sferici, nessun valore mancante\n",
    "# Output: etichette, inertia, silhouette, grafici\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.8, random_state=42)\n",
    "assert X.shape == (300, 2) and y_true.shape == (300,), 'Shape inattesa'\n",
    "assert not np.isnan(X).any(), 'Sono presenti NaN nei dati'\n",
    "print('[Checkpoint] Dataset generato con shape', X.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "assert X_scaled.shape == X.shape, 'Scaling ha alterato la shape'\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "sil = silhouette_score(X_scaled, labels)\n",
    "assert -1 <= sil <= 1, 'Silhouette fuori range'\n",
    "print('Etichette uniche:', np.unique(labels))\n",
    "print('Punti per cluster:', np.bincount(labels))\n",
    "print(f'Inertia: {kmeans.inertia_:.2f} | Iterazioni: {kmeans.n_iter_} | Silhouette: {sil:.3f}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.7)\n",
    "axes[0].set_title('Gruppi veri (se disponibili)')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "centroidi_originali = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "axes[1].scatter(centroidi_originali[:, 0], centroidi_originali[:, 1], c='red', marker='X', s=300, edgecolors='black', linewidth=2, label='Centroidi')\n",
    "axes[1].set_title(f'K-Means (K=3) | Silhouette={sil:.3f}')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 2)\nVisualizza il movimento dei centroidi e perche inizializzazione e n_init contano.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 2: Visualizzare le iterazioni di K-Means\n",
    "# Intento: mostrare il movimento dei centroidi passo-passo\n",
    "# Assunzioni: 3 cluster sferici, nessun NaN\n",
    "# Output: grafici per iterazione e spostamento dei centroidi\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "np.random.seed(123)\n",
    "\n",
    "X_demo, _ = make_blobs(n_samples=150, centers=3, cluster_std=1.2, random_state=123)\n",
    "assert X_demo.shape[1] == 2, 'Attesa 2 feature per visualizzare'\n",
    "\n",
    "np.random.seed(999)\n",
    "initial_centroids = X_demo[np.random.choice(len(X_demo), 3, replace=False)]\n",
    "\n",
    "\n",
    "def kmeans_step_by_step(X, K, initial_centroids, max_iter=10):\n",
    "    centroids = initial_centroids.copy()\n",
    "    history = [centroids.copy()]\n",
    "    for iteration in range(max_iter):\n",
    "        distances = np.zeros((len(X), K))\n",
    "        for k in range(K):\n",
    "            distances[:, k] = np.sqrt(np.sum((X - centroids[k])**2, axis=1))\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        for k in range(K):\n",
    "            if np.sum(labels == k) > 0:\n",
    "                new_centroids[k] = X[labels == k].mean(axis=0)\n",
    "            else:\n",
    "                new_centroids[k] = centroids[k]\n",
    "        if np.allclose(centroids, new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "        history.append(centroids.copy())\n",
    "    return labels, centroids, history\n",
    "\n",
    "labels_final, centroids_final, history = kmeans_step_by_step(X_demo, 3, initial_centroids)\n",
    "print('[Checkpoint] Iterazioni registrate:', len(history))\n",
    "\n",
    "n_plots = min(4, len(history))\n",
    "fig, axes = plt.subplots(1, n_plots, figsize=(4*n_plots, 4))\n",
    "iterations_to_show = [0, 1, len(history)//2, len(history)-1][:n_plots]\n",
    "\n",
    "for ax, iter_num in zip(axes, iterations_to_show):\n",
    "    centroids_iter = history[iter_num]\n",
    "    distances = np.zeros((len(X_demo), 3))\n",
    "    for k in range(3):\n",
    "        distances[:, k] = np.sqrt(np.sum((X_demo - centroids_iter[k])**2, axis=1))\n",
    "    labels_iter = np.argmin(distances, axis=1)\n",
    "    ax.scatter(X_demo[:, 0], X_demo[:, 1], c=labels_iter, cmap='viridis', s=40, alpha=0.6)\n",
    "    ax.scatter(centroids_iter[:, 0], centroids_iter[:, 1], c='red', marker='X', s=200, edgecolors='black', linewidth=2)\n",
    "    title = 'Iterazione ' + str(iter_num)\n",
    "    if iter_num == 0:\n",
    "        title += ' (centroidi random)'\n",
    "    if iter_num == len(history)-1:\n",
    "        title += ' (convergenza)'\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Spostamento dei centroidi (dal punto di partenza):')\n",
    "for i in range(3):\n",
    "    start = initial_centroids[i]\n",
    "    end = centroids_final[i]\n",
    "    dist = np.sqrt(np.sum((end - start)**2))\n",
    "    print(f'  Centroide {i}: da ({start[0]:.2f},{start[1]:.2f}) a ({end[0]:.2f},{end[1]:.2f}) | spostamento {dist:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 3)\nMostra i fallimenti geometrici di K-Means su forme non convesse o varianze diverse.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 3: Quando K-Means fallisce\n",
    "# Intento: mostrare i limiti con forme non sferiche\n",
    "# Assunzioni: dataset moons/circles/varianza diversa\n",
    "# Output: grafici che evidenziano assegnazioni errate\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
    "\n",
    "np.random.seed(42)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.08, random_state=42)\n",
    "labels_moons = KMeans(n_clusters=2, random_state=42, n_init=10).fit_predict(X_moons)\n",
    "axes[0,0].scatter(X_moons[:,0], X_moons[:,1], c=y_moons, cmap='coolwarm', s=40, alpha=0.7)\n",
    "axes[0,0].set_title('Moons: gruppi veri')\n",
    "axes[1,0].scatter(X_moons[:,0], X_moons[:,1], c=labels_moons, cmap='coolwarm', s=40, alpha=0.7)\n",
    "axes[1,0].set_title('Moons: K-Means fallisce')\n",
    "\n",
    "X_circles, y_circles = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
    "labels_circles = KMeans(n_clusters=2, random_state=42, n_init=10).fit_predict(X_circles)\n",
    "axes[0,1].scatter(X_circles[:,0], X_circles[:,1], c=y_circles, cmap='coolwarm', s=40, alpha=0.7)\n",
    "axes[0,1].set_title('Cerchi: gruppi veri')\n",
    "axes[1,1].scatter(X_circles[:,0], X_circles[:,1], c=labels_circles, cmap='coolwarm', s=40, alpha=0.7)\n",
    "axes[1,1].set_title('Cerchi: K-Means fallisce')\n",
    "\n",
    "X_var, y_var = make_blobs(n_samples=300, centers=[[-2,0],[2,0]], cluster_std=[0.3,1.5], random_state=42)\n",
    "labels_var = KMeans(n_clusters=2, random_state=42, n_init=10).fit_predict(X_var)\n",
    "axes[0,2].scatter(X_var[:,0], X_var[:,1], c=y_var, cmap='coolwarm', s=40, alpha=0.7)\n",
    "axes[0,2].set_title('Varianza diversa: veri')\n",
    "axes[1,2].scatter(X_var[:,0], X_var[:,1], c=labels_var, cmap='coolwarm', s=40, alpha=0.7)\n",
    "axes[1,2].set_title('Varianza diversa: confini distorti')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 4)\nEvidenzia l'impatto degli outlier e introduce strategie robuste (cleaning o modelli alternativi).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 4: Effetto degli outlier\n",
    "# Intento: mostrare come pochi outlier spostano i centroidi\n",
    "# Assunzioni: 2 cluster sferici + 3 outlier lontani\n",
    "# Output: grafici comparativi e spostamento centroidi\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.seed(42)\n",
    "X_clean = np.vstack([\n",
    "    np.random.randn(100, 2) * 0.5 + np.array([-2, 0]),\n",
    "    np.random.randn(100, 2) * 0.5 + np.array([2, 0])\n",
    "])\n",
    "assert X_clean.shape == (200, 2)\n",
    "\n",
    "outliers = np.array([[0, 8], [-1, 7], [1, 7.5]])\n",
    "X_with_outliers = np.vstack([X_clean, outliers])\n",
    "assert X_with_outliers.shape == (203, 2)\n",
    "\n",
    "kmeans_clean = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "labels_clean = kmeans_clean.fit_predict(X_clean)\n",
    "\n",
    "kmeans_outliers = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "labels_outliers = kmeans_outliers.fit_predict(X_with_outliers)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].scatter(X_clean[:, 0], X_clean[:, 1], c=labels_clean, cmap='coolwarm', s=50, alpha=0.7)\n",
    "axes[0].scatter(kmeans_clean.cluster_centers_[:, 0], kmeans_clean.cluster_centers_[:, 1], c='black', marker='X', s=300, edgecolors='white', linewidth=2, label='Centroidi')\n",
    "axes[0].set_title('Senza outlier')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(-3, 10)\n",
    "\n",
    "axes[1].scatter(X_clean[:, 0], X_clean[:, 1], c=labels_outliers[:200], cmap='coolwarm', s=50, alpha=0.7)\n",
    "axes[1].scatter(outliers[:, 0], outliers[:, 1], c='green', s=150, marker='*', edgecolors='black', linewidth=1, label='Outlier')\n",
    "axes[1].scatter(kmeans_outliers.cluster_centers_[:, 0], kmeans_outliers.cluster_centers_[:, 1], c='black', marker='X', s=300, edgecolors='white', linewidth=2, label='Centroidi')\n",
    "axes[1].set_title('Con outlier')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(-3, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Spostamento centroidi (norma L2):')\n",
    "for i in range(2):\n",
    "    shift = np.sqrt(np.sum((kmeans_clean.cluster_centers_[i] - kmeans_outliers.cluster_centers_[i])**2))\n",
    "    print(f'  Cluster {i}: shift = {shift:.2f}')\n",
    "print('Nota: pochi outlier possono distorcere i centroidi e le assegnazioni.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sezione 5 - Esercizi guidati (step by step)\n",
    "Esegui in ordine 20.1 -> 20.3 con razionale, assunzioni e output attesi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perche questo esercizio (20.1)\nConsolidare calcoli manuali di centroide, distanza euclidea e inertia prima di usare sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0421e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 4  Esercizi Svolti\n",
    "\n",
    "---\n",
    "\n",
    "## Esercizio 20.1  Calcolo manuale della distanza euclidea e del centroide\n",
    "\n",
    "**Consegna:**\n",
    "Dati i seguenti punti in 2D appartenenti a un cluster:\n",
    "- A = (1, 2)\n",
    "- B = (3, 4)\n",
    "- C = (2, 1)\n",
    "- D = (4, 3)\n",
    "\n",
    "1. Calcola il centroide del cluster\n",
    "2. Calcola la distanza euclidea di ogni punto dal centroide\n",
    "3. Calcola l'inertia (somma delle distanze al quadrato)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 20.1 - SOLUZIONE GUIDATA\n",
    "# Intento: calcolare centroide, distanze e inertia a mano\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "punti = np.array([[1, 2], [3, 4], [2, 1], [4, 3]])\n",
    "nomi = ['A', 'B', 'C', 'D']\n",
    "assert punti.shape == (4, 2), 'Attesi 4 punti 2D'\n",
    "\n",
    "centroide = punti.mean(axis=0)\n",
    "print('Centroide:', centroide)\n",
    "\n",
    "print('\n",
    "Distanze euclidee dal centroide:')\n",
    "distanze = []\n",
    "for nome, p in zip(nomi, punti):\n",
    "    d = np.linalg.norm(p - centroide)\n",
    "    distanze.append(d)\n",
    "    print(f'  {nome}: {d:.3f}')\n",
    "\n",
    "inertia = np.sum((punti - centroide)**2)\n",
    "print(f'Inertia (WCSS): {inertia:.3f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.scatter(punti[:,0], punti[:,1], c='blue', s=80, label='Punti')\n",
    "ax.scatter(centroide[0], centroide[1], c='red', marker='X', s=200, label='Centroide')\n",
    "for nome, p, d in zip(nomi, punti, distanze):\n",
    "    ax.plot([centroide[0], p[0]], [centroide[1], p[1]], 'k--', alpha=0.5)\n",
    "    ax.annotate(f'{nome}\n",
    "d={d:.2f}', (p[0], p[1]), textcoords='offset points', xytext=(8,4), fontsize=9)\n",
    "ax.set_title(f'Cluster e centroide (Inertia={inertia:.3f})')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perche questo esercizio (20.2)\nApplicare K-Means a un dataset RFM sintetico con scaling, silhouette e lettura business dei cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c0fc2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Esercizio 20.2  Clustering di clienti e-commerce\n",
    "\n",
    "**Consegna:**\n",
    "Un e-commerce ha i seguenti dati sui clienti:\n",
    "- `frequenza`: numero di acquisti negli ultimi 12 mesi\n",
    "- `spesa_media`: valore medio per acquisto ()\n",
    "- `recency`: giorni dall'ultimo acquisto\n",
    "\n",
    "Applica K-Means per segmentare i clienti in 3 gruppi. Interpreta i cluster trovati."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 20.2 - SOLUZIONE\n",
    "# Intento: segmentare clienti e-commerce con K-Means (K=3)\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "premium = pd.DataFrame({\n",
    "    'frequenza': np.random.normal(25, 5, 80).clip(10, 40),\n",
    "    'spesa_media': np.random.normal(150, 30, 80).clip(80, 250),\n",
    "    'recency': np.random.normal(10, 5, 80).clip(1, 30)\n",
    "})\n",
    "\n",
    "tocasual = pd.DataFrame({\n",
    "    'frequenza': np.random.normal(5, 2, 120).clip(1, 12),\n",
    "    'spesa_media': np.random.normal(50, 15, 120).clip(20, 100),\n",
    "    'recency': np.random.normal(45, 15, 120).clip(15, 90)\n",
    "})\n",
    "\n",
    "dormienti = pd.DataFrame({\n",
    "    'frequenza': np.random.normal(12, 4, 100).clip(3, 25),\n",
    "    'spesa_media': np.random.normal(80, 25, 100).clip(30, 150),\n",
    "    'recency': np.random.normal(150, 40, 100).clip(90, 250)\n",
    "})\n",
    "\n",
    "df_clienti = pd.concat([premium, tocasual, dormienti], ignore_index=True)\n",
    "assert df_clienti.shape == (300, 3), 'Shape inattesa per df_clienti'\n",
    "assert not df_clienti.isna().any().any(), 'NaN nel dataset sintetico'\n",
    "print('[Checkpoint] Dataset clienti pronto', df_clienti.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_clienti)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "sil = silhouette_score(X_scaled, labels)\n",
    "assert -1 <= sil <= 1, 'Silhouette fuori range'\n",
    "print('Punti per cluster:', np.bincount(labels))\n",
    "print(f'Inertia: {kmeans.inertia_:.2f} | Silhouette: {sil:.3f}')\n",
    "\n",
    "df_clienti['cluster'] = labels\n",
    "cluster_stats = df_clienti.groupby('cluster').agg({'frequenza':['mean','std'], 'spesa_media':['mean','std'], 'recency':['mean','std']}).round(1)\n",
    "print('\n",
    "Statistiche per cluster:\n",
    "', cluster_stats)\n",
    "\n",
    "print('\n",
    "Interpretazione cluster (descrittiva):')\n",
    "for cid in sorted(df_clienti['cluster'].unique()):\n",
    "    subset = df_clienti[df_clienti['cluster']==cid]\n",
    "    freq = subset['frequenza'].mean()\n",
    "    spesa = subset['spesa_media'].mean()\n",
    "    rec = subset['recency'].mean()\n",
    "    if freq > 20 and rec < 40:\n",
    "        profilo = 'Clienti premium (frequenti, alta spesa, molto attivi)'\n",
    "        azione = 'Programma loyalty, anteprime, offerte esclusive'\n",
    "    elif rec > 100:\n",
    "        profilo = 'Clienti dormienti (erano attivi, ora assenti)'\n",
    "        azione = 'Campagne win-back e sconti di rientro'\n",
    "    else:\n",
    "        profilo = 'Clienti occasionali (acquisti sporadici)'\n",
    "        azione = 'Stimolare frequenza: cross-sell, newsletter'\n",
    "    print(f\"- Cluster {cid}: {profilo} | Azione: {azione}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].scatter(df_clienti['frequenza'], df_clienti['spesa_media'], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "axes[0].set_xlabel('Frequenza (acquisti/anno)')\n",
    "axes[0].set_ylabel('Spesa media (EUR)')\n",
    "axes[0].set_title('Frequenza vs Spesa')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(df_clienti['frequenza'], df_clienti['recency'], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "axes[1].set_xlabel('Frequenza (acquisti/anno)')\n",
    "axes[1].set_ylabel('Recency (giorni)')\n",
    "axes[1].set_title('Frequenza vs Recency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].scatter(df_clienti['spesa_media'], df_clienti['recency'], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "axes[2].set_xlabel('Spesa media (EUR)')\n",
    "axes[2].set_ylabel('Recency (giorni)')\n",
    "axes[2].set_title('Spesa vs Recency')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.colorbar(axes[2].collections[0], ax=axes, label='Cluster', shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perche questo esercizio (20.3)\nDimostrare quantitativamente perche lo scaling e cruciale per K-Means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n###  Esercizio 20.3  Confronto: Con e senza Scaling\n\n**Consegna:**\nHai un dataset con feature su scale molto diverse. Dimostra visivamente e quantitativamente la differenza tra applicare K-Means con e senza StandardScaler.\n\n**Cosa deve emergere:**\n1. Quanto cambiano le assegnazioni ai cluster\n2. Quanto cambia la silhouette score\n3. Perche lo scaling  cruciale\n\n**Dataset:**\n```python\nfeature_1 = [2, 3, 8, 9, 100]     # Scala: 2-100\nfeature_2 = [0.01, 0.02, 0.08, 0.09, 0.5]  # Scala: 0.01-0.5\n```\n\n**Hint:** Senza scaling, quale feature \"domina\" la distanza euclidea"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 20.3 - Confronto K-Means con/ senza scaling\n",
    "# Intento: mostrare come le scale diverse alterano assegnazioni e silhouette\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "c1_f1 = np.random.normal(10, 2, 30)\n",
    "c1_f2 = np.random.normal(0.1, 0.02, 30)\n",
    "\n",
    "c2_f1 = np.random.normal(50, 5, 30)\n",
    "c2_f2 = np.random.normal(0.5, 0.05, 30)\n",
    "\n",
    "c3_f1 = np.random.normal(90, 3, 30)\n",
    "c3_f2 = np.random.normal(0.9, 0.03, 30)\n",
    "\n",
    "feature_1 = np.concatenate([c1_f1, c2_f1, c3_f1])\n",
    "feature_2 = np.concatenate([c1_f2, c2_f2, c3_f2])\n",
    "true_labels = np.array([0]*30 + [1]*30 + [2]*30)\n",
    "\n",
    "X = np.column_stack([feature_1, feature_2])\n",
    "assert X.shape == (90, 2)\n",
    "print('[Checkpoint] Range feature1:', feature_1.min(), '-', feature_1.max())\n",
    "print('[Checkpoint] Range feature2:', feature_2.min(), '-', feature_2.max())\n",
    "\n",
    "kmeans_no = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_no = kmeans_no.fit_predict(X)\n",
    "sil_no = silhouette_score(X, labels_no)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans_sc = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_sc = kmeans_sc.fit_predict(X_scaled)\n",
    "sil_sc = silhouette_score(X_scaled, labels_sc)\n",
    "\n",
    "changed = np.sum(labels_no != labels_sc)\n",
    "print('Silhouette senza scaling:', round(sil_no,3))\n",
    "print('Silhouette con scaling:', round(sil_sc,3))\n",
    "print('Punti che cambiano cluster:', changed, '/', len(X))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].scatter(X[:,0], X[:,1], c=true_labels, cmap='viridis', s=60, alpha=0.7)\n",
    "axes[0].set_title('Cluster veri (ground truth)')\n",
    "axes[0].set_xlabel('Feature 1 (scala grande)')\n",
    "axes[0].set_ylabel('Feature 2 (scala piccola)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(X[:,0], X[:,1], c=labels_no, cmap='viridis', s=60, alpha=0.7)\n",
    "axes[1].set_title(f'Senza scaling\n",
    "Silhouette: {sil_no:.3f}')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].scatter(X[:,0], X[:,1], c=labels_sc, cmap='viridis', s=60, alpha=0.7)\n",
    "axes[2].set_title(f'Con scaling\n",
    "Silhouette: {sil_sc:.3f}')\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Lezione: con scale diverse K-Means privilegia la feature a range maggiore; con scaling le feature pesano uguale e la qualita migliora.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sezione 6 - Conclusione operativa\n",
    "Punti chiave, metodi usati e errori ricorrenti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacefcf1",
   "metadata": {},
   "source": [
    "---\n\n## 5. Cosa Portarsi a Casa\n\n### Concetti Fondamentali\n\n| Concetto | Definizione |\n|----------|-------------|\n| **K-Means** | Algoritmo che partiziona i dati in K cluster minimizzando l'inertia |\n| **Centroide** | Punto medio di un cluster, calcolato come media delle coordinate |\n| **Inertia (WCSS)** | Somma delle distanze quadrate dai punti ai centroidi |\n| **k-means++** | Inizializzazione intelligente che distribuisce i centroidi iniziali |\n\n### Errori Comuni da Evitare\n\n| Errore | Problema | Soluzione |\n|--------|----------|-----------|\n| Non scalare i dati | Le feature con range grande dominano | Sempre `StandardScaler` prima |\n| Scegliere K a caso | Cluster non significativi | Usare Elbow + Silhouette (Lezione 21) |\n| Ignorare la forma dei cluster | K-Means fallisce con forme non convesse | Usare DBSCAN per forme arbitrarie (Lezione 23) |\n| Una sola inizializzazione | Risultato dipende dal caso | Usare `n_init=10` o superiore |\n| Non interpretare i cluster | Clustering inutile senza significato | Sempre analizzare le caratteristiche |\n\n### Ponte verso la Lezione 21\n\n**Problema aperto:** Come scegliere K in modo sistematico?\n\nIn questa lezione abbiamo sempre \"saputo\" K a priori. Ma nella realta:\n- Non sappiamo quanti cluster ci siano.\n- Diverse scelte di K danno risultati diversi.\n- Servono metriche oggettive per decidere.\n\n**Nella prossima lezione:**\n- Metodo del Gomito (Elbow Method).\n- Silhouette Analysis approfondita.\n- Gap Statistic.\n- Trade-off interpretabilita vs performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods explained\n",
    "| Metodo/Funzione | Cosa fa | Input (tipo/shape) | Output | Errori comuni | Quando usarlo / evitarlo |\n",
    "|-----------------|---------|--------------------|--------|---------------|---------------------------|\n",
    "| `KMeans.fit_predict` | Esegue clustering e restituisce label | X array 2D (n,p) | labels 1D (n,) | Senza scaling domina la feature piu' ampia; K errato | Cluster sferici e K stimato; evita forme arbitrarie |\n",
    "| `StandardScaler` | Scala feature a media 0, std 1 | X 2D numerico | X_scaled 2D | Shape errata o NaN | Prima di modelli basati su distanza |\n",
    "| `silhouette_score` | Valuta coesione/separazione cluster | X 2D, labels 1D | float [-1,1] | Valori fuori range segnalano label errate | Confronto configurazioni di clustering |\n",
    "| `make_blobs` | Genera dati sintetici separabili | n_samples int, centers | X 2D, y 1D | Cluster sovrapposti se std alto | Demo controllate |\n",
    "| `make_moons/make_circles` | Genera forme non sferiche | n_samples, noise | X 2D, y 1D | Non separabili linearmente | Per mostrare limiti di K-Means |\n",
    "| `np.linalg.norm` | Calcola norma/distanza | array | float | Uso su shape errata | Per distanze euclidee semplici |\n",
    "| `np.clip` | Tronca valori in un intervallo | array, min, max | array | Range scelto male distorce | Per limitare valori simulati |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common errors and quick debug\n",
    "| Sintomo | Causa probabile | Fix rapido |\n",
    "|---------|-----------------|------------|\n",
    "| Silhouette molto bassa/negativa | K errato o cluster sovrapposti | Provare altri K, valutare forma dei dati |\n",
    "| Cluster sbilanciati senza motivo | Feature non scalate | Applicare StandardScaler |\n",
    "| Risultato cambia a ogni run | Seed non fissato o n_init basso | Impostare random_state, aumentare n_init |\n",
    "| Centroidi spostati verso punti isolati | Outlier non gestiti | Rimuovere outlier o usare metodi robusti |\n",
    "| K-Means taglia forme a mezzaluna/cerchi | Assunzioni geometriche violate | Usare DBSCAN o spectral clustering |\n",
    "| Inertia molto alta anche con K alto | Dati non sferici o K insufficiente | Verificare shape, provare alternative |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bignami operativo\n",
    "- K-Means minimizza l'inertia su cluster sferici: scala sempre le feature.\n",
    "- Numero K va motivato (Elbow/Silhouette nella lezione 21).\n",
    "- Outlier e forme non convesse possono distorcere i centroidi: valuta alternative.\n",
    "- Usa n_init >= 10 e random_state per risultati stabili.\n",
    "- Le label sono ipotesi: interpreta e valida con il dominio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382a2ec",
   "metadata": {},
   "source": [
    "### Bignami esteso - Lezione 20\n\n## BIGNAMI Lezione 20: K-Means Clustering\n\n### Definizioni Essenziali\n\n| Termine | Definizione |\n|---------|-------------|\n| **K-Means** | Algoritmo di clustering che partiziona N punti in K cluster, assegnando ogni punto al cluster con centroide piu vicino |\n| **Centroide** | Centro geometrico di un cluster: $\\mathbf{c}_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\mathbf{x}_i$ |\n| **Inertia** | Within-Cluster Sum of Squares: $\\text{WCSS} = \\sum_{k=1}^{K} \\sum_{i \\in C_k} \\|\\mathbf{x}_i - \\mathbf{c}_k\\|^2$ |\n| **Silhouette** | Misura di qualita del clustering: $s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$ |\n| **k-means++** | Inizializzazione che sceglie centroidi iniziali distanti tra loro per evitare minimi locali |\n\n---\n\n### Formule Chiave\n\n**Distanza Euclidea:**\n$$d(\\mathbf{x}, \\mathbf{c}) = \\sqrt{\\sum_{j=1}^{d} (x_j - c_j)^2}$$\n\n**Centroide di un cluster:**\n$$\\mathbf{c}_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\mathbf{x}_i$$\n\n**Inertia (WCSS):**\n$$J = \\sum_{k=1}^{K} \\sum_{i \\in C_k} \\|\\mathbf{x}_i - \\mathbf{c}_k\\|^2$$\n\n---\n\n### Checklist K-Means\n\n```\n Feature selezionate (solo numeriche rilevanti)\n StandardScaler applicato\n K scelto con criterio (Elbow/Silhouette - Lezione 21)\n n_init >= 10 per evitare minimi locali\n Cluster interpretati e nominati\n Visualizzazione per validare i risultati\n Outlier gestiti se necessario\n```\n\n---\n\n### Template di Codice\n\n```python\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\n\n# 1. Preprocessing\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 2. K-Means\nkmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\nlabels = kmeans.fit_predict(X_scaled)\n\n# 3. Valutazione\nprint(f\"Inertia: {kmeans.inertia_:.2f}\")\nprint(f\"Silhouette: {silhouette_score(X_scaled, labels):.3f}\")\n\n# 4. Interpretazione\ndf['cluster'] = labels\ndf.groupby('cluster').mean()  # Profilo dei cluster\n```\n\n---\n\n### Quando K-Means Funziona vs Quando Fallisce\n\n| Funziona Bene | Fallisce |\n|---------------|----------|\n| Cluster sferici/convessi | Forme arbitrarie (mezzalune, anelli) |\n| Cluster di dimensioni simili | Cluster molto sbilanciati |\n| Dati ben separati | Cluster sovrapposti |\n| Pochi outlier | Molti outlier (spostano i centroidi) |\n\n---\n\n### Parametri Chiave di KMeans\n\n| Parametro | Default | Significato |\n|-----------|---------|-------------|\n| `n_clusters` | 8 | Numero di cluster K |\n| `init` | 'k-means++' | Metodo di inizializzazione |\n| `n_init` | 10 | Numero di inizializzazioni diverse |\n| `max_iter` | 300 | Iterazioni massime per convergenza |\n| `random_state` | None | Seed per riproducibilita |\n\n---\n\n### Flusso Mentale\n\n```\nDATI  StandardScaler  KMeans(K)  Valutazione  Interpretazione\n                                                   \n      Normalizza        Itera      Silhouette    Significato\n      le scale       converge     Inertia       business\n```\n\n---\n\n\"K-Means trova K centroidi che minimizzano le distanze intra-cluster.\\nFunziona meglio con cluster sferici, dati scalati e K scelto con criterio.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sezione 7 - End-of-lesson checklist e glossario\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist finale\n",
    "- [ ] Dati verificati (shape, assenza di NaN) e scaling applicato\n",
    "- [ ] Demos eseguite con silhouette e inertia lette criticamente\n",
    "- [ ] Esercizi 20.1-20.3 completati con interpretazioni testuali\n",
    "- [ ] Outlier considerati prima di K-Means\n",
    "- [ ] Numero K motivato (Elbow/Silhouette in follow-up)\n",
    "- [ ] Cluster descritti e condivisi come ipotesi, non verita'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossario (termini usati)\n",
    "1. K-Means - algoritmo di clustering a centroidi.\n",
    "2. Centroide - media delle coordinate di un cluster.\n",
    "3. Inertia/WCSS - somma degli scarti quadratici dai centroidi.\n",
    "4. Silhouette score - coesione/separazione dei cluster in [-1,1].\n",
    "5. k-means++ - inizializzazione che distanzia i centroidi iniziali.\n",
    "6. StandardScaler - scaling a media 0 e deviazione 1.\n",
    "7. Cluster sferico - gruppo convesso adatto alla distanza euclidea.\n",
    "8. Outlier - punto isolato che distorce i centroidi.\n",
    "9. n_init - numero di inizializzazioni K-Means eseguite.\n",
    "10. Random seed - valore che rende replicabile la casualita'.\n",
    "11. Flow decisionale - schema per scegliere algoritmo/approccio.\n",
    "12. Feature scaling - normalizzazione delle scale prima di misurare distanze.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sezione 8 - Didactic changelog (max 10 voci)\n",
    "1. Riorganizzata la lezione nelle 8 sezioni obbligatorie con heading chiari.\n",
    "2. Aggiunte rationale e checkpoint a tutte le demo e agli esercizi 20.1-20.3.\n",
    "3. Inserite Methods explained, Common errors, Glossario e Checklist finale.\n",
    "4. Ripulito testo da emoji e reso ASCII-safe nelle nuove sezioni.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}