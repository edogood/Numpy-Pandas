{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fcdb09f",
   "metadata": {},
   "source": [
    "# Lezione 30 — Rappresentare il Testo come Dato\n",
    "\n",
    "## Obiettivi di Apprendimento\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "1. **Comprendere** perché il testo deve essere convertito in numeri per il Machine Learning\n",
    "2. **Applicare** tecniche di preprocessing testuale (tokenizzazione, normalizzazione)\n",
    "3. **Implementare** rappresentazioni Bag of Words (BoW) e varianti\n",
    "4. **Distinguere** tra rappresentazioni sparse e dense\n",
    "5. **Valutare** vantaggi e limiti delle diverse rappresentazioni\n",
    "\n",
    "## Importanza per il Data Analyst\n",
    "\n",
    "Il testo è uno dei dati più abbondanti in azienda: email, ticket di supporto, recensioni, documenti, chat. Ma gli algoritmi di ML lavorano solo con numeri. La capacità di **trasformare testo in vettori numerici** è fondamentale per:\n",
    "\n",
    "- Classificazione documenti\n",
    "- Sentiment analysis\n",
    "- Clustering di ticket/email\n",
    "- Information retrieval\n",
    "- Topic modeling\n",
    "\n",
    "## Posizione nel Percorso\n",
    "\n",
    "```\n",
    "BLOCCO 4: AI & NLP\n",
    "├── Lezione 29: Fondamenti AI ✓\n",
    "├── Lezione 30: Rappresentare il Testo come Dato ◄── SEI QUI\n",
    "├── Lezione 31: TF-IDF e Text Mining\n",
    "├── Lezione 32: Sentiment Analysis\n",
    "└── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c5de7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Teoria Concettuale\n",
    "\n",
    "## 1.1 Il Problema Fondamentale\n",
    "\n",
    "### Perché i Numeri?\n",
    "\n",
    "Gli algoritmi di Machine Learning operano su vettori numerici. Internamente, ogni modello esegue operazioni matematiche:\n",
    "\n",
    "- **Distanze** (KNN, clustering): $d(x_1, x_2) = \\sqrt{\\sum(x_{1i} - x_{2i})^2}$\n",
    "- **Prodotti scalari** (SVM, regressione): $w \\cdot x + b$\n",
    "- **Derivate** (gradient descent): $\\frac{\\partial L}{\\partial w}$\n",
    "\n",
    "Queste operazioni **non sono definite** su stringhe di testo:\n",
    "- Non puoi calcolare `\"ciao\" + \"mondo\"`\n",
    "- Non puoi calcolare `distanza(\"gatto\", \"cane\")`\n",
    "- Non puoi calcolare `derivata(\"documento\")`\n",
    "\n",
    "### La Trasformazione Necessaria\n",
    "\n",
    "```\n",
    "TESTO                    VETTORE NUMERICO\n",
    "\"Il gatto nero\"    →     [0, 1, 0, 1, 0, 1, 0, ...]\n",
    "\"Il cane bianco\"   →     [0, 0, 1, 0, 1, 0, 1, ...]\n",
    "```\n",
    "\n",
    "Questa trasformazione si chiama **vectorization** o **text embedding**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e37e4c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 Preprocessing del Testo\n",
    "\n",
    "Prima di vettorizzare, il testo grezzo deve essere **preprocessato**. Questo migliora la qualità della rappresentazione.\n",
    "\n",
    "### Pipeline di Preprocessing Tipica\n",
    "\n",
    "```\n",
    "TESTO RAW\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────────────────────┐\n",
    "│  1. LOWERCASING                 │  \"Il Gatto NERO\" → \"il gatto nero\"\n",
    "└─────────────────────────────────┘\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────────────────────┐\n",
    "│  2. TOKENIZZAZIONE              │  \"il gatto nero\" → [\"il\", \"gatto\", \"nero\"]\n",
    "└─────────────────────────────────┘\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────────────────────┐\n",
    "│  3. RIMOZIONE PUNTEGGIATURA     │  [\"ciao\", \"!\", \"come\", \"?\"] → [\"ciao\", \"come\"]\n",
    "└─────────────────────────────────┘\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────────────────────┐\n",
    "│  4. RIMOZIONE STOPWORDS         │  [\"il\", \"gatto\", \"nero\"] → [\"gatto\", \"nero\"]\n",
    "└─────────────────────────────────┘\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────────────────────┐\n",
    "│  5. STEMMING / LEMMATIZATION    │  [\"gatti\", \"gattini\"] → [\"gatt\", \"gatt\"]\n",
    "└─────────────────────────────────┘\n",
    "    │\n",
    "    ▼\n",
    "TOKENS PULITI\n",
    "```\n",
    "\n",
    "### Definizioni\n",
    "\n",
    "| Operazione | Descrizione | Esempio |\n",
    "|------------|-------------|---------|\n",
    "| **Tokenizzazione** | Dividere il testo in unità minime (token) | \"oggi piove\" → [\"oggi\", \"piove\"] |\n",
    "| **Stopwords** | Parole molto frequenti ma poco informative | \"il\", \"la\", \"di\", \"che\", \"e\", \"a\" |\n",
    "| **Stemming** | Riduzione brutale al tema | \"correndo\" → \"corr\" |\n",
    "| **Lemmatization** | Riduzione alla forma base del dizionario | \"correndo\" → \"correre\" |\n",
    "\n",
    "### Considerazioni\n",
    "\n",
    "Non tutte le operazioni sono sempre necessarie:\n",
    "- **Sentiment analysis**: le stopwords possono essere importanti (\"non\" è cruciale!)\n",
    "- **Named Entity Recognition**: il case è informativo (\"apple\" vs \"Apple\")\n",
    "- **Document similarity**: stemming aggressivo può perdere sfumature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7b1b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 Bag of Words (BoW)\n",
    "\n",
    "### L'Idea Fondamentale\n",
    "\n",
    "Il modello **Bag of Words** rappresenta un documento come un **multi-set di parole**, ignorando l'ordine.\n",
    "\n",
    "È chiamato \"bag\" (sacchetto) perché:\n",
    "- Le parole vengono \"buttate in un sacchetto\"\n",
    "- Si conta quante volte appare ogni parola\n",
    "- L'ordine originale viene perso\n",
    "\n",
    "### Costruzione del Vocabolario\n",
    "\n",
    "**Step 1**: Raccogliere tutti i token unici dal corpus (insieme di documenti)\n",
    "\n",
    "```\n",
    "Documento 1: \"il gatto mangia\"\n",
    "Documento 2: \"il cane mangia\"\n",
    "Documento 3: \"il gatto dorme\"\n",
    "\n",
    "Vocabolario: [\"il\", \"gatto\", \"cane\", \"mangia\", \"dorme\"]\n",
    "             idx: 0     1       2        3        4\n",
    "```\n",
    "\n",
    "**Step 2**: Rappresentare ogni documento come vettore di conteggi\n",
    "\n",
    "```\n",
    "                    il  gatto  cane  mangia  dorme\n",
    "Documento 1:       [ 1,   1,    0,     1,      0  ]\n",
    "Documento 2:       [ 1,   0,    1,     1,      0  ]\n",
    "Documento 3:       [ 1,   1,    0,     0,      1  ]\n",
    "```\n",
    "\n",
    "### Formalizzazione\n",
    "\n",
    "Dato un vocabolario $V = \\{w_1, w_2, ..., w_n\\}$, un documento $d$ viene rappresentato come:\n",
    "\n",
    "$$\\vec{d} = [c(w_1, d), c(w_2, d), ..., c(w_n, d)]$$\n",
    "\n",
    "Dove $c(w_i, d)$ = numero di occorrenze della parola $w_i$ nel documento $d$.\n",
    "\n",
    "### Variante: Binary BoW\n",
    "\n",
    "Invece di contare, si usa 0/1 (presenza/assenza):\n",
    "\n",
    "$$\\vec{d} = [\\mathbb{1}(w_1 \\in d), \\mathbb{1}(w_2 \\in d), ..., \\mathbb{1}(w_n \\in d)]$$\n",
    "\n",
    "Dove $\\mathbb{1}(condizione)$ = 1 se vera, 0 altrimenti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f0d7c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 N-grammi\n",
    "\n",
    "### Limitazione del BoW\n",
    "\n",
    "Il BoW perde completamente l'ordine delle parole. Questo può essere problematico:\n",
    "\n",
    "| Frase | BoW | Significato |\n",
    "|-------|-----|-------------|\n",
    "| \"Il gatto mangia il topo\" | {gatto:1, mangia:1, topo:1} | Il gatto è predatore |\n",
    "| \"Il topo mangia il gatto\" | {gatto:1, mangia:1, topo:1} | Il topo è predatore |\n",
    "\n",
    "**Stesso vettore, significato opposto!**\n",
    "\n",
    "### Soluzione: N-grammi\n",
    "\n",
    "Un **n-gramma** è una sequenza contigua di n elementi.\n",
    "\n",
    "| n | Nome | Esempio per \"il gatto nero\" |\n",
    "|---|------|----------------------------|\n",
    "| 1 | Unigram | [\"il\", \"gatto\", \"nero\"] |\n",
    "| 2 | Bigram | [\"il gatto\", \"gatto nero\"] |\n",
    "| 3 | Trigram | [\"il gatto nero\"] |\n",
    "\n",
    "### Esempio Pratico\n",
    "\n",
    "```\n",
    "Frase: \"non mi piace\"\n",
    "\n",
    "Unigrammi:  [\"non\", \"mi\", \"piace\"]\n",
    "Bigrammi:   [\"non mi\", \"mi piace\"]\n",
    "Trigrammi:  [\"non mi piace\"]\n",
    "\n",
    "Combinato (1,2): [\"non\", \"mi\", \"piace\", \"non mi\", \"mi piace\"]\n",
    "```\n",
    "\n",
    "### Trade-off\n",
    "\n",
    "| Aspetto | Unigrammi | Bigrammi/Trigrammi |\n",
    "|---------|-----------|-------------------|\n",
    "| **Dimensionalità** | Moderata | Alta (esponenziale) |\n",
    "| **Cattura ordine** | No | Parzialmente |\n",
    "| **Sparsità** | Moderata | Molto alta |\n",
    "| **Generalizzazione** | Buona | Può essere limitata |\n",
    "\n",
    "Tipicamente si usa un mix: `ngram_range=(1,2)` cattura sia parole singole che coppie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5ba65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 Rappresentazioni Sparse vs Dense\n",
    "\n",
    "### Caratteristiche delle Rappresentazioni\n",
    "\n",
    "Le rappresentazioni testuali possono essere:\n",
    "\n",
    "**SPARSE (BoW, TF-IDF)**\n",
    "```\n",
    "Vettore: [0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, ...]\n",
    "          ↑  ↑  ↑     ↑  ↑  ↑     ↑  ↑  ↑  ↑  ↑     ↑\n",
    "        Molti zeri, pochi valori non-nulli\n",
    "        \n",
    "Dimensionalità: 10.000 - 100.000+ (dimensione vocabolario)\n",
    "```\n",
    "\n",
    "**DENSE (Word2Vec, BERT embeddings)**\n",
    "```\n",
    "Vettore: [0.23, -0.45, 0.12, 0.78, -0.33, 0.56, ...]\n",
    "          ↑      ↑      ↑      ↑      ↑      ↑\n",
    "        Tutti valori non-nulli, significativi\n",
    "        \n",
    "Dimensionalità: 100 - 1024 (fissa, compatta)\n",
    "```\n",
    "\n",
    "### Confronto\n",
    "\n",
    "| Aspetto | Sparse (BoW) | Dense (Embeddings) |\n",
    "|---------|--------------|-------------------|\n",
    "| **Dimensionalità** | Alta (~vocabolario) | Bassa (100-1024) |\n",
    "| **Interpretabilità** | Alta (ogni dim = parola) | Bassa (dimensioni astratte) |\n",
    "| **Similarità semantica** | Limitata | Catturata |\n",
    "| **Out-of-vocabulary** | Ignorato | Gestibile |\n",
    "| **Complessità** | Semplice | Richiede pre-training |\n",
    "| **Storage** | Efficiente (CSR) | Denso ma compatto |\n",
    "\n",
    "### Esempio di Similarità\n",
    "\n",
    "**Sparse (BoW)**:\n",
    "```\n",
    "\"re\"    → [0, 0, 1, 0, 0, 0, ...]  (indice di \"re\")\n",
    "\"regina\"→ [0, 0, 0, 0, 1, 0, ...]  (indice di \"regina\")\n",
    "\n",
    "Similarità coseno = 0 (vettori ortogonali!)\n",
    "```\n",
    "\n",
    "**Dense (Word2Vec)**:\n",
    "```\n",
    "\"re\"    → [0.23, 0.45, -0.12, ...]\n",
    "\"regina\"→ [0.25, 0.42, -0.10, ...]\n",
    "\n",
    "Similarità coseno ≈ 0.95 (vettori molto simili)\n",
    "```\n",
    "\n",
    "### Quando Usare Cosa\n",
    "\n",
    "| Scenario | Consigliato |\n",
    "|----------|-------------|\n",
    "| Classificazione documenti (molti dati) | BoW/TF-IDF |\n",
    "| Similarità semantica | Dense embeddings |\n",
    "| Interpretabilità richiesta | BoW (ogni feature = parola) |\n",
    "| Risorse limitate | BoW (nessun modello pre-trained) |\n",
    "| Transfer learning | Dense embeddings pre-trained |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384bc7d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Schema Mentale\n",
    "\n",
    "## Mappa Decisionale: Vettorizzazione del Testo\n",
    "\n",
    "```\n",
    "                    ┌─────────────────────────┐\n",
    "                    │    HAI TESTO DA         │\n",
    "                    │    PROCESSARE?          │\n",
    "                    └────────────┬────────────┘\n",
    "                                 │\n",
    "                    ┌────────────▼────────────┐\n",
    "                    │   PREPROCESSING         │\n",
    "                    │   (tokenize, clean)     │\n",
    "                    └────────────┬────────────┘\n",
    "                                 │\n",
    "              ┌──────────────────┼──────────────────┐\n",
    "              │                  │                  │\n",
    "              ▼                  ▼                  ▼\n",
    "    ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐\n",
    "    │  Bag of Words   │ │  TF-IDF         │ │  Embeddings     │\n",
    "    │  (CountVec)     │ │  (TfidfVec)     │ │  (Word2Vec, etc)│\n",
    "    └────────┬────────┘ └────────┬────────┘ └────────┬────────┘\n",
    "             │                   │                   │\n",
    "             │                   │                   │\n",
    "    ┌────────▼────────┐ ┌────────▼────────┐ ┌────────▼────────┐\n",
    "    │ SPARSE          │ │ SPARSE          │ │ DENSE           │\n",
    "    │ Interpretabile  │ │ Pesato per      │ │ Semanticamente  │\n",
    "    │ Conta frequenze │ │ importanza      │ │ ricco           │\n",
    "    └─────────────────┘ └─────────────────┘ └─────────────────┘\n",
    "```\n",
    "\n",
    "## Checklist Preprocessing\n",
    "\n",
    "```\n",
    "□ Lowercase? \n",
    "  → Sì per la maggior parte dei task\n",
    "  → No se case è informativo (NER, formal vs informal)\n",
    "\n",
    "□ Rimuovere punteggiatura?\n",
    "  → Sì solitamente\n",
    "  → No se emoji/emoticon sono informative (sentiment)\n",
    "\n",
    "□ Rimuovere stopwords?\n",
    "  → Sì per topic modeling, similarity\n",
    "  → No per sentiment (\"non\" è cruciale), sequenze\n",
    "\n",
    "□ Stemming/Lemmatization?\n",
    "  → Stemming: veloce, aggressivo, può perdere info\n",
    "  → Lemmatization: più accurato, più lento\n",
    "  → Nessuno: se le forme flesse sono informative\n",
    "```\n",
    "\n",
    "## Parametri Chiave CountVectorizer\n",
    "\n",
    "| Parametro | Significato | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `max_features` | Limita vocabolario alle top N parole | None (tutte) |\n",
    "| `ngram_range` | Range di n-grammi (min, max) | (1,1) unigram |\n",
    "| `min_df` | Ignora termini in meno di X documenti | 1 |\n",
    "| `max_df` | Ignora termini in più del X% documenti | 1.0 |\n",
    "| `binary` | 0/1 invece di conteggi | False |\n",
    "| `stop_words` | Lista di stopwords da rimuovere | None |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436c8e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Notebook Dimostrativo\n",
    "\n",
    "## Rappresentazione del Testo in Pratica\n",
    "\n",
    "Dimostriamo step-by-step come trasformare testo in vettori numerici utilizzabili per ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP: Importazione librerie per NLP\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np                          # Array numerici\n",
    "import pandas as pd                         # DataFrames\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Bag of Words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDF (preview)\n",
    "import re                                   # Regular expressions per pulizia\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Seed per riproducibilità\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Librerie importate correttamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATASET: Corpus di esempio (recensioni prodotto)\n",
    "# ============================================================\n",
    "\n",
    "# Creiamo un piccolo corpus di recensioni per dimostrazione\n",
    "corpus = [\n",
    "    \"Ottimo prodotto, qualità eccellente!\",\n",
    "    \"Prodotto scadente, non lo consiglio\",\n",
    "    \"Qualità buona, prezzo giusto\",\n",
    "    \"Non funziona, prodotto difettoso\",\n",
    "    \"Consiglio questo prodotto, ottimo acquisto\",\n",
    "    \"Prezzo alto ma qualità ottima\",\n",
    "    \"Prodotto arrivato rotto, pessima esperienza\",\n",
    "    \"Acquisto consigliato, spedizione veloce\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CORPUS DI ESEMPIO\")\n",
    "print(\"=\" * 60)\n",
    "for i, doc in enumerate(corpus, 1):\n",
    "    print(f\"Doc {i}: \\\"{doc}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1: PREPROCESSING MANUALE\n",
    "# ============================================================\n",
    "# Dimostriamo ogni step del preprocessing\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Applica preprocessing di base a un testo.\n",
    "    \n",
    "    Steps:\n",
    "    1. Lowercase\n",
    "    2. Rimozione punteggiatura\n",
    "    3. Rimozione numeri\n",
    "    4. Rimozione spazi multipli\n",
    "    \"\"\"\n",
    "    # Step 1: Tutto minuscolo\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 2: Rimuovi punteggiatura (sostituisci con spazio)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Step 3: Rimuovi numeri\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Step 4: Rimuovi spazi multipli\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPROCESSING MANUALE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nTrasformazione testo grezzo → testo pulito:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, doc in enumerate(corpus[:3], 1):\n",
    "    cleaned = preprocess_text(doc)\n",
    "    print(f\"\\nDoc {i}:\")\n",
    "    print(f\"  Originale: \\\"{doc}\\\"\")\n",
    "    print(f\"  Pulito:    \\\"{cleaned}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166bcb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2: TOKENIZZAZIONE\n",
    "# ============================================================\n",
    "# Dividiamo il testo in token (parole)\n",
    "\n",
    "def tokenize(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenizza un testo in lista di parole.\n",
    "    Versione semplice: split su spazi.\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOKENIZZAZIONE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nDivisione in token:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, doc in enumerate(corpus[:3], 1):\n",
    "    cleaned = preprocess_text(doc)\n",
    "    tokens = tokenize(cleaned)\n",
    "    print(f\"\\nDoc {i}:\")\n",
    "    print(f\"  Testo pulito: \\\"{cleaned}\\\"\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  N. tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63122db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: BAG OF WORDS con CountVectorizer\n",
    "# ============================================================\n",
    "# sklearn gestisce preprocessing + tokenizzazione + BoW\n",
    "\n",
    "# Creiamo il vectorizer con configurazione base\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit: costruisce il vocabolario\n",
    "# Transform: converte i documenti in vettori\n",
    "X_bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BAG OF WORDS - CountVectorizer\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Vocabolario appreso\n",
    "print(\"\\n1. VOCABOLARIO APPRESO\")\n",
    "print(\"-\" * 40)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(f\"Dimensione vocabolario: {len(vocab)} parole\")\n",
    "print(f\"Parole: {list(vocab)}\")\n",
    "\n",
    "# Matrice documento-termine\n",
    "print(\"\\n2. MATRICE DOCUMENTO-TERMINE\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Shape: {X_bow.shape}\")\n",
    "print(f\"→ {X_bow.shape[0]} documenti x {X_bow.shape[1]} features (parole)\")\n",
    "print(f\"Tipo: {type(X_bow)} (matrice sparsa)\")\n",
    "\n",
    "# Convertiamo in DataFrame per visualizzazione\n",
    "df_bow = pd.DataFrame(\n",
    "    X_bow.toarray(),                    # .toarray() converte sparse → dense\n",
    "    columns=vocab,\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(corpus))]\n",
    ")\n",
    "\n",
    "print(\"\\n3. VISUALIZZAZIONE (prime 5 colonne)\")\n",
    "print(\"-\" * 40)\n",
    "print(df_bow.iloc[:, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cfe27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANALISI: Sparsità della matrice\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ANALISI DELLA SPARSITÀ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calcolo sparsità\n",
    "n_total = X_bow.shape[0] * X_bow.shape[1]  # Elementi totali\n",
    "n_nonzero = X_bow.nnz                       # Elementi non-zero\n",
    "sparsity = 1 - (n_nonzero / n_total)\n",
    "\n",
    "print(f\"\\nElementi totali nella matrice: {n_total}\")\n",
    "print(f\"Elementi non-zero: {n_nonzero}\")\n",
    "print(f\"Elementi zero: {n_total - n_nonzero}\")\n",
    "print(f\"Sparsità: {sparsity:.1%}\")\n",
    "\n",
    "print(f\"\\n→ Solo il {100-sparsity*100:.1f}% della matrice contiene informazione utile\")\n",
    "print(\"→ Con vocabolari grandi (10k+ parole), la sparsità supera il 99%\")\n",
    "\n",
    "# Visualizziamo la matrice come heatmap testuale\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PATTERN DELLA MATRICE (0 = assente, ≥1 = presente)\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "# Mostriamo presenza/assenza\n",
    "for idx, row in df_bow.iterrows():\n",
    "    pattern = \"\".join([\"█\" if v > 0 else \"░\" for v in row])\n",
    "    n_words = (row > 0).sum()\n",
    "    print(f\"{idx}: {pattern}  ({n_words} parole uniche)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ff0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# N-GRAMMI: Catturare sequenze di parole\n",
    "# ============================================================\n",
    "\n",
    "# Vectorizer con bigrammi\n",
    "vec_bigram = CountVectorizer(ngram_range=(1, 2))  # Unigram + bigram\n",
    "X_bigram = vec_bigram.fit_transform(corpus)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"N-GRAMMI: UNIGRAM + BIGRAM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "vocab_bigram = vec_bigram.get_feature_names_out()\n",
    "\n",
    "print(f\"\\nDimensione vocabolario (solo unigram): {len(vocab)} parole\")\n",
    "print(f\"Dimensione vocabolario (uni+bigram): {len(vocab_bigram)} features\")\n",
    "print(f\"→ Aumento: +{len(vocab_bigram) - len(vocab)} features ({(len(vocab_bigram)/len(vocab)-1)*100:.0f}% in più)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"ESEMPI DI BIGRAMMI ESTRATTI:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Filtriamo solo i bigrammi (contengono spazio)\n",
    "bigrams = [f for f in vocab_bigram if ' ' in f]\n",
    "print(f\"\\nBigrammi trovati ({len(bigrams)}):\")\n",
    "for bg in bigrams[:15]:\n",
    "    print(f\"  • \\\"{bg}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PARAMETRI UTILI DI CountVectorizer\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PARAMETRI CountVectorizer: ESEMPI PRATICI\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. max_features: limita vocabolario\n",
    "print(\"\\n1. max_features=10 (solo le 10 parole più frequenti)\")\n",
    "print(\"-\" * 50)\n",
    "vec_limited = CountVectorizer(max_features=10)\n",
    "X_limited = vec_limited.fit_transform(corpus)\n",
    "print(f\"Vocabolario: {list(vec_limited.get_feature_names_out())}\")\n",
    "\n",
    "# 2. min_df: ignora parole rare\n",
    "print(\"\\n2. min_df=2 (parole in almeno 2 documenti)\")\n",
    "print(\"-\" * 50)\n",
    "vec_mindf = CountVectorizer(min_df=2)\n",
    "X_mindf = vec_mindf.fit_transform(corpus)\n",
    "print(f\"Vocabolario ridotto: {list(vec_mindf.get_feature_names_out())}\")\n",
    "print(f\"Parole rimosse: {len(vocab) - len(vec_mindf.get_feature_names_out())}\")\n",
    "\n",
    "# 3. max_df: ignora parole troppo comuni\n",
    "print(\"\\n3. max_df=0.5 (parole in max 50% documenti)\")\n",
    "print(\"-\" * 50)\n",
    "vec_maxdf = CountVectorizer(max_df=0.5)\n",
    "X_maxdf = vec_maxdf.fit_transform(corpus)\n",
    "print(f\"Vocabolario: {list(vec_maxdf.get_feature_names_out())}\")\n",
    "\n",
    "# 4. binary: presenza/assenza invece di conteggi\n",
    "print(\"\\n4. binary=True (0/1 invece di conteggi)\")\n",
    "print(\"-\" * 50)\n",
    "vec_binary = CountVectorizer(binary=True)\n",
    "X_binary = vec_binary.fit_transform(corpus)\n",
    "# Esempio con documento che ripete una parola\n",
    "doc_repeat = [\"buono buono buono ottimo\"]\n",
    "X_count = CountVectorizer().fit_transform(doc_repeat)\n",
    "X_bin = CountVectorizer(binary=True).fit_transform(doc_repeat)\n",
    "print(f\"Documento: \\\"{doc_repeat[0]}\\\"\")\n",
    "print(f\"Con conteggi: {X_count.toarray()[0]}\")\n",
    "print(f\"Con binary:   {X_bin.toarray()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9014acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRASFORMAZIONE DI NUOVI DOCUMENTI\n",
    "# ============================================================\n",
    "# Importante: usare .transform() (non fit_transform) per nuovi dati\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRASFORMARE NUOVI DOCUMENTI\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Vectorizer già addestrato (su corpus)\n",
    "print(\"\\nVectorizer addestrato sul corpus originale\")\n",
    "print(f\"Vocabolario: {list(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# Nuovo documento da trasformare\n",
    "nuovo_doc = [\"Prodotto eccellente, prezzo ottimo\"]\n",
    "print(f\"\\nNuovo documento: \\\"{nuovo_doc[0]}\\\"\")\n",
    "\n",
    "# CORRETTO: usare transform (non fit_transform)\n",
    "X_nuovo = vectorizer.transform(nuovo_doc)\n",
    "\n",
    "print(f\"\\nVettore risultante:\")\n",
    "print(f\"  Shape: {X_nuovo.shape}\")\n",
    "print(f\"  Valori: {X_nuovo.toarray()[0]}\")\n",
    "\n",
    "# Quali parole sono state riconosciute?\n",
    "parole_riconosciute = []\n",
    "for i, count in enumerate(X_nuovo.toarray()[0]):\n",
    "    if count > 0:\n",
    "        parole_riconosciute.append(vectorizer.get_feature_names_out()[i])\n",
    "\n",
    "print(f\"\\nParole riconosciute dal vocabolario: {parole_riconosciute}\")\n",
    "\n",
    "# Parole nuove (out-of-vocabulary) vengono ignorate\n",
    "print(\"\\n⚠️  NOTA: 'eccellente' non era nel vocabolario → IGNORATA\")\n",
    "print(\"    Questo è un limite del BoW: parole nuove scompaiono\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da6256",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Esercizi Svolti\n",
    "\n",
    "## Esercizio 1: Preprocessing Pipeline\n",
    "\n",
    "**Problema**: Implementa una pipeline completa di preprocessing che:\n",
    "1. Converta in lowercase\n",
    "2. Rimuova punteggiatura e numeri\n",
    "3. Tokenizzi\n",
    "4. Rimuova le stopwords italiane\n",
    "5. Restituisca i token puliti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0537368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESERCIZIO 1 - SOLUZIONE\n",
    "# Pipeline completa di preprocessing testuale\n",
    "# ============================================================\n",
    "\n",
    "# Stopwords italiane comuni\n",
    "STOPWORDS_IT = {\n",
    "    'il', 'lo', 'la', 'i', 'gli', 'le', 'un', 'uno', 'una',\n",
    "    'di', 'a', 'da', 'in', 'con', 'su', 'per', 'tra', 'fra',\n",
    "    'e', 'o', 'ma', 'che', 'non', 'è', 'sono', 'ha', 'ho',\n",
    "    'questo', 'questa', 'questi', 'queste', 'quello', 'quella',\n",
    "    'come', 'quando', 'dove', 'perché', 'chi', 'cosa',\n",
    "    'più', 'molto', 'anche', 'solo', 'già', 'ancora', 'sempre'\n",
    "}\n",
    "\n",
    "def full_preprocessing_pipeline(text: str, remove_stopwords: bool = True) -> list:\n",
    "    \"\"\"\n",
    "    Pipeline completa di preprocessing testuale.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Testo da preprocessare\n",
    "    remove_stopwords : bool\n",
    "        Se True, rimuove le stopwords italiane\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : Lista di token puliti\n",
    "    \n",
    "    Steps:\n",
    "    1. Lowercase\n",
    "    2. Rimozione punteggiatura e numeri\n",
    "    3. Tokenizzazione\n",
    "    4. Rimozione stopwords (opzionale)\n",
    "    5. Rimozione token vuoti/corti\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 2: Rimozione punteggiatura e numeri\n",
    "    # Manteniamo solo lettere e spazi\n",
    "    text = re.sub(r'[^a-zàèéìòù\\s]', ' ', text)\n",
    "    \n",
    "    # Step 3: Tokenizzazione\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Step 4: Rimozione stopwords (se richiesto)\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in STOPWORDS_IT]\n",
    "    \n",
    "    # Step 5: Rimozione token troppo corti (1 carattere)\n",
    "    tokens = [t for t in tokens if len(t) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# ── TEST DELLA PIPELINE ──\n",
    "print(\"=\" * 70)\n",
    "print(\"ESERCIZIO 1: PREPROCESSING PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "testi_test = [\n",
    "    \"Il prodotto è OTTIMO! Costa solo 29.99€ ma vale molto di più.\",\n",
    "    \"Non mi piace questo servizio... 3 stelle su 5.\",\n",
    "    \"Consegna veloce, il pacco è arrivato in 2 giorni!!!\"\n",
    "]\n",
    "\n",
    "for i, testo in enumerate(testi_test, 1):\n",
    "    print(f\"\\n{'─' * 70}\")\n",
    "    print(f\"TESTO {i}\")\n",
    "    print(f\"{'─' * 70}\")\n",
    "    print(f\"Originale: \\\"{testo}\\\"\")\n",
    "    \n",
    "    tokens_con_sw = full_preprocessing_pipeline(testo, remove_stopwords=False)\n",
    "    tokens_senza_sw = full_preprocessing_pipeline(testo, remove_stopwords=True)\n",
    "    \n",
    "    print(f\"\\nCon stopwords:    {tokens_con_sw}\")\n",
    "    print(f\"Senza stopwords:  {tokens_senza_sw}\")\n",
    "    print(f\"Stopwords rimosse: {set(tokens_con_sw) - set(tokens_senza_sw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819070ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Esercizio 2: Costruzione Manuale BoW\n",
    "\n",
    "**Problema**: Costruisci \"a mano\" (senza CountVectorizer) la matrice Bag of Words per un piccolo corpus, per capire esattamente cosa fa CountVectorizer internamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a06e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESERCIZIO 2 - SOLUZIONE\n",
    "# Costruzione manuale della matrice Bag of Words\n",
    "# ============================================================\n",
    "\n",
    "def manual_bag_of_words(corpus: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Costruisce manualmente la matrice BoW.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    corpus : list\n",
    "        Lista di documenti (stringhe)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (matrice BoW come numpy array, vocabolario come lista)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Preprocessa e tokenizza ogni documento\n",
    "    tokenized_docs = []\n",
    "    for doc in corpus:\n",
    "        # Semplice preprocessing\n",
    "        doc_lower = doc.lower()\n",
    "        doc_clean = re.sub(r'[^\\w\\s]', '', doc_lower)\n",
    "        tokens = doc_clean.split()\n",
    "        tokenized_docs.append(tokens)\n",
    "    \n",
    "    # Step 2: Costruisci vocabolario (insieme di tutte le parole uniche)\n",
    "    all_words = set()\n",
    "    for tokens in tokenized_docs:\n",
    "        all_words.update(tokens)\n",
    "    \n",
    "    # Step 3: Ordina alfabeticamente (come fa CountVectorizer)\n",
    "    vocabulary = sorted(list(all_words))\n",
    "    \n",
    "    # Step 4: Crea mapping parola → indice\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    \n",
    "    # Step 5: Costruisci matrice\n",
    "    n_docs = len(corpus)\n",
    "    n_words = len(vocabulary)\n",
    "    bow_matrix = np.zeros((n_docs, n_words), dtype=int)\n",
    "    \n",
    "    for doc_idx, tokens in enumerate(tokenized_docs):\n",
    "        for token in tokens:\n",
    "            word_idx = word_to_idx[token]\n",
    "            bow_matrix[doc_idx, word_idx] += 1\n",
    "    \n",
    "    return bow_matrix, vocabulary\n",
    "\n",
    "# ── TEST E CONFRONTO ──\n",
    "print(\"=\" * 70)\n",
    "print(\"ESERCIZIO 2: COSTRUZIONE MANUALE BoW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Corpus semplice per test\n",
    "mini_corpus = [\n",
    "    \"il gatto mangia il pesce\",\n",
    "    \"il cane gioca\",\n",
    "    \"il gatto gioca con il cane\"\n",
    "]\n",
    "\n",
    "print(\"\\nCORPUS:\")\n",
    "for i, doc in enumerate(mini_corpus, 1):\n",
    "    print(f\"  Doc {i}: \\\"{doc}\\\"\")\n",
    "\n",
    "# Costruzione manuale\n",
    "bow_manual, vocab_manual = manual_bag_of_words(mini_corpus)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"RISULTATO MANUALE:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\nVocabolario: {vocab_manual}\")\n",
    "print(f\"\\nMatrice BoW:\")\n",
    "df_manual = pd.DataFrame(bow_manual, columns=vocab_manual, \n",
    "                         index=[f\"Doc {i}\" for i in range(1, 4)])\n",
    "print(df_manual)\n",
    "\n",
    "# Confronto con CountVectorizer\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"CONFRONTO CON CountVectorizer:\")\n",
    "print(\"-\" * 70)\n",
    "cv = CountVectorizer()\n",
    "bow_sklearn = cv.fit_transform(mini_corpus)\n",
    "vocab_sklearn = list(cv.get_feature_names_out())\n",
    "\n",
    "print(f\"\\nVocabolario sklearn: {vocab_sklearn}\")\n",
    "print(f\"\\nMatrici identiche? {np.array_equal(bow_manual, bow_sklearn.toarray())}\")\n",
    "print(\"✓ La nostra implementazione produce lo stesso risultato di sklearn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a602f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Esercizio 3: Classificazione con BoW\n",
    "\n",
    "**Problema**: Usa la rappresentazione BoW per classificare recensioni come positive o negative (mini sentiment analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESERCIZIO 3 - SOLUZIONE\n",
    "# Classificazione di sentiment con BoW\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ESERCIZIO 3: CLASSIFICAZIONE SENTIMENT CON BoW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ── STEP 1: Dataset di recensioni etichettate ──\n",
    "recensioni = [\n",
    "    # Positive (1)\n",
    "    (\"Prodotto eccezionale, lo consiglio a tutti\", 1),\n",
    "    (\"Ottimo acquisto, sono molto soddisfatto\", 1),\n",
    "    (\"Qualità superiore, prezzo giusto\", 1),\n",
    "    (\"Spedizione veloce, prodotto perfetto\", 1),\n",
    "    (\"Fantastico, supera le aspettative\", 1),\n",
    "    (\"Molto buono, lo ricomprerei\", 1),\n",
    "    (\"Ottima qualità, consigliato\", 1),\n",
    "    (\"Prodotto top, eccellente\", 1),\n",
    "    (\"Bellissimo prodotto, funziona benissimo\", 1),\n",
    "    (\"Soddisfatto dell'acquisto, ottimo\", 1),\n",
    "    \n",
    "    # Negative (0)\n",
    "    (\"Prodotto scadente, non funziona\", 0),\n",
    "    (\"Pessima qualità, soldi sprecati\", 0),\n",
    "    (\"Non lo consiglio, deludente\", 0),\n",
    "    (\"Arrivato rotto, pessimo servizio\", 0),\n",
    "    (\"Qualità scarsa, non vale il prezzo\", 0),\n",
    "    (\"Terribile, da evitare\", 0),\n",
    "    (\"Prodotto difettoso, reso immediato\", 0),\n",
    "    (\"Brutta esperienza, non comprate\", 0),\n",
    "    (\"Deluso totalmente, prodotto inutile\", 0),\n",
    "    (\"Pessimo acquisto, sconsigliato\", 0)\n",
    "]\n",
    "\n",
    "# Separiamo testi e labels\n",
    "texts = [r[0] for r in recensioni]\n",
    "labels = [r[1] for r in recensioni]\n",
    "\n",
    "print(f\"\\nDataset: {len(texts)} recensioni\")\n",
    "print(f\"  Positive: {sum(labels)}\")\n",
    "print(f\"  Negative: {len(labels) - sum(labels)}\")\n",
    "\n",
    "# ── STEP 2: Vettorizzazione ──\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"STEP 2: VETTORIZZAZIONE BOW\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "vectorizer_sent = CountVectorizer(max_features=100)\n",
    "X = vectorizer_sent.fit_transform(texts)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"Matrice features: {X.shape}\")\n",
    "print(f\"Vocabolario: {len(vectorizer_sent.get_feature_names_out())} parole\")\n",
    "\n",
    "# ── STEP 3: Split train/test ──\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {X_train.shape[0]} campioni\")\n",
    "print(f\"Test: {X_test.shape[0]} campioni\")\n",
    "\n",
    "# ── STEP 4: Training classificatore ──\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"STEP 4: TRAINING NAIVE BAYES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Naive Bayes è particolarmente adatto per text classification\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ── STEP 5: Valutazione ──\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nAccuracy sul test set: {accuracy:.2%}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negativo', 'Positivo']))\n",
    "\n",
    "# ── STEP 6: Test su nuove recensioni ──\n",
    "print(\"-\" * 70)\n",
    "print(\"TEST SU NUOVE RECENSIONI:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "nuove_recensioni = [\n",
    "    \"Prodotto buono, mi piace molto\",\n",
    "    \"Terribile esperienza, non comprate mai\",\n",
    "    \"Niente di speciale, nella media\"\n",
    "]\n",
    "\n",
    "for rec in nuove_recensioni:\n",
    "    X_new = vectorizer_sent.transform([rec])\n",
    "    pred = clf.predict(X_new)[0]\n",
    "    prob = clf.predict_proba(X_new)[0]\n",
    "    \n",
    "    sentiment = \"POSITIVO\" if pred == 1 else \"NEGATIVO\"\n",
    "    conf = max(prob) * 100\n",
    "    \n",
    "    print(f\"\\n\\\"{rec}\\\"\")\n",
    "    print(f\"  → Predizione: {sentiment} (confidence: {conf:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0be0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Conclusione Operativa\n",
    "\n",
    "## Cosa Abbiamo Imparato\n",
    "\n",
    "| Concetto | Implicazione Pratica |\n",
    "|----------|---------------------|\n",
    "| **Testo → Numeri** | Gli algoritmi ML richiedono input numerici; il testo va vettorizzato |\n",
    "| **Preprocessing** | Pulizia del testo (lowercase, punteggiatura, stopwords) migliora la qualità |\n",
    "| **Bag of Words** | Rappresenta documenti come conteggi di parole, ignora l'ordine |\n",
    "| **N-grammi** | Catturano sequenze di parole, aumentano dimensionalità |\n",
    "| **Sparse vs Dense** | BoW produce vettori sparsi; embeddings producono vettori densi |\n",
    "| **CountVectorizer** | Strumento sklearn per BoW, con molti parametri utili |\n",
    "\n",
    "## Workflow Operativo\n",
    "\n",
    "```\n",
    "1. ESPLORAZIONE\n",
    "   └── Analizza un campione di testi (lunghezza, lingua, rumore)\n",
    "\n",
    "2. PREPROCESSING\n",
    "   ├── Decidi: lowercase? stopwords? stemming?\n",
    "   └── Implementa pipeline coerente train/test\n",
    "\n",
    "3. VETTORIZZAZIONE\n",
    "   ├── Scegli metodo (BoW, TF-IDF, embeddings)\n",
    "   ├── Configura parametri (max_features, ngram_range)\n",
    "   └── fit_transform su train, transform su test\n",
    "\n",
    "4. MODELLAZIONE\n",
    "   └── Usa la matrice risultante come input per classificatori\n",
    "```\n",
    "\n",
    "## Errori Comuni da Evitare\n",
    "\n",
    "1. ❌ Usare `fit_transform` anche sui dati di test (data leakage)\n",
    "2. ❌ Ignorare parole out-of-vocabulary senza rendersene conto\n",
    "3. ❌ Applicare preprocessing diverso tra train e inference\n",
    "4. ❌ Non considerare la sparsità della matrice risultante\n",
    "5. ❌ Rimuovere stopwords quando sono informative (es. sentiment)\n",
    "\n",
    "## Prossimi Passi\n",
    "\n",
    "- **Lezione 31**: TF-IDF — Pesare le parole per importanza\n",
    "- **Lezione 32**: Sentiment Analysis — Applicazione pratica completa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e05363",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Bignami — Scheda di Riferimento Rapido\n",
    "\n",
    "## Pipeline di Preprocessing\n",
    "\n",
    "| Step | Operazione | Esempio |\n",
    "|------|------------|---------|\n",
    "| 1 | Lowercase | \"CIAO\" → \"ciao\" |\n",
    "| 2 | Rimuovi punteggiatura | \"ciao!\" → \"ciao\" |\n",
    "| 3 | Tokenizza | \"il gatto\" → [\"il\", \"gatto\"] |\n",
    "| 4 | Rimuovi stopwords | [\"il\", \"gatto\"] → [\"gatto\"] |\n",
    "| 5 | Stemming/Lemma | [\"gatti\"] → [\"gatt\"] |\n",
    "\n",
    "## Rappresentazioni Testuali\n",
    "\n",
    "| Metodo | Tipo | Dimensionalità | Pro | Contro |\n",
    "|--------|------|----------------|-----|--------|\n",
    "| **BoW** | Sparse | Alta (~vocabolario) | Semplice, interpretabile | Perde ordine |\n",
    "| **TF-IDF** | Sparse | Alta | Pesa importanza | Perde ordine |\n",
    "| **Word2Vec** | Dense | Bassa (100-300) | Semantica | Richiede training |\n",
    "| **BERT** | Dense | Media (768) | Contestuale | Computazionalmente pesante |\n",
    "\n",
    "## Codice Essenziale\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Configurazione tipica\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=10000,    # Limita vocabolario\n",
    "    ngram_range=(1, 2),    # Unigram + bigram\n",
    "    min_df=2,              # Ignora parole rare\n",
    "    max_df=0.95,           # Ignora parole troppo comuni\n",
    "    stop_words=None        # O lista custom\n",
    ")\n",
    "\n",
    "# Training: costruisce vocabolario\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "# Inference: usa vocabolario esistente\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "# Ispeziona vocabolario\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "```\n",
    "\n",
    "## Checklist Pre-Vettorizzazione\n",
    "\n",
    "```\n",
    "□ Il preprocessing è coerente tra train e test?\n",
    "□ Ho considerato se le stopwords sono informative?\n",
    "□ Ho scelto max_features appropriato per la RAM disponibile?\n",
    "□ Ho verificato la sparsità risultante?\n",
    "□ Uso transform() (non fit_transform) sui nuovi dati?\n",
    "```\n",
    "\n",
    "---\n",
    "*Fine Lezione 30 — Rappresentare il Testo come Dato*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
