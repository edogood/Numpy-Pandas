{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b38b19",
   "metadata": {},
   "source": [
    "# Lezione 37 — Introduzione al Deep Learning (concettuale)\n",
    "\n",
    "**Obiettivi della lezione**\n",
    "- Comprendere il neurone artificiale e il meccanismo forward/backward pass.\n",
    "- Confrontare deep learning con machine learning classico per un data analyst.\n",
    "- Identificare quando il deep learning è (e non è) la scelta giusta.\n",
    "- Evidenziare limiti pratici per chi fa analisi dati (costi, dati, interpretabilità)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee192a3a",
   "metadata": {},
   "source": [
    "## Teoria concettuale approfondita\n",
    "\n",
    "**Neurone artificiale (percettrone moderno)**\n",
    "- Input $x$, pesi $w$, bias $b$. Output: $y = \\sigma(w \\cdot x + b)$ con funzione di attivazione non lineare $\\sigma$.\n",
    "- Funzione di attivazione tipiche: sigmoid (0-1, rischio saturazione), ReLU (0, max(0,z), semplice e stabile), tanh (-1,1).\n",
    "\n",
    "**Forward pass**\n",
    "- Calcolo diretto dell'output dato l'input: passa attraverso strati densi o convoluzionali; ogni strato applica trasformazioni affini + non linearità.\n",
    "\n",
    "**Backward pass (backpropagation)**\n",
    "- Calcolo dei gradienti della loss rispetto ai pesi usando la regola della catena.\n",
    "- Aggiornamento pesi con ottimizzatore (es. SGD): $w \\leftarrow w - \\eta \\cdot \\nabla_w \\mathcal{L}$.\n",
    "\n",
    "**Differenze con ML classico (per un data analyst)**\n",
    "- **Feature engineering**: nel deep learning è in gran parte automatizzata; nel ML classico è cruciale e manuale.\n",
    "- **Dati richiesti**: deep learning richiede molti esempi e dati etichettati; ML classico può funzionare con meno dati.\n",
    "- **Interpretabilità**: modelli profondi sono spesso black-box; modelli lineari/alberi sono più leggibili.\n",
    "- **Costi computazionali**: reti profonde richiedono GPU/TPU; ML classico è più leggero.\n",
    "\n",
    "**Perché NON è sempre la soluzione giusta**\n",
    "- Scarso volume di dati o etichette rumorose.\n",
    "- Vincoli di spiegabilità (audit, compliance).\n",
    "- Vincoli di latenza o budget computazionale limitato.\n",
    "\n",
    "**Limiti pratici per un data analyst**\n",
    "- Curva di apprendimento degli strumenti (framework, tuning).\n",
    "- Necessità di pipeline dati stabili, logging sperimentale, monitoraggio drift.\n",
    "- Rischio di overfitting senza adeguata regolarizzazione e validazione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec72644",
   "metadata": {},
   "source": [
    "## Schema mentale / mappa logica\n",
    "- **Quando usare**: immagini, audio, testo lungo, pattern complessi dove feature manuali sono difficili; grandi volumi di dati; disponibilità di GPU e tempo.\n",
    "- **Quando NON usare**: dataset piccolo, alta richiesta di interpretabilità, problemi tabellari semplici risolvibili con alberi/boosting, forti vincoli di latenza/budget.\n",
    "- **Segnali pratici nei dati**: alta dimensionalità grezza (pixel, token), segnali non lineari, pattern locali ripetuti (immagini), dipendenze sequenziali (testo/tempo).\n",
    "- **Pattern operativo**: partire da modello semplice (baseline), valutare se la complessità del problema giustifica una rete; monitorare overfitting; mantenere baseline classica per confronto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6373f",
   "metadata": {},
   "source": [
    "## Notebook dimostrativo (concettuale, senza training complesso)\n",
    "Mostriamo un percettrone multistrato minimale su un dataset toy 2D per classificazione lineare vs non lineare. Useremo solo NumPy per evidenziare forward e backward pass semplificati (1 hidden layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiamo NumPy per il calcolo numerico\n",
    "import numpy as np\n",
    "np.random.seed(42)  # fissiamo il seed per riproducibilità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f9a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo un piccolo dataset 2D non lineare (due moon) per mostrare il ruolo della non linearità\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be87c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializziamo un MLP minimale: input -> hidden (ReLU) -> output (sigmoid)\n",
    "input_dim = 2\n",
    "hidden_dim = 8\n",
    "output_dim = 1\n",
    "\n",
    "# Pesiamo piccoli valori random per evitare saturazione iniziale\n",
    "W1 = 0.1 * np.random.randn(input_dim, hidden_dim)\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = 0.1 * np.random.randn(hidden_dim, output_dim)\n",
    "b2 = np.zeros((1, output_dim))\n",
    "\n",
    "# Funzioni di attivazione e derivate\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc346811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo forward pass: calcolo delle attivazioni intermedie e output\n",
    "\n",
    "def forward(X_batch):\n",
    "    z1 = X_batch @ W1 + b1           # combinazione lineare primo strato\n",
    "    a1 = relu(z1)                    # non linearità\n",
    "    z2 = a1 @ W2 + b2                # logits finali\n",
    "    a2 = sigmoid(z2)                 # probabilità classe 1\n",
    "    cache = {\"X\": X_batch, \"z1\": z1, \"a1\": a1, \"z2\": z2, \"a2\": a2}\n",
    "    return a2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb151cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo la loss binaria (log loss) e la backward pass per aggiornare i pesi\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    eps = 1e-8  # evitiamo log(0)\n",
    "    return -np.mean(y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps))\n",
    "\n",
    "\n",
    "def backward(cache, y_true, y_pred, lr=0.1):\n",
    "    global W1, b1, W2, b2\n",
    "    m = y_true.shape[0]\n",
    "\n",
    "    # Gradienti strato output\n",
    "    dz2 = y_pred - y_true.reshape(-1, 1)         # derivata della log-loss wrt z2\n",
    "    dW2 = cache[\"a1\"].T @ dz2 / m\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Gradienti strato hidden\n",
    "    da1 = dz2 @ W2.T\n",
    "    dz1 = da1 * relu_derivative(cache[\"z1\"])\n",
    "    dW1 = cache[\"X\"].T @ dz1 / m\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Aggiornamento pesi\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf57436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eseguiamo un training brevissimo (poche epoche) solo per mostrare la dinamica\n",
    "\n",
    "y = y.astype(float)\n",
    "loss_history = []\n",
    "for epoch in range(50):  # numero piccolo per dimostrazione\n",
    "    y_pred, cache = forward(X)\n",
    "    loss = compute_loss(y, y_pred)\n",
    "    loss_history.append(loss)\n",
    "    backward(cache, y, y_pred, lr=0.5)\n",
    "\n",
    "loss_history[:5], loss_history[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fae2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcoliamo l'accuratezza finale per capire se la rete ha imparato qualcosa\n",
    "\n",
    "y_pred_label = (y_pred.flatten() > 0.5).astype(int)\n",
    "accuracy = np.mean(y_pred_label == y)\n",
    "loss_history[-1], accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8557261",
   "metadata": {},
   "source": [
    "### Osservazioni sul risultato\n",
    "- Con poche epoche e un MLP minimo la loss scende e l'accuratezza cresce, mostrando la capacità di apprendere pattern non lineari.\n",
    "- Non abbiamo fatto tuning serio: il focus è capire forward/backward e non ottenere SOTA.\n",
    "- Limiti: nessuna validazione separata, rischio overfitting, nessuna regolarizzazione; serve baseline classica per confronto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba266e",
   "metadata": {},
   "source": [
    "## Esercizi svolti (step-by-step)\n",
    "Gli esercizi servono a collegare concetti e pratica minimale, senza entrare in training complesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040cb060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 1: variazione ampiezza hidden layer e osservazione della loss\n",
    "\n",
    "for hidden_dim in [4, 8, 16]:\n",
    "    # reinizializziamo pesi per confronto equo\n",
    "    W1 = 0.1 * np.random.randn(input_dim, hidden_dim)\n",
    "    b1 = np.zeros((1, hidden_dim))\n",
    "    W2 = 0.1 * np.random.randn(hidden_dim, output_dim)\n",
    "    b2 = np.zeros((1, output_dim))\n",
    "\n",
    "    loss_history = []\n",
    "    for epoch in range(40):\n",
    "        y_pred, cache = forward(X)\n",
    "        loss = compute_loss(y, y_pred)\n",
    "        loss_history.append(loss)\n",
    "        backward(cache, y, y_pred, lr=0.4)\n",
    "\n",
    "    print(f\"hidden={hidden_dim}, loss finale={loss_history[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25754e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 2: proviamo una funzione di attivazione diversa (tanh) e confrontiamo\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.tanh(z) ** 2\n",
    "\n",
    "# Riutilizziamo lo stesso schema ma sostituendo ReLU con tanh\n",
    "W1 = 0.1 * np.random.randn(input_dim, hidden_dim)\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = 0.1 * np.random.randn(hidden_dim, output_dim)\n",
    "b2 = np.zeros((1, output_dim))\n",
    "\n",
    "\n",
    "def forward_tanh(X_batch):\n",
    "    z1 = X_batch @ W1 + b1\n",
    "    a1 = tanh(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    cache = {\"X\": X_batch, \"z1\": z1, \"a1\": a1, \"z2\": z2, \"a2\": a2}\n",
    "    return a2, cache\n",
    "\n",
    "\n",
    "def backward_tanh(cache, y_true, y_pred, lr=0.1):\n",
    "    global W1, b1, W2, b2\n",
    "    m = y_true.shape[0]\n",
    "    dz2 = y_pred - y_true.reshape(-1, 1)\n",
    "    dW2 = cache[\"a1\"].T @ dz2 / m\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "    da1 = dz2 @ W2.T\n",
    "    dz1 = da1 * tanh_derivative(cache[\"z1\"])\n",
    "    dW1 = cache[\"X\"].T @ dz1 / m\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "# Training breve con tanh\n",
    "loss_history_tanh = []\n",
    "for epoch in range(40):\n",
    "    y_pred, cache = forward_tanh(X)\n",
    "    loss = compute_loss(y, y_pred)\n",
    "    loss_history_tanh.append(loss)\n",
    "    backward_tanh(cache, y, y_pred, lr=0.3)\n",
    "\n",
    "loss_history_tanh[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df558f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 3: confronto con baseline lineare (senza hidden) per vedere il limite della linearità\n",
    "\n",
    "# Modello lineare: logistic regression manuale (1 strato)\n",
    "W_lin = 0.1 * np.random.randn(input_dim, 1)\n",
    "b_lin = np.zeros((1, 1))\n",
    "\n",
    "\n",
    "def forward_linear(X_batch):\n",
    "    z = X_batch @ W_lin + b_lin\n",
    "    a = sigmoid(z)\n",
    "    return a, {\"X\": X_batch, \"z\": z, \"a\": a}\n",
    "\n",
    "\n",
    "def backward_linear(cache, y_true, y_pred, lr=0.1):\n",
    "    global W_lin, b_lin\n",
    "    m = y_true.shape[0]\n",
    "    dz = y_pred - y_true.reshape(-1, 1)\n",
    "    dW = cache[\"X\"].T @ dz / m\n",
    "    db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "    W_lin -= lr * dW\n",
    "    b_lin -= lr * db\n",
    "\n",
    "# Training breve per baseline\n",
    "loss_lin_hist = []\n",
    "for epoch in range(40):\n",
    "    y_pred_lin, cache_lin = forward_linear(X)\n",
    "    loss_lin = compute_loss(y, y_pred_lin)\n",
    "    loss_lin_hist.append(loss_lin)\n",
    "    backward_linear(cache_lin, y, y_pred_lin, lr=0.3)\n",
    "\n",
    "loss_lin_hist[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0aeff0",
   "metadata": {},
   "source": [
    "## Conclusione operativa\n",
    "- Portarsi a casa: il deep learning automatizza parte del feature learning ma richiede dati, calcolo e cura ingegneristica. Il forward/backward pass sono il cuore dell’ottimizzazione.\n",
    "- Errori da evitare: scegliere reti complesse senza baseline, ignorare overfitting/validazione, sottovalutare interpretabilità e costi.\n",
    "- Ponte verso la prossima lezione: introdurremo i modelli generativi e la differenza tra predizione e generazione (LLM come modelli probabilistici)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
