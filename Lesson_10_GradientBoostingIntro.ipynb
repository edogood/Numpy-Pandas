{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da94b54e",
   "metadata": {},
   "source": [
    "# LEZIONE 10 - Gradient Boosting: Ensemble Sequenziale\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivi della Lezione\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "1. Comprendere l'intuizione del boosting e come differisce dal bagging\n",
    "2. Distinguere Random Forest da Gradient Boosting in termini di strategia\n",
    "3. Analizzare il trade-off bias-variance nei modelli ad alberi\n",
    "4. Riconoscere l'overfitting nel boosting e come mitigarlo\n",
    "5. Padroneggiare i parametri chiave: n_estimators, learning_rate, max_depth\n",
    "6. Confrontare empiricamente Random Forest vs Gradient Boosting\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisiti\n",
    "\n",
    "- Lezione 9: Decision Tree e Random Forest\n",
    "- Lezione 8: Validazione, Overfitting e Generalizzazione\n",
    "- Concetti di bias e varianza\n",
    "- numpy, pandas, matplotlib, scikit-learn\n",
    "\n",
    "---\n",
    "\n",
    "## Indice\n",
    "\n",
    "1. SEZIONE 1 - Teoria\n",
    "2. SEZIONE 2 - Mappa Mentale\n",
    "3. SEZIONE 3 - Quaderno Dimostrativo\n",
    "4. SEZIONE 4 - Metodi e Funzioni\n",
    "5. SEZIONE 5 - Glossario\n",
    "6. SEZIONE 6 - Errori Comuni\n",
    "7. SEZIONE 7 - Conclusione\n",
    "8. SEZIONE 8 - Checklist\n",
    "9. SEZIONE 9 - Changelog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab019a37",
   "metadata": {},
   "source": [
    "# SEZIONE 1 - Teoria\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Da Bagging a Boosting: Un Cambio di Paradigma\n",
    "\n",
    "Nella Lezione 9 abbiamo visto il Random Forest, un metodo ensemble basato sul bagging:\n",
    "- Costruisce alberi indipendenti su campioni bootstrap\n",
    "- Combina le predizioni per ridurre la varianza\n",
    "- Gli alberi non \"parlano\" tra loro\n",
    "\n",
    "Il Boosting adotta una strategia radicalmente diversa:\n",
    "- Costruisce alberi sequenzialmente (uno dopo l'altro)\n",
    "- Ogni nuovo albero corregge gli errori del precedente\n",
    "- Si concentra sui campioni \"difficili\" che i modelli precedenti hanno sbagliato\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 L'Analogia dello Studente\n",
    "\n",
    "| Bagging (Random Forest) | Boosting (Gradient Boosting) |\n",
    "|------------------------|------------------------------|\n",
    "| Studia tutto il programma piu volte | Studia, fa un test, poi studia SOLO gli argomenti sbagliati |\n",
    "| Approccio \"a tappeto\" | Approccio \"mirato agli errori\" |\n",
    "| Ogni sessione e indipendente | Ogni sessione dipende dalla precedente |\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Il Meccanismo Sequenziale\n",
    "\n",
    "Il Gradient Boosting costruisce il modello finale come somma di modelli deboli:\n",
    "\n",
    "$$F_M(x) = F_0(x) + \\sum_{m=1}^{M} \\eta \\cdot h_m(x)$$\n",
    "\n",
    "Dove:\n",
    "- $F_0(x)$ = predizione iniziale (spesso la media del target)\n",
    "- $h_m(x)$ = m-esimo albero \"debole\" (weak learner)\n",
    "- $\\eta$ = learning rate (quanto \"fidarsi\" di ogni nuovo albero)\n",
    "- $M$ = numero totale di alberi (n_estimators)\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4 Algoritmo Passo per Passo\n",
    "\n",
    "```\n",
    "Inizializzazione:\n",
    "    F0(x) = media(y)  # predizione costante iniziale\n",
    "\n",
    "Per m = 1, 2, ..., M:\n",
    "    1. Calcola i RESIDUI: ri = yi - Fm-1(xi)\n",
    "       (quanto il modello attuale sbaglia per ogni campione)\n",
    "    \n",
    "    2. Addestra un NUOVO ALBERO hm(x) sui residui\n",
    "       (l'albero impara a predire GLI ERRORI)\n",
    "    \n",
    "    3. Aggiorna il modello: Fm(x) = Fm-1(x) + eta * hm(x)\n",
    "       (aggiungi la correzione, scalata dal learning rate)\n",
    "\n",
    "Predizione finale: F_M(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1.5 Perche \"Gradient\"?\n",
    "\n",
    "Il termine Gradient deriva dal fatto che i residui che minimizziamo corrispondono al gradiente negativo della funzione di perdita:\n",
    "\n",
    "- Per regressione (MSE): residui = y - F(x)\n",
    "- Per classificazione: si usano pseudo-residui derivati dalla log-loss\n",
    "\n",
    "In pratica, il boosting esegue una discesa del gradiente nello spazio delle funzioni.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6 Random Forest vs Gradient Boosting\n",
    "\n",
    "| Aspetto | Random Forest | Gradient Boosting |\n",
    "|---------|---------------|-------------------|\n",
    "| Strategia | Parallela (bagging) | Sequenziale (boosting) |\n",
    "| Obiettivo | Ridurre la varianza | Ridurre il bias |\n",
    "| Alberi | Profondi e indipendenti | Poco profondi e correlati |\n",
    "| Errori | Mediati via voting | Corretti iterativamente |\n",
    "| Velocita training | Parallelizzabile | Intrinsecamente sequenziale |\n",
    "| Rischio overfitting | Basso | Piu alto (richiede tuning) |\n",
    "\n",
    "---\n",
    "\n",
    "## 1.7 Quando Usare Quale?\n",
    "\n",
    "**Random Forest e preferibile quando:**\n",
    "- Hai poco tempo per il tuning\n",
    "- Vuoi un modello robusto out-of-the-box\n",
    "- I dati hanno molto rumore\n",
    "- Hai bisogno di parallelizzazione\n",
    "\n",
    "**Gradient Boosting e preferibile quando:**\n",
    "- Vuoi massimizzare le performance\n",
    "- Sei disposto a fare hyperparameter tuning\n",
    "- I dati hanno pattern complessi ma strutturati\n",
    "- Hai risorse per evitare l'overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## 1.8 Varianti Moderne del Gradient Boosting\n",
    "\n",
    "| Libreria | Caratteristiche |\n",
    "|----------|-----------------|\n",
    "| XGBoost | Regularizzazione L1/L2, gestione missing values, parallelismo |\n",
    "| LightGBM | Leaf-wise growth, molto veloce su grandi dataset |\n",
    "| CatBoost | Gestione nativa delle feature categoriche |\n",
    "\n",
    "---\n",
    "\n",
    "## 1.9 Bias vs Variance nei Modelli ad Alberi\n",
    "\n",
    "$$\\text{Errore Totale} = \\text{Bias}^2 + \\text{Varianza} + \\text{Rumore irriducibile}$$\n",
    "\n",
    "| Componente | Significato | Causa |\n",
    "|------------|-------------|-------|\n",
    "| Bias | Errore sistematico | Modello troppo semplice |\n",
    "| Varianza | Sensibilita ai dati di training | Modello troppo complesso |\n",
    "| Rumore | Errore inevitabile nei dati | Intrinseco al problema |\n",
    "\n",
    "---\n",
    "\n",
    "## 1.10 Come gli Ensemble Riducono l'Errore\n",
    "\n",
    "**Random Forest riduce la VARIANZA:**\n",
    "- Usa alberi profondi (basso bias, alta varianza)\n",
    "- La media di B alberi instabili produce predizione stabile\n",
    "- Il bias rimane circa lo stesso\n",
    "\n",
    "**Gradient Boosting riduce il BIAS:**\n",
    "- Usa alberi poco profondi (alto bias, bassa varianza)\n",
    "- Ogni albero corregge gli errori, bias diminuisce progressivamente\n",
    "- Attenzione: troppi alberi aumentano la varianza (overfitting)\n",
    "\n",
    "---\n",
    "\n",
    "## 1.11 Perche il Boosting e Piu Soggetto a Overfitting\n",
    "\n",
    "1. Correzione iterativa degli errori: ogni nuovo albero si focalizza sugli errori, incluso il rumore\n",
    "2. Alberi correlati: a differenza di RF, gli alberi non sono indipendenti\n",
    "3. Nessuna media: si sommano le predizioni, non si fa averaging\n",
    "\n",
    "---\n",
    "\n",
    "## 1.12 Segnali di Overfitting nel Boosting\n",
    "\n",
    "| Segnale | Cosa indica |\n",
    "|---------|-------------|\n",
    "| Train accuracy verso 100% | Il modello memorizza i dati |\n",
    "| Gap train-test che cresce | Generalizzazione che peggiora |\n",
    "| Test accuracy che decresce dopo un picco | Troppi alberi |\n",
    "| Learning curve che non migliora | Modello saturato |\n",
    "\n",
    "---\n",
    "\n",
    "## 1.13 Strategie Anti-Overfitting\n",
    "\n",
    "1. Ridurre n_estimators: meno alberi = meno correzioni = meno rischio di fittare il rumore\n",
    "2. Abbassare learning_rate: correzioni piu piccole = convergenza piu lenta ma stabile\n",
    "3. Limitare max_depth: alberi piu \"deboli\" (depth 2-5) prevengono l'overfitting\n",
    "4. Usare subsample < 1.0: campionamento casuale aggiunge regolarizzazione\n",
    "5. Early Stopping: fermare il training quando la validation score smette di migliorare\n",
    "\n",
    "---\n",
    "\n",
    "## 1.14 La Regola d'Oro\n",
    "\n",
    "```\n",
    "n_estimators UP + learning_rate DOWN = Piu stabile, ma piu lento\n",
    "learning_rate x n_estimators ~ costante\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1.15 I Tre Parametri Fondamentali\n",
    "\n",
    "**n_estimators - Numero di Alberi:**\n",
    "| Valore | Effetto |\n",
    "|--------|---------|\n",
    "| Troppo basso (< 50) | Underfitting, bias alto |\n",
    "| Ottimale (50-300) | Buon trade-off |\n",
    "| Troppo alto (> 500) | Rischio overfitting, tempo lungo |\n",
    "\n",
    "**learning_rate - Tasso di Apprendimento:**\n",
    "| Valore | Effetto |\n",
    "|--------|---------|\n",
    "| Alto (0.1 - 1.0) | Convergenza veloce, rischio overfit |\n",
    "| Basso (0.01 - 0.1) | Convergenza lenta, piu stabile |\n",
    "| Molto basso (< 0.01) | Richiede molti alberi |\n",
    "\n",
    "**max_depth - Profondita degli Alberi:**\n",
    "| Valore | Effetto |\n",
    "|--------|---------|\n",
    "| 1 (decision stump) | Alberi molto deboli |\n",
    "| 2-4 | Raccomandato per boosting |\n",
    "| 5-8 | Per problemi complessi, rischio overfit |\n",
    "\n",
    "---\n",
    "\n",
    "## 1.16 Altri Parametri Utili\n",
    "\n",
    "| Parametro | Descrizione | Valori tipici |\n",
    "|-----------|-------------|---------------|\n",
    "| subsample | Frazione di campioni per albero | 0.5 - 1.0 |\n",
    "| min_samples_split | Campioni minimi per split | 2 - 20 |\n",
    "| min_samples_leaf | Campioni minimi nelle foglie | 1 - 10 |\n",
    "| max_features | Features considerate per split | sqrt, log2, 0.5 |\n",
    "\n",
    "---\n",
    "\n",
    "## 1.17 Configurazioni Tipiche\n",
    "\n",
    "**Stabilita Massima:**\n",
    "```python\n",
    "learning_rate = 0.01\n",
    "n_estimators = 1000\n",
    "max_depth = 3\n",
    "subsample = 0.8\n",
    "```\n",
    "\n",
    "**Velocita Massima:**\n",
    "```python\n",
    "learning_rate = 0.3\n",
    "n_estimators = 50\n",
    "max_depth = 5\n",
    "subsample = 1.0\n",
    "```\n",
    "\n",
    "**Bilanciato:**\n",
    "```python\n",
    "learning_rate = 0.1\n",
    "n_estimators = 100-200\n",
    "max_depth = 3-4\n",
    "subsample = 0.8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456cff3",
   "metadata": {},
   "source": [
    "# SEZIONE 2 - Mappa Mentale\n",
    "\n",
    "---\n",
    "\n",
    "## Flusso Decisionale: Quando Usare RF vs GB\n",
    "\n",
    "```\n",
    "PROBLEMA DI CLASSIFICAZIONE/REGRESSIONE\n",
    "            |\n",
    "            v\n",
    "    [Hai tempo per tuning?]\n",
    "            |\n",
    "    +-------+-------+\n",
    "    |               |\n",
    "   NO              SI\n",
    "    |               |\n",
    "    v               v\n",
    "Random Forest   [Dati molto rumorosi?]\n",
    "(robusto,           |\n",
    "out-of-box)    +----+----+\n",
    "               |         |\n",
    "              SI        NO\n",
    "               |         |\n",
    "               v         v\n",
    "          Random     Gradient\n",
    "          Forest     Boosting\n",
    "          (robusto)  (max performance)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Diagnosi Overfitting nel Boosting\n",
    "\n",
    "```\n",
    "Train Accuracy ~ 100%?\n",
    "        |\n",
    "    +---+---+\n",
    "    |       |\n",
    "   NO      SI\n",
    "    |       |\n",
    "    v       v\n",
    "   OK    [Gap train-test > 0.1?]\n",
    "              |\n",
    "          +---+---+\n",
    "          |       |\n",
    "         NO      SI\n",
    "          |       |\n",
    "          v       v\n",
    "         OK    OVERFITTING!\n",
    "                  |\n",
    "                  v\n",
    "          [Soluzioni]\n",
    "          - Riduci n_estimators\n",
    "          - Abbassa learning_rate\n",
    "          - Limita max_depth\n",
    "          - Usa subsample < 1.0\n",
    "          - Early stopping\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Relazione tra Parametri\n",
    "\n",
    "```\n",
    "learning_rate x n_estimators ~ COSTANTE\n",
    "\n",
    "Se learning_rate SCENDE:\n",
    "    -> n_estimators deve SALIRE\n",
    "    -> Training piu LENTO ma piu STABILE\n",
    "\n",
    "Se learning_rate SALE:\n",
    "    -> n_estimators puo SCENDERE\n",
    "    -> Training piu VELOCE ma piu RISCHIOSO\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## RF vs GB: Strategia Ensemble\n",
    "\n",
    "```\n",
    "RANDOM FOREST                    GRADIENT BOOSTING\n",
    "      |                                |\n",
    "      v                                v\n",
    "[Albero 1] [Albero 2] ... [Albero N]   [Albero 1]\n",
    "      |         |              |            |\n",
    "      +---------+--------------+            v\n",
    "                |                       [Albero 2] (corregge errori 1)\n",
    "                v                            |\n",
    "         VOTO MAGGIORANZA                    v\n",
    "         (parallelo)                    [Albero 3] (corregge errori 2)\n",
    "                                             |\n",
    "                                             v\n",
    "                                        [Albero N]\n",
    "                                             |\n",
    "                                             v\n",
    "                                       SOMMA PESATA\n",
    "                                       (sequenziale)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa38336",
   "metadata": {},
   "source": [
    "# SEZIONE 3 - Quaderno Dimostrativo\n",
    "\n",
    "In questa sezione applichiamo i concetti appresi attraverso esercizi pratici.\n",
    "Ogni esercizio include:\n",
    "- Obiettivo chiaro\n",
    "- Spiegazione del \"perche\" di ogni passaggio\n",
    "- Micro-checkpoint con assert per verificare la correttezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ESERCIZIO 1: Visualizzazione del Boosting Sequenziale ===\n",
    "# Perche: visualizziamo come il Gradient Boosting costruisce il modello\n",
    "#         passo dopo passo, correggendo i residui ad ogni iterazione\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Creiamo un dataset semplice per regressione\n",
    "# Perche: un dataset 1D permette di visualizzare chiaramente l'approssimazione\n",
    "X_demo = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_true = np.sin(X_demo.ravel()) + 0.5 * np.cos(2 * X_demo.ravel())\n",
    "y_demo = y_true + np.random.randn(100) * 0.2\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert X_demo.shape == (100, 1), \"X_demo deve avere shape (100, 1)\"\n",
    "assert len(y_demo) == 100, \"y_demo deve avere 100 elementi\"\n",
    "print(\"Micro-checkpoint 1: Dataset creato correttamente\")\n",
    "\n",
    "# Simuliamo il boosting manualmente\n",
    "# Perche: capire il meccanismo interno aiuta a scegliere i parametri giusti\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Step 0: Predizione iniziale (media)\n",
    "F0 = np.full_like(y_demo, y_demo.mean())\n",
    "residuals = y_demo - F0\n",
    "\n",
    "# Parametri del boosting\n",
    "learning_rate = 0.5\n",
    "predictions = [F0.copy()]\n",
    "trees = []\n",
    "\n",
    "for step in range(5):\n",
    "    # Addestra albero sui residui\n",
    "    tree = DecisionTreeRegressor(max_depth=2)\n",
    "    tree.fit(X_demo, residuals)\n",
    "    trees.append(tree)\n",
    "    \n",
    "    # Predizione dell'albero\n",
    "    h = tree.predict(X_demo)\n",
    "    \n",
    "    # Aggiorna predizione\n",
    "    F_new = predictions[-1] + learning_rate * h\n",
    "    predictions.append(F_new)\n",
    "    \n",
    "    # Nuovi residui\n",
    "    residuals = y_demo - F_new\n",
    "\n",
    "# Visualizzazione\n",
    "titles = ['Step 0: F0 = media', 'Step 1: +h1', 'Step 2: +h2', \n",
    "          'Step 3: +h3', 'Step 4: +h4', 'Step 5: +h5']\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.scatter(X_demo, y_demo, alpha=0.4, s=20, label='Dati reali', c='gray')\n",
    "    ax.plot(X_demo, y_true, 'g--', linewidth=2, label='Funzione vera', alpha=0.7)\n",
    "    ax.plot(X_demo, predictions[i], 'r-', linewidth=2, label=f'F_{i}(x)')\n",
    "    ax.set_title(titles[i], fontsize=11)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.set_ylim([-2, 2.5])\n",
    "\n",
    "plt.suptitle('Gradient Boosting: Correzione Sequenziale degli Errori\\n(learning_rate=0.5, max_depth=2)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert len(predictions) == 6, \"Devono esserci 6 predizioni (F0 a F5)\"\n",
    "assert len(trees) == 5, \"Devono esserci 5 alberi\"\n",
    "print(\"Micro-checkpoint 2: Boosting simulato correttamente\")\n",
    "print(\"\\nOSSERVAZIONE:\")\n",
    "print(\"- Ogni step aggiunge una correzione che riduce l'errore\")\n",
    "print(\"- Gli alberi 'deboli' (depth=2) contribuiscono poco singolarmente\")\n",
    "print(\"- L'ensemble finale approssima bene la funzione vera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc183a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ESERCIZIO 2: Confronto Bias-Variance in RF vs GB ===\n",
    "# Perche: confrontiamo come RF e GB si comportano all'aumentare\n",
    "#         del numero di alberi, evidenziando le differenze nel trade-off\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dataset con rumore moderato\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=15, n_informative=8, \n",
    "    n_redundant=2, flip_y=0.1, random_state=42\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert X_train.shape[0] == 750, \"Training set deve avere 750 campioni\"\n",
    "assert X_test.shape[0] == 250, \"Test set deve avere 250 campioni\"\n",
    "print(\"Micro-checkpoint 1: Dataset preparato correttamente\")\n",
    "\n",
    "# Test al variare di n_estimators\n",
    "n_estimators_range = [1, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "rf_train_scores = []\n",
    "rf_test_scores = []\n",
    "gb_train_scores = []\n",
    "gb_test_scores = []\n",
    "\n",
    "print(\"Analisi Bias-Variance: RF vs GB al variare di n_estimators\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=10, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_train_scores.append(rf.score(X_train, y_train))\n",
    "    rf_test_scores.append(rf.score(X_test, y_test))\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "    gb_train_scores.append(gb.score(X_train, y_train))\n",
    "    gb_test_scores.append(gb.score(X_test, y_test))\n",
    "\n",
    "# Visualizzazione\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Random Forest\n",
    "ax1 = axes[0]\n",
    "ax1.plot(n_estimators_range, rf_train_scores, 'b-o', label='Train', linewidth=2, markersize=8)\n",
    "ax1.plot(n_estimators_range, rf_test_scores, 'r-s', label='Test', linewidth=2, markersize=8)\n",
    "ax1.fill_between(n_estimators_range, rf_train_scores, rf_test_scores, alpha=0.2, color='purple')\n",
    "ax1.set_xlabel('n_estimators')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Random Forest: Stabilita con piu alberi', fontsize=11, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.set_ylim([0.7, 1.02])\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient Boosting\n",
    "ax2 = axes[1]\n",
    "ax2.plot(n_estimators_range, gb_train_scores, 'b-o', label='Train', linewidth=2, markersize=8)\n",
    "ax2.plot(n_estimators_range, gb_test_scores, 'r-s', label='Test', linewidth=2, markersize=8)\n",
    "ax2.fill_between(n_estimators_range, gb_train_scores, gb_test_scores, alpha=0.2, color='purple')\n",
    "ax2.set_xlabel('n_estimators')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Gradient Boosting: Miglioramento poi rischio overfit', fontsize=11, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.set_ylim([0.7, 1.02])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert len(rf_train_scores) == len(n_estimators_range), \"Devono esserci score per ogni n_estimators\"\n",
    "assert all(0 <= s <= 1 for s in rf_test_scores), \"Gli score devono essere tra 0 e 1\"\n",
    "print(\"\\nMicro-checkpoint 2: Analisi completata\")\n",
    "print(\"INTERPRETAZIONE:\")\n",
    "print(\"- RF: il gap train-test rimane STABILE con piu alberi\")\n",
    "print(\"- GB: il gap train-test puo AUMENTARE con troppi alberi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06903661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ESERCIZIO 3: Overfitting nel Gradient Boosting ===\n",
    "# Perche: visualizziamo il fenomeno dell'overfitting nel boosting\n",
    "#         su un dataset rumoroso per capire quando fermarsi\n",
    "\n",
    "# Dataset MOLTO rumoroso per evidenziare l'overfitting\n",
    "X_noisy, y_noisy = make_classification(\n",
    "    n_samples=500, n_features=10, n_informative=4, \n",
    "    n_redundant=2, flip_y=0.25,  # 25% rumore!\n",
    "    random_state=42\n",
    ")\n",
    "X_train_n, X_test_n, y_train_n, y_test_n = train_test_split(\n",
    "    X_noisy, y_noisy, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert X_noisy.shape == (500, 10), \"Dataset deve avere 500 campioni e 10 features\"\n",
    "print(\"Micro-checkpoint 1: Dataset rumoroso creato\")\n",
    "\n",
    "# Test con diversi n_estimators\n",
    "estimators_range = range(5, 305, 10)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n_est in estimators_range:\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=n_est, \n",
    "        max_depth=4,  # alberi abbastanza profondi\n",
    "        learning_rate=0.15,  # learning rate medio-alto\n",
    "        random_state=42\n",
    "    )\n",
    "    gb.fit(X_train_n, y_train_n)\n",
    "    train_scores.append(gb.score(X_train_n, y_train_n))\n",
    "    test_scores.append(gb.score(X_test_n, y_test_n))\n",
    "\n",
    "# Trova il punto ottimale\n",
    "best_idx = np.argmax(test_scores)\n",
    "best_n_est = list(estimators_range)[best_idx]\n",
    "best_test_score = test_scores[best_idx]\n",
    "\n",
    "# Visualizzazione\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Curve di apprendimento\n",
    "ax1 = axes[0]\n",
    "ax1.plot(estimators_range, train_scores, 'b-', label='Train Accuracy', linewidth=2)\n",
    "ax1.plot(estimators_range, test_scores, 'r-', label='Test Accuracy', linewidth=2)\n",
    "ax1.axvline(x=best_n_est, color='green', linestyle='--', linewidth=2, label=f'Ottimo: {best_n_est}')\n",
    "ax1.fill_between(estimators_range, train_scores, test_scores, alpha=0.2, color='purple', label='Gap (overfit)')\n",
    "ax1.scatter([best_n_est], [best_test_score], s=100, c='green', zorder=5)\n",
    "ax1.set_xlabel('n_estimators')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Gradient Boosting: Overfitting su Dati Rumorosi\\n(flip_y=0.25, learning_rate=0.15)', fontsize=11)\n",
    "ax1.legend(loc='right')\n",
    "ax1.set_ylim([0.6, 1.02])\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Gap come indicatore\n",
    "gaps = [tr - te for tr, te in zip(train_scores, test_scores)]\n",
    "colors = ['green' if g < 0.1 else 'orange' if g < 0.2 else 'red' for g in gaps]\n",
    "ax2 = axes[1]\n",
    "ax2.bar(estimators_range, gaps, color=colors, width=8, alpha=0.7)\n",
    "ax2.axhline(y=0.1, color='orange', linestyle='--', label='Soglia attenzione')\n",
    "ax2.axhline(y=0.2, color='red', linestyle='--', label='Soglia overfitting')\n",
    "ax2.set_xlabel('n_estimators')\n",
    "ax2.set_ylabel('Train - Test Gap')\n",
    "ax2.set_title('Gap di Generalizzazione', fontsize=11)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert best_n_est > 0, \"Deve esistere un numero ottimale di alberi\"\n",
    "assert 0 < best_test_score < 1, \"Test accuracy deve essere valida\"\n",
    "print(f\"\\nMicro-checkpoint 2: Analisi overfitting completata\")\n",
    "print(f\"RISULTATI:\")\n",
    "print(f\"- Numero ottimale di alberi: {best_n_est}\")\n",
    "print(f\"- Test accuracy massima: {best_test_score:.4f}\")\n",
    "print(f\"- Oltre {best_n_est} alberi: il modello inizia a memorizzare il rumore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0feeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ESERCIZIO 4: Effetto del Learning Rate ===\n",
    "# Perche: il learning rate e uno dei parametri piu importanti del GB\n",
    "#         e dobbiamo capire come influenza le performance\n",
    "\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.3, 0.5]\n",
    "n_est_fixed = 150\n",
    "\n",
    "results_lr = {}\n",
    "for lr in learning_rates:\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=n_est_fixed,\n",
    "        learning_rate=lr,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb.fit(X_train, y_train)\n",
    "    train_acc = gb.score(X_train, y_train)\n",
    "    test_acc = gb.score(X_test, y_test)\n",
    "    results_lr[lr] = {'train': train_acc, 'test': test_acc, 'gap': train_acc - test_acc}\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert len(results_lr) == 5, \"Devono esserci risultati per 5 learning rate\"\n",
    "print(\"Micro-checkpoint 1: Esperimenti completati\")\n",
    "\n",
    "# Visualizzazione\n",
    "lrs = list(results_lr.keys())\n",
    "trains = [results_lr[lr]['train'] for lr in lrs]\n",
    "tests = [results_lr[lr]['test'] for lr in lrs]\n",
    "gaps = [results_lr[lr]['gap'] for lr in lrs]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Train vs Test per learning rate\n",
    "x = np.arange(len(lrs))\n",
    "width = 0.35\n",
    "\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(x - width/2, trains, width, label='Train', color='lightblue', edgecolor='navy')\n",
    "bars2 = ax1.bar(x + width/2, tests, width, label='Test', color='salmon', edgecolor='darkred')\n",
    "ax1.set_xlabel('learning_rate')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title(f'Effetto del Learning Rate\\n(n_estimators={n_est_fixed}, max_depth=3)', fontsize=11)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([str(lr) for lr in lrs])\n",
    "ax1.legend()\n",
    "ax1.set_ylim([0.75, 1.02])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Valori sopra le barre\n",
    "for bar, val in zip(bars2, tests):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: Gap\n",
    "colors_gap = ['green' if g < 0.05 else 'orange' if g < 0.1 else 'red' for g in gaps]\n",
    "ax2 = axes[1]\n",
    "ax2.bar(x, gaps, color=colors_gap, edgecolor='black')\n",
    "ax2.set_xlabel('learning_rate')\n",
    "ax2.set_ylabel('Train - Test Gap')\n",
    "ax2.set_title('Gap di Generalizzazione per Learning Rate', fontsize=11)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([str(lr) for lr in lrs])\n",
    "ax2.axhline(y=0.05, color='orange', linestyle='--', alpha=0.7)\n",
    "ax2.axhline(y=0.1, color='red', linestyle='--', alpha=0.7)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "best_lr = max(results_lr.keys(), key=lambda x: results_lr[x]['test'])\n",
    "assert 0 < best_lr <= 1, \"Learning rate ottimale deve essere valido\"\n",
    "print(f\"\\nMicro-checkpoint 2: Analisi learning rate completata\")\n",
    "print(\"INTERPRETAZIONE:\")\n",
    "print(f\"- Learning rate ottimale: {best_lr}\")\n",
    "print(\"- lr BASSO (0.01): convergenza lenta, potrebbe servire piu alberi\")\n",
    "print(\"- lr ALTO (0.3-0.5): convergenza veloce ma gap maggiore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ESERCIZIO 5: Confronto Completo RF vs GB ===\n",
    "# Perche: eseguiamo un confronto sistematico con tutte le metriche\n",
    "#         per capire quando scegliere un modello rispetto all'altro\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Dataset piu grande e realistico\n",
    "X_full, y_full = make_classification(\n",
    "    n_samples=1200, n_features=20, n_informative=12,\n",
    "    n_redundant=4, n_clusters_per_class=2, flip_y=0.1, random_state=42\n",
    ")\n",
    "\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(\n",
    "    X_full, y_full, test_size=0.25, stratify=y_full, random_state=42\n",
    ")\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert X_train_f.shape[0] == 900, \"Training set deve avere 900 campioni\"\n",
    "print(\"Micro-checkpoint 1: Dataset preparato\")\n",
    "\n",
    "# Definiamo i modelli\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=150, max_depth=10, min_samples_split=5, random_state=42\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=150, learning_rate=0.1, max_depth=4, subsample=0.8, random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Confronto completo\n",
    "results_comparison = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFRONTO RANDOM FOREST vs GRADIENT BOOSTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    model.fit(X_train_f, y_train_f)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train_f)\n",
    "    y_test_pred = model.predict(X_test_f)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train_f, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test_f, y_test_pred)\n",
    "    precision = precision_score(y_test_f, y_test_pred)\n",
    "    recall = recall_score(y_test_f, y_test_pred)\n",
    "    f1 = f1_score(y_test_f, y_test_pred)\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_full, y_full, cv=5, scoring='accuracy')\n",
    "    \n",
    "    results_comparison[name] = {\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'gap': train_acc - test_acc\n",
    "    }\n",
    "    \n",
    "    print(f\"   Train Accuracy:    {train_acc:.4f}\")\n",
    "    print(f\"   Test Accuracy:     {test_acc:.4f}\")\n",
    "    print(f\"   Gap (overfit):     {train_acc - test_acc:.4f}\")\n",
    "    print(f\"   Precision:         {precision:.4f}\")\n",
    "    print(f\"   Recall:            {recall:.4f}\")\n",
    "    print(f\"   F1-Score:          {f1:.4f}\")\n",
    "    print(f\"   CV Accuracy:       {cv_scores.mean():.4f} +/- {cv_scores.std():.4f}\")\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert len(results_comparison) == 2, \"Devono esserci risultati per entrambi i modelli\"\n",
    "assert all(0 < r['test_acc'] < 1 for r in results_comparison.values()), \"Accuracy valide\"\n",
    "print(\"\\nMicro-checkpoint 2: Confronto completato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aeac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ESERCIZIO 6: Feature Importance e Visualizzazione Finale ===\n",
    "# Perche: confrontiamo come RF e GB assegnano importanza alle features\n",
    "#         e visualizziamo tutte le metriche del confronto\n",
    "\n",
    "# Estrai feature importance\n",
    "rf_model = models['Random Forest']\n",
    "gb_model = models['Gradient Boosting']\n",
    "\n",
    "rf_importance = rf_model.feature_importances_\n",
    "gb_importance = gb_model.feature_importances_\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert len(rf_importance) == 20, \"Devono esserci 20 feature importance\"\n",
    "assert np.isclose(rf_importance.sum(), 1.0), \"Feature importance deve sommare a 1\"\n",
    "print(\"Micro-checkpoint 1: Feature importance estratte\")\n",
    "\n",
    "# Ordina per importanza media\n",
    "avg_importance = (rf_importance + gb_importance) / 2\n",
    "sorted_idx = np.argsort(avg_importance)[::-1][:10]  # Top 10\n",
    "\n",
    "feature_names_demo = [f'Feature_{i}' for i in range(X_full.shape[1])]\n",
    "\n",
    "# Visualizzazione completa\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Accuracy (Train, Test, CV)\n",
    "model_names = list(results_comparison.keys())\n",
    "ax1 = axes[0, 0]\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.25\n",
    "\n",
    "train_accs = [results_comparison[m]['train_acc'] for m in model_names]\n",
    "test_accs = [results_comparison[m]['test_acc'] for m in model_names]\n",
    "cv_means = [results_comparison[m]['cv_mean'] for m in model_names]\n",
    "\n",
    "bars1 = ax1.bar(x - width, train_accs, width, label='Train', color='lightblue', edgecolor='navy')\n",
    "bars2 = ax1.bar(x, test_accs, width, label='Test', color='salmon', edgecolor='darkred')\n",
    "bars3 = ax1.bar(x + width, cv_means, width, label='CV (5-fold)', color='lightgreen', edgecolor='darkgreen')\n",
    "\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Confronto Accuracy', fontsize=11, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(model_names)\n",
    "ax1.legend()\n",
    "ax1.set_ylim([0.75, 1.02])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Precision, Recall, F1\n",
    "ax2 = axes[0, 1]\n",
    "metrics = ['precision', 'recall', 'f1']\n",
    "x2 = np.arange(len(metrics))\n",
    "width2 = 0.35\n",
    "\n",
    "rf_metrics = [results_comparison['Random Forest'][m] for m in metrics]\n",
    "gb_metrics = [results_comparison['Gradient Boosting'][m] for m in metrics]\n",
    "\n",
    "bars_rf = ax2.bar(x2 - width2/2, rf_metrics, width2, label='Random Forest', color='forestgreen', alpha=0.8)\n",
    "bars_gb = ax2.bar(x2 + width2/2, gb_metrics, width2, label='Gradient Boosting', color='darkred', alpha=0.8)\n",
    "\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Precision, Recall, F1', fontsize=11, fontweight='bold')\n",
    "ax2.set_xticks(x2)\n",
    "ax2.set_xticklabels(['Precision', 'Recall', 'F1-Score'])\n",
    "ax2.legend()\n",
    "ax2.set_ylim([0.75, 1.0])\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Gap (indicatore overfitting)\n",
    "ax3 = axes[1, 0]\n",
    "gaps_comp = [results_comparison[m]['gap'] for m in model_names]\n",
    "colors_gap = ['forestgreen', 'darkred']\n",
    "bars = ax3.bar(model_names, gaps_comp, color=colors_gap, alpha=0.7, edgecolor='black')\n",
    "ax3.axhline(y=0.05, color='orange', linestyle='--', label='Soglia attenzione')\n",
    "ax3.axhline(y=0.1, color='red', linestyle='--', label='Soglia overfitting')\n",
    "ax3.set_ylabel('Train - Test Gap')\n",
    "ax3.set_title('Gap di Generalizzazione', fontsize=11, fontweight='bold')\n",
    "ax3.legend(loc='upper right')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, g in zip(bars, gaps_comp):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, f'{g:.4f}', \n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 4: Feature Importance\n",
    "ax4 = axes[1, 1]\n",
    "x4 = np.arange(len(sorted_idx))\n",
    "width4 = 0.35\n",
    "\n",
    "bars1 = ax4.barh(x4 - width4/2, rf_importance[sorted_idx], width4, \n",
    "                label='Random Forest', color='forestgreen', alpha=0.8)\n",
    "bars2 = ax4.barh(x4 + width4/2, gb_importance[sorted_idx], width4, \n",
    "                label='Gradient Boosting', color='darkred', alpha=0.8)\n",
    "\n",
    "ax4.set_yticks(x4)\n",
    "ax4.set_yticklabels([feature_names_demo[i] for i in sorted_idx])\n",
    "ax4.set_xlabel('Importance')\n",
    "ax4.set_title('Top 10 Feature Importance', fontsize=11, fontweight='bold')\n",
    "ax4.legend(loc='lower right')\n",
    "ax4.invert_yaxis()\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "winner_acc = max(results_comparison.items(), key=lambda x: x[1]['test_acc'])\n",
    "print(f\"\\nMicro-checkpoint 2: Visualizzazione completata\")\n",
    "print(f\"\\nCONCLUSIONI DEL CONFRONTO:\")\n",
    "print(f\"- Migliore Test Accuracy: {winner_acc[0]} ({winner_acc[1]['test_acc']:.4f})\")\n",
    "print(f\"- Gap piu basso: {min(results_comparison.items(), key=lambda x: x[1]['gap'])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b83b7",
   "metadata": {},
   "source": [
    "# SEZIONE 4 - Metodi e Funzioni\n",
    "\n",
    "---\n",
    "\n",
    "## Classi Principali\n",
    "\n",
    "| Classe | Modulo | Descrizione |\n",
    "|--------|--------|-------------|\n",
    "| GradientBoostingClassifier | sklearn.ensemble | Classificatore Gradient Boosting |\n",
    "| GradientBoostingRegressor | sklearn.ensemble | Regressore Gradient Boosting |\n",
    "| RandomForestClassifier | sklearn.ensemble | Classificatore Random Forest (confronto) |\n",
    "| DecisionTreeRegressor | sklearn.tree | Albero singolo per simulazione |\n",
    "\n",
    "---\n",
    "\n",
    "## Parametri GradientBoostingClassifier\n",
    "\n",
    "| Parametro | Default | Descrizione |\n",
    "|-----------|---------|-------------|\n",
    "| n_estimators | 100 | Numero di alberi nell'ensemble |\n",
    "| learning_rate | 0.1 | Tasso di apprendimento |\n",
    "| max_depth | 3 | Profondita massima degli alberi |\n",
    "| min_samples_split | 2 | Campioni minimi per split |\n",
    "| min_samples_leaf | 1 | Campioni minimi nelle foglie |\n",
    "| subsample | 1.0 | Frazione di campioni per albero |\n",
    "| max_features | None | Features considerate per split |\n",
    "| random_state | None | Seed per riproducibilita |\n",
    "\n",
    "---\n",
    "\n",
    "## Metodi Principali\n",
    "\n",
    "| Metodo | Descrizione |\n",
    "|--------|-------------|\n",
    "| fit(X, y) | Addestra il modello sui dati |\n",
    "| predict(X) | Predice le classi per X |\n",
    "| predict_proba(X) | Predice le probabilita per X |\n",
    "| score(X, y) | Restituisce accuracy su X, y |\n",
    "| feature_importances_ | Importanza delle features (attributo) |\n",
    "\n",
    "---\n",
    "\n",
    "## Funzioni di Valutazione\n",
    "\n",
    "| Funzione | Modulo | Descrizione |\n",
    "|----------|--------|-------------|\n",
    "| cross_val_score | sklearn.model_selection | Cross-validation scores |\n",
    "| train_test_split | sklearn.model_selection | Split train/test |\n",
    "| accuracy_score | sklearn.metrics | Accuracy |\n",
    "| precision_score | sklearn.metrics | Precision |\n",
    "| recall_score | sklearn.metrics | Recall |\n",
    "| f1_score | sklearn.metrics | F1-Score |\n",
    "\n",
    "---\n",
    "\n",
    "## Pattern di Utilizzo\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "# Split dei dati\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Creare il modello\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Addestrare\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Valutare\n",
    "train_score = gb.score(X_train, y_train)\n",
    "test_score = gb.score(X_test, y_test)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(gb, X, y, cv=5)\n",
    "\n",
    "# Feature importance\n",
    "importance = gb.feature_importances_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcd25a",
   "metadata": {},
   "source": [
    "# SEZIONE 5 - Glossario\n",
    "\n",
    "---\n",
    "\n",
    "| Termine | Definizione |\n",
    "|---------|-------------|\n",
    "| Boosting | Tecnica ensemble che costruisce modelli sequenzialmente, correggendo errori |\n",
    "| Bagging | Tecnica ensemble che costruisce modelli in parallelo su campioni bootstrap |\n",
    "| Gradient Boosting | Boosting che minimizza una loss function tramite discesa del gradiente |\n",
    "| Weak Learner | Modello debole (es. albero poco profondo) usato nel boosting |\n",
    "| Learning Rate | Parametro che controlla quanto ogni albero contribuisce alla predizione |\n",
    "| n_estimators | Numero di alberi nell'ensemble |\n",
    "| Residui | Differenza tra valori veri e predetti, target per il prossimo albero |\n",
    "| Bias | Errore sistematico dovuto a modello troppo semplice |\n",
    "| Varianza | Sensibilita del modello ai dati di training |\n",
    "| Trade-off Bias-Variance | Compromesso tra underfitting (bias alto) e overfitting (varianza alta) |\n",
    "| Overfitting | Modello che memorizza il training set, generalizza male |\n",
    "| Underfitting | Modello troppo semplice, non cattura i pattern |\n",
    "| Early Stopping | Tecnica per fermare il training quando la validation peggiora |\n",
    "| Subsample | Frazione di campioni usata per addestrare ogni albero |\n",
    "| max_depth | Profondita massima di ciascun albero |\n",
    "| Feature Importance | Misura dell'importanza di ogni feature nel modello |\n",
    "| Cross-Validation | Tecnica per stimare la performance su dati non visti |\n",
    "| Gap Train-Test | Differenza tra accuracy di training e test (indicatore di overfitting) |\n",
    "| XGBoost | Implementazione ottimizzata di Gradient Boosting |\n",
    "| LightGBM | Gradient Boosting ottimizzato per grandi dataset |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9216d7d",
   "metadata": {},
   "source": [
    "# SEZIONE 6 - Errori Comuni\n",
    "\n",
    "---\n",
    "\n",
    "| Errore | Problema | Soluzione |\n",
    "|--------|----------|-----------|\n",
    "| learning_rate troppo alto | Overfitting rapido, instabilita | Abbassare a 0.05-0.1 |\n",
    "| learning_rate troppo basso | Convergenza lenta, underfitting | Aumentare n_estimators |\n",
    "| n_estimators troppo alto | Overfitting, tempo di training lungo | Usare early stopping o ridurre |\n",
    "| n_estimators troppo basso | Underfitting, modello debole | Aumentare gradualmente |\n",
    "| max_depth troppo alto | Alberi troppo complessi, overfitting | Limitare a 3-5 per GB |\n",
    "| max_depth troppo basso | Alberi troppo deboli, underfitting | Aumentare leggermente |\n",
    "| Non fare cross-validation | Stima inaffidabile delle performance | Usare sempre CV 5-fold |\n",
    "| Ignorare il gap train-test | Non rilevare overfitting | Monitorare sempre il gap |\n",
    "| Non normalizzare i dati | Prestazioni sub-ottimali | StandardScaler (meno critico per alberi) |\n",
    "| Usare GB su dati molto rumorosi | Overfitting sul rumore | Preferire Random Forest |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc9806",
   "metadata": {},
   "source": [
    "# SEZIONE 7 - Conclusione\n",
    "\n",
    "---\n",
    "\n",
    "## Cosa Abbiamo Imparato\n",
    "\n",
    "In questa lezione abbiamo esplorato il Gradient Boosting, un algoritmo potente che costruisce modelli sequenzialmente, correggendo gli errori dei modelli precedenti.\n",
    "\n",
    "---\n",
    "\n",
    "## Concetti Chiave\n",
    "\n",
    "| Concetto | Cosa ricordare |\n",
    "|----------|----------------|\n",
    "| Boosting | Costruzione sequenziale, ogni modello corregge gli errori |\n",
    "| RF vs GB | RF riduce varianza (parallelo), GB riduce bias (sequenziale) |\n",
    "| Overfitting | GB piu soggetto, richiede tuning attento |\n",
    "| Learning Rate | Piu basso = piu stabile ma piu lento |\n",
    "| n_estimators | Piu alto = potenzialmente meglio, ma rischio overfit |\n",
    "| max_depth | Per GB preferire alberi poco profondi (3-5) |\n",
    "\n",
    "---\n",
    "\n",
    "## Formula Chiave\n",
    "\n",
    "$$F_M(x) = F_0(x) + \\sum_{m=1}^{M} \\eta \\cdot h_m(x)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Regola d'Oro\n",
    "\n",
    "```\n",
    "n_estimators UP + learning_rate DOWN = Piu stabile\n",
    "learning_rate x n_estimators ~ costante (10-30)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quando Usare Cosa\n",
    "\n",
    "| Situazione | Modello Consigliato |\n",
    "|------------|---------------------|\n",
    "| Dati con rumore alto | Random Forest |\n",
    "| Massima performance | Gradient Boosting (con tuning) |\n",
    "| Poco tempo per tuning | Random Forest |\n",
    "| Dataset molto grande | RF o XGBoost/LightGBM |\n",
    "\n",
    "---\n",
    "\n",
    "## Prossimi Passi\n",
    "\n",
    "Nella prossima lezione affronteremo:\n",
    "- Pipeline: concatenare preprocessing + modello\n",
    "- Grid Search e Random Search: ricerca automatica degli iperparametri\n",
    "- Cross-Validation Stratificata: validazione robusta\n",
    "- Evitare il Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9becf5e1",
   "metadata": {},
   "source": [
    "# SEZIONE 8 - Checklist\n",
    "\n",
    "---\n",
    "\n",
    "## Prima di Usare Gradient Boosting\n",
    "\n",
    "- [ ] Ho compreso la differenza tra bagging e boosting\n",
    "- [ ] So che GB costruisce alberi sequenzialmente\n",
    "- [ ] Capisco la formula: F(x) = F0 + sum(eta * h_m(x))\n",
    "- [ ] So che il learning rate controlla la velocita di apprendimento\n",
    "\n",
    "---\n",
    "\n",
    "## Durante l'Addestramento\n",
    "\n",
    "- [ ] Ho scelto un learning_rate appropriato (0.05-0.2)\n",
    "- [ ] Ho limitato max_depth (3-5 per GB)\n",
    "- [ ] Sto monitorando il gap train-test\n",
    "- [ ] Uso cross-validation per la valutazione\n",
    "\n",
    "---\n",
    "\n",
    "## Diagnosi Overfitting\n",
    "\n",
    "- [ ] Il gap train-test e inferiore a 0.1\n",
    "- [ ] La test accuracy non decresce con piu alberi\n",
    "- [ ] Le curve di apprendimento sono stabili\n",
    "\n",
    "---\n",
    "\n",
    "## Confronto RF vs GB\n",
    "\n",
    "- [ ] Ho testato entrambi i modelli\n",
    "- [ ] Ho confrontato le metriche (accuracy, precision, recall, F1)\n",
    "- [ ] Ho considerato il tempo di training\n",
    "- [ ] Ho scelto in base al contesto (rumore, tempo, performance)\n",
    "\n",
    "---\n",
    "\n",
    "## Prima di Concludere\n",
    "\n",
    "- [ ] Ho estratto le feature importance\n",
    "- [ ] Ho documentato i parametri usati\n",
    "- [ ] Ho salvato il modello finale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e14c8",
   "metadata": {},
   "source": [
    "# SEZIONE 9 - Changelog\n",
    "\n",
    "---\n",
    "\n",
    "| Versione | Data | Modifiche |\n",
    "|----------|------|-----------|\n",
    "| 1.0 | 2024-01-15 | Creazione iniziale del notebook |\n",
    "| 2.0 | 2024-12-XX | Ristrutturazione completa secondo template a 9 sezioni |\n",
    "\n",
    "---\n",
    "\n",
    "## Note sulla Ristrutturazione\n",
    "\n",
    "- Aggiunta SEZIONE 2 - Mappa Mentale con flussi decisionali\n",
    "- SEZIONE 3 - Quaderno Dimostrativo con 6 esercizi pratici\n",
    "- Aggiunta micro-checkpoint con assert per verifica\n",
    "- Rimossi emoji secondo linee guida\n",
    "- Consolidata teoria in SEZIONE 1 con 17 sottosezioni\n",
    "- Aggiunta tabella metodi e funzioni\n",
    "- Glossario con 20 termini chiave\n",
    "- Tabella errori comuni con 10 problemi tipici"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
