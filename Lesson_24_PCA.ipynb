{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7b49ed",
   "metadata": {},
   "source": [
    "# Lezione 24 - PCA: Principal Component Analysis\n\n## Sezione 1 - Titolo e obiettivi\n\nObiettivo: ridurre la dimensionalita mantenendo la varianza rilevante, interpretare le componenti e usarle per visualizzazione o preprocessing ML.\n\n### Cosa impari\n- Concetto di componenti principali e varianza spiegata.\n- Come scegliere il numero di componenti con soglie e scree plot.\n- Come leggere i loadings e collegarli alle feature originali.\n- Come usare PCA per visualizzare, comprimere e ridurre rumore.\n\n### Perche serve\nTroppe feature portano a overfitting, costi computazionali e difficolta di interpretazione. PCA comprime l'informazione massimizzando la varianza nelle prime componenti e permette visualizzazioni 2D/3D di dati complessi.\n\n### Prerequisiti minimi\n- Dati numerici continui (scalati con StandardScaler).\n- Conoscenza base di matrice di covarianza e varianza.\n- Familiarita con sklearn e pipeline di preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061eae91",
   "metadata": {},
   "source": [
    "## Sezione 2 - Teoria profonda\n\n### 1.1 Perche ridurre la dimensionalita\n- Curse of dimensionality: le distanze perdono significato quando le dimensioni crescono.\n- Overfitting: troppe feature rispetto ai campioni.\n- Visualizzazione: impossibile plottare >3 dimensioni senza riduzione.\n\n### 1.2 Cosa fa PCA (passi)\n1. Centra i dati (media zero).\n2. Calcola la matrice di covarianza.\n3. Estrae autovalori/autovettori (varianza e direzioni).\n4. Proietta i dati sulle prime k direzioni (componenti principali).\n\n### 1.3 Varianza spiegata e scelta di k\n- Varianza spiegata = quota di informazione catturata da una PC.\n- Criteri: soglia cumulativa (es. 90-95%), gomito nello scree plot, Kaiser (autovalore>1 su dati standardizzati).\n\n### 1.4 Loadings e interpretazione\n- Loadings = pesi delle feature nella PC (pca.components_).\n- Loadings alti in modulo indicano feature che guidano quella componente.\n- Segno positivo/negativo indica associazione direzionale.\n\n### 1.5 Quando usare / evitare PCA\n- Usa: molte feature correlate, visualizzazione HD, riduzione rumore, velocizzare training.\n- Evita: poche feature gia interpretabili, dati categorici, quando serve interpretabilita diretta sulle feature originali.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be81c1",
   "metadata": {},
   "source": [
    "## Sezione 3 - Schema mentale e decision map\n\nWorkflow sintetico dopo scaling:\n\n```\nDati numerici -> StandardScaler -> PCA() esplorativo\n            -> Scree plot / soglia varianza -> scegli k o n_components=0.95\n            -> PCA finale -> varianza spiegata + loadings\n            -> Usa X_pca per visualizzare o addestrare modelli\n```\n\nChecklist rapida\n- [ ] Dati scalati, nessun NaN.\n- [ ] Obiettivo chiaro (visual, compressione, noise reduction, preprocessing).\n- [ ] Varianza cumulativa controllata e scelta di k motivata.\n- [ ] Loadings letti per interpretare le componenti.\n- [ ] Valutato l'impatto su prestazioni (accuracy, tempo) se usata prima di modelli.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13de423",
   "metadata": {},
   "source": [
    "## Sezione 4 - Notebook dimostrativo\n\n### Perche questo passo (Demo 1 - PCA Iris)\nMostrare come PCA compatta 4 feature in 2 PC mantenendo quasi tutta la varianza e separando le classi nel piano PC1-PC2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9c8ba",
   "metadata": {},
   "source": [
    "---\n\n### 1.4 Interpretare i Loadings\n\nI **loadings** ti dicono come ogni feature originale contribuisce alla PC:\n\n```\nEsempio: PC1 per dati di clienti\n\nFeature          Loading\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\net\u00e0              +0.45    \u2190 contribuisce molto (positivo)\nreddito          +0.52    \u2190 contribuisce molto (positivo)\nspesa_mensile    +0.48    \u2190 contribuisce molto (positivo)\nn_figli          -0.10    \u2190 contribuisce poco\n\nInterpretazione: PC1 cattura il \"potere d'acquisto\"\n(et\u00e0 + reddito + spesa vanno insieme)\n```\n\n### \ufe0f Importante: Scaling!\n\n**PCA \u00e8 sensibile alla scala!** Feature con valori grandi dominano.\n\n```\nSBAGLIATO:                    CORRETTO:\net\u00e0: 20-80                    StandardScaler prima di PCA!\nreddito: 20000-200000  \u2190 domina tutto\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5afed5e",
   "metadata": {},
   "source": [
    "---\n\n### 1.5 Quando Usare PCA\n\n|  Usa PCA quando... |  Evita PCA quando... |\n|----------------------|------------------------|\n| Hai troppe feature (>20) | Le feature sono gi\u00e0 poche |\n| Vuoi visualizzare dati HD | Ogni feature \u00e8 importante singolarmente |\n| C'\u00e8 multicollinearit\u00e0 | I dati non sono correlati |\n| Vuoi ridurre overfitting | Hai bisogno di interpretabilit\u00e0 diretta |\n| Come preprocessing per ML | I dati sono categorici |\n\n###  PCA nel Pipeline ML\n\n```\nDati \u2500\u2192 StandardScaler \u2500\u2192 PCA \u2500\u2192 Modello ML\n         (necessario!)    \u2193\n                      n_components=0.95\n                      (mantieni 95% varianza)\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371870c",
   "metadata": {},
   "source": [
    "---\n\n##  2. Schema Mentale\n\n### Workflow PCA\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          PCA WORKFLOW                               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                     \u2502\n\u2502  1. PREPARAZIONE                                                    \u2502\n\u2502     \u2514\u2500\u2500 StandardScaler() \u2014 OBBLIGATORIO per PCA!                    \u2502\n\u2502     \u2514\u2500\u2500 Verifica: dati numerici, no NaN                             \u2502\n\u2502                                                                     \u2502\n\u2502  2. PCA ESPLORATIVO (tutte le componenti)                           \u2502\n\u2502     \u2514\u2500\u2500 pca = PCA()  # senza n_components                           \u2502\n\u2502     \u2514\u2500\u2500 pca.fit(X_scaled)                                           \u2502\n\u2502     \u2514\u2500\u2500 Analizza: explained_variance_ratio_                         \u2502\n\u2502                                                                     \u2502\n\u2502  3. SCEGLI n_components                                             \u2502\n\u2502     \u2514\u2500\u2500 Plot varianza cumulativa                                    \u2502\n\u2502     \u2514\u2500\u2500 Soglia: 90-95% varianza                                     \u2502\n\u2502     \u2514\u2500\u2500 Oppure: PCA(n_components=0.95) automatico                   \u2502\n\u2502                                                                     \u2502\n\u2502  4. FIT FINALE                                                      \u2502\n\u2502     \u2514\u2500\u2500 pca = PCA(n_components=k)                                   \u2502\n\u2502     \u2514\u2500\u2500 X_pca = pca.fit_transform(X_scaled)                         \u2502\n\u2502                                                                     \u2502\n\u2502  5. INTERPRETAZIONE                                                 \u2502\n\u2502     \u2514\u2500\u2500 pca.components_ \u2192 loadings                                  \u2502\n\u2502     \u2514\u2500\u2500 Quali feature contribuiscono a ogni PC?                     \u2502\n\u2502                                                                     \u2502\n\u2502  6. USO                                                             \u2502\n\u2502     \u2514\u2500\u2500 Visualizzazione (2D/3D)                                     \u2502\n\u2502     \u2514\u2500\u2500 Input per clustering o classificazione                      \u2502\n\u2502     \u2514\u2500\u2500 Riduzione rumore                                            \u2502\n\u2502                                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n###  Checklist Pre-PCA\n\n```\n\u25a1 Dati scalati con StandardScaler?\n\u25a1 Nessun valore mancante?\n\u25a1 Solo feature numeriche?\n\u25a1 Hai un obiettivo chiaro? (visualizzazione, preprocessing, noise reduction)\n```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6f519",
   "metadata": {},
   "source": [
    "---\n\n##  3. Demo Pratiche\n\n### Demo 1 \u2014 Primo PCA: Dataset Iris\n\nVisualizziamo il classico dataset Iris (4 feature) in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb402ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1 - PCA sul dataset Iris (da 4 feature a 2 PC)\n",
    "# Intento: mostrare quanta varianza resta con 2 componenti e separazione classi.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "assert X_iris.ndim == 2 and not np.isnan(X_iris).any(), \"Dati Iris malformati\"\n",
    "\n",
    "# Scaling obbligatorio\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_iris)\n",
    "assert X_scaled.shape == X_iris.shape\n",
    "\n",
    "# PCA esplorativa\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# PCA 2 componenti per visualizzazione\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Varianza spiegata da 2 PC: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, name in enumerate(iris.target_names):\n",
    "    mask = y_iris == i\n",
    "    axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[i], label=name, s=50, alpha=0.7, edgecolors='black')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "axes[0].set_title('Iris in 2D con PCA')\n",
    "axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].bar(range(1, 5), pca_full.explained_variance_ratio_, alpha=0.7, color='steelblue', label='Varianza singola')\n",
    "axes[1].plot(range(1, 5), np.cumsum(pca_full.explained_variance_ratio_), 'ro-', label='Cumulativa')\n",
    "axes[1].axhline(y=0.95, color='red', linestyle='--', alpha=0.6, label='95%')\n",
    "axes[1].set_xlabel('Componente'); axes[1].set_ylabel('Varianza spiegata'); axes[1].set_title('Scree plot')\n",
    "axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8456e27",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 2 - Interpretare i loadings)\nCollegare PC1/PC2 alle feature originali per capire cosa rappresentano le direzioni principali.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2 - Interpretare i loadings\n",
    "# Intento: collegare PC1/PC2 alle feature originali tramite loadings e biplot.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert 'pca' in globals() and 'X_pca' in globals(), \"Esegui prima Demo 1\"\n",
    "\n",
    "loadings = pca.components_\n",
    "df_load = pd.DataFrame(loadings, columns=feature_names, index=['PC1','PC2'])\n",
    "print(\"Loadings PC1/PC2:\")\n",
    "print(df_load.round(3))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
    "im = axes[0].imshow(loadings, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[0].set_xticks(range(len(feature_names)))\n",
    "axes[0].set_xticklabels([f.replace(' (cm)','') for f in feature_names], rotation=45, ha='right')\n",
    "axes[0].set_yticks([0,1]); axes[0].set_yticklabels(['PC1','PC2'])\n",
    "axes[0].set_title('Heatmap loadings')\n",
    "for i in range(loadings.shape[0]):\n",
    "    for j in range(loadings.shape[1]):\n",
    "        axes[0].text(j, i, f\"{loadings[i,j]:.2f}\", ha='center', va='center', color='white' if abs(loadings[i,j])>0.5 else 'black')\n",
    "plt.colorbar(im, ax=axes[0], label='Loading')\n",
    "\n",
    "ax = axes[1]\n",
    "colors = ['red','green','blue']\n",
    "for i, name in enumerate(iris.target_names):\n",
    "    mask = y_iris == i\n",
    "    ax.scatter(X_pca[mask,0], X_pca[mask,1], c=colors[i], s=30, alpha=0.5, label=name)\n",
    "scale=3\n",
    "for i, feat in enumerate(feature_names):\n",
    "    ax.arrow(0,0, loadings[0,i]*scale, loadings[1,i]*scale, head_width=0.08, head_length=0.08, fc='black', ec='black')\n",
    "    ax.text(loadings[0,i]*scale*1.15, loadings[1,i]*scale*1.15, feat.replace(' (cm)',''), fontsize=9, ha='center')\n",
    "ax.set_title('Biplot PC1-PC2 + loadings'); ax.grid(True, alpha=0.3); ax.axhline(0,color='gray',linewidth=0.5); ax.axvline(0,color='gray',linewidth=0.5); ax.legend(loc='lower left')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29acb2f",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 3 - Scelta automatica di n_components)\nUsare soglia di varianza e scree plot su un dataset piu grande (digits) per decidere quante componenti tenere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d629b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3 - Scelta automatica di n_components (digits)\n",
    "# Intento: usare soglie di varianza e scree plot per decidere quante componenti tenere.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X_digits = digits.data; y_digits = digits.target\n",
    "assert X_digits.ndim == 2 and not np.isnan(X_digits).any()\n",
    "\n",
    "scaler_digits = StandardScaler()\n",
    "X_digits_scaled = scaler_digits.fit_transform(X_digits)\n",
    "assert X_digits_scaled.shape == X_digits.shape\n",
    "\n",
    "soglie = [0.80, 0.90, 0.95, 0.99]\n",
    "for soglia in soglie:\n",
    "    p = PCA(n_components=soglia)\n",
    "    p.fit(X_digits_scaled)\n",
    "    print(f\"Varianza {soglia*100:.0f}% -> {p.n_components_} componenti\")\n",
    "\n",
    "pca_full_digits = PCA()\n",
    "pca_full_digits.fit(X_digits_scaled)\n",
    "cumsum = np.cumsum(pca_full_digits.explained_variance_ratio_)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "axes[0].plot(range(1,65), pca_full_digits.explained_variance_ratio_, 'b-o', markersize=3)\n",
    "axes[0].set_title('Scree plot'); axes[0].set_xlabel('Componente'); axes[0].set_ylabel('Varianza spiegata'); axes[0].grid(True, alpha=0.3)\n",
    "axes[1].plot(range(1,65), cumsum, 'g-o', markersize=3)\n",
    "axes[1].axhline(y=0.90, color='orange', linestyle='--', label='90%'); axes[1].axhline(y=0.95, color='red', linestyle='--', label='95%')\n",
    "axes[1].set_title('Varianza cumulativa'); axes[1].set_xlabel('Componente'); axes[1].set_ylabel('Varianza cumulativa'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703fc78",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 4 - Visualizzare cifre in PCA 2D)\nVedere come le cifre si distribuiscono in 2D e quanto si separano le classi con sole due componenti.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b33230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 4 - Distribuzione delle cifre in PCA 2D\n",
    "# Intento: visualizzare digits in 2D e osservare separazione fra classi.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "assert 'X_digits_scaled' in globals(), \"Esegui Demo 3 prima\"\n",
    "\n",
    "pca2 = PCA(n_components=2)\n",
    "X_digits_2d = pca2.fit_transform(X_digits_scaled)\n",
    "assert X_digits_2d.shape[1] == 2\n",
    "\n",
    "fig, axes = plt.subplots(2,5, figsize=(15,6))\n",
    "axes = axes.flatten()\n",
    "for digit in range(10):\n",
    "    ax = axes[digit]\n",
    "    mask = y_digits == digit\n",
    "    ax.scatter(X_digits_2d[mask,0], X_digits_2d[mask,1], c='blue', s=10, alpha=0.7)\n",
    "    ax.set_title(f'Cifra {digit}'); ax.set_xticks([]); ax.set_yticks([]); ax.grid(True, alpha=0.2)\n",
    "plt.suptitle('Digits in PCA 2D'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188906e1",
   "metadata": {},
   "source": [
    "### Perche questo passo (Demo 5 - PCA per denoising)\nUsare poche componenti per ricostruire immagini rumorose, osservando il trade-off tra dettaglio e riduzione del rumore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 5 - PCA per riduzione rumore\n",
    "# Intento: ricostruire immagini rumorose con poche componenti per filtrare il rumore.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "assert 'digits' in globals() and 'X_digits_scaled' in globals(), \"Esegui Demo 3 prima\"\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = [0,100,200,300,400]\n",
    "X_sample = digits.images[indices].copy()\n",
    "noise_level = 5\n",
    "X_noisy = X_sample + np.random.normal(0, noise_level, X_sample.shape)\n",
    "X_noisy = np.clip(X_noisy, 0, 16)\n",
    "X_noisy_flat = X_noisy.reshape(len(indices), -1)\n",
    "\n",
    "n_components_list = [5,10,20,40]\n",
    "fig, axes = plt.subplots(len(indices), len(n_components_list)+2, figsize=(16,10))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[i,0].imshow(X_sample[i], cmap='gray'); axes[i,0].set_title('Originale' if i==0 else ''); axes[i,0].axis('off')\n",
    "    axes[i,1].imshow(X_noisy[i], cmap='gray'); axes[i,1].set_title(f'+Rumore' if i==0 else ''); axes[i,1].axis('off')\n",
    "    for j, n_comp in enumerate(n_components_list):\n",
    "        pca_dn = PCA(n_components=n_comp)\n",
    "        pca_dn.fit(X_digits_scaled)\n",
    "        x_noisy_scaled = (X_noisy_flat[i] - scaler_digits.mean_) / scaler_digits.scale_\n",
    "        x_pca = pca_dn.transform(x_noisy_scaled.reshape(1,-1))\n",
    "        x_rec = pca_dn.inverse_transform(x_pca)\n",
    "        x_rec = x_rec * scaler_digits.scale_ + scaler_digits.mean_\n",
    "        axes[i, j+2].imshow(x_rec.reshape(8,8), cmap='gray')\n",
    "        if i==0:\n",
    "            var = pca_dn.explained_variance_ratio_.sum()*100\n",
    "            axes[i, j+2].set_title(f'n={n_comp}\n",
    "({var:.0f}%)')\n",
    "        axes[i, j+2].axis('off')\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[i,0].set_ylabel(f'Cifra {y_digits[idx]}')\n",
    "plt.suptitle('PCA per denoising'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44147d87",
   "metadata": {},
   "source": [
    "## Sezione 5 - Esercizi guidati (step by step)\n\n### Perche questo esercizio (24.1)\nApplicare il flusso completo PCA sul dataset Wine: scelta di k, visualizzazione e interpretazione loadings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633263de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 24.1 - PCA su dataset Wine\n",
    "# Intento: scegliere componenti per 80/95% varianza, visualizzare e leggere i loadings.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "wine = load_wine()\n",
    "X_wine = wine.data; y_wine = wine.target; feature_names_wine = wine.feature_names\n",
    "assert not np.isnan(X_wine).any()\n",
    "\n",
    "scaler_wine = StandardScaler(); X_wine_scaled = scaler_wine.fit_transform(X_wine)\n",
    "assert X_wine_scaled.shape == X_wine.shape\n",
    "\n",
    "pca_wine = PCA(); pca_wine.fit(X_wine_scaled)\n",
    "cumsum_var = np.cumsum(pca_wine.explained_variance_ratio_)\n",
    "n_80 = int(np.argmax(cumsum_var>=0.80)+1)\n",
    "n_95 = int(np.argmax(cumsum_var>=0.95)+1)\n",
    "print(f\"Componenti per 80%: {n_80}, per 95%: {n_95}\")\n",
    "\n",
    "pca_2d = PCA(n_components=2); X_wine_2d = pca_2d.fit_transform(X_wine_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(16,5))\n",
    "axes[0].bar(range(1,len(pca_wine.explained_variance_ratio_)+1), pca_wine.explained_variance_ratio_, color='steelblue', alpha=0.7)\n",
    "axes[0].plot(range(1,len(cumsum_var)+1), cumsum_var, 'ro-', label='Cumulativa')\n",
    "axes[0].axhline(y=0.80, color='orange', linestyle='--'); axes[0].axhline(y=0.95, color='red', linestyle='--')\n",
    "axes[0].set_title('Scree plot'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "colors = ['red','green','blue']\n",
    "for i,name in enumerate(wine.target_names):\n",
    "    mask = y_wine == i\n",
    "    axes[1].scatter(X_wine_2d[mask,0], X_wine_2d[mask,1], c=colors[i], s=50, alpha=0.7, edgecolors='black', label=name)\n",
    "axes[1].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "axes[1].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)'); axes[1].set_title('Wine in 2D'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "loadings = pca_2d.components_\n",
    "axes[2].barh(range(len(feature_names_wine)), loadings[0], alpha=0.7, label='PC1')\n",
    "axes[2].barh(range(len(feature_names_wine)), loadings[1], alpha=0.7, label='PC2', left=loadings[0])\n",
    "axes[2].set_yticks(range(len(feature_names_wine))); axes[2].set_yticklabels(feature_names_wine, fontsize=8)\n",
    "axes[2].set_title('Loadings PC1/PC2'); axes[2].legend(); axes[2].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Top feature per interpretazione\n",
    "import pandas as pd\n",
    "df_load = pd.DataFrame({'feature':feature_names_wine, 'PC1':loadings[0], 'PC2':loadings[1]})\n",
    "print(df_load.reindex(df_load['PC1'].abs().sort_values(ascending=False).index).head(3))\n",
    "print(df_load.reindex(df_load['PC2'].abs().sort_values(ascending=False).index).head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb30d6e2",
   "metadata": {},
   "source": [
    "### Perche questo esercizio (24.2)\nValutare l'impatto di PCA sul training di un classificatore confrontando tempo e accuracy prima/dopo la riduzione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfbe818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 24.2 - PCA come preprocessing per classificazione (Digits)\n",
    "# Intento: confrontare tempo e accuracy con/ senza PCA al 90% di varianza.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Dati\n",
    "if 'digits' not in globals():\n",
    "    digits = load_digits()\n",
    "X = digits.data; y = digits.target\n",
    "assert not np.isnan(X).any()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler_cls = StandardScaler(); X_train_s = scaler_cls.fit_transform(X_train); X_test_s = scaler_cls.transform(X_test)\n",
    "\n",
    "# Senza PCA\n",
    "start = time.time(); lr_no = LogisticRegression(max_iter=5000, random_state=42)\n",
    "lr_no.fit(X_train_s, y_train); t_no = time.time()-start; acc_no = lr_no.score(X_test_s, y_test)\n",
    "\n",
    "# Con PCA (90%)\n",
    "pca_cls = PCA(n_components=0.90)\n",
    "X_train_pca = pca_cls.fit_transform(X_train_s); X_test_pca = pca_cls.transform(X_test_s)\n",
    "start = time.time(); lr_pca = LogisticRegression(max_iter=5000, random_state=42)\n",
    "lr_pca.fit(X_train_pca, y_train); t_pca = time.time()-start; acc_pca = lr_pca.score(X_test_pca, y_test)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Metrica':['Feature','Tempo (s)','Accuracy (%)'],\n",
    "    'Senza PCA':[X_train_s.shape[1], f\"{t_no:.4f}\", f\"{acc_no*100:.2f}\"],\n",
    "    'Con PCA':[pca_cls.n_components_, f\"{t_pca:.4f}\", f\"{acc_pca*100:.2f}\"]\n",
    "})\n",
    "print(comparison.to_string(index=False))\n",
    "feature_reduction = (1 - pca_cls.n_components_/X_train_s.shape[1]) * 100\n",
    "print(f\"Riduzione feature: {feature_reduction:.1f}% | Speedup ~{(t_no/t_pca if t_pca>0 else 1):.2f}x | Delta accuracy {((acc_pca-acc_no)*100):+.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc54f7",
   "metadata": {},
   "source": [
    "### Perche questo esercizio (24.3)\nConfrontare separabilita nello spazio originale vs PCA calcolando distanze tra centroidi e visualizzando le classi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3063998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 24.3 - Confronto separabilita spazio originale vs PCA (Iris)\n",
    "# Intento: misurare quanto PCA aumenta la distanza tra classi rispetto a due feature originali.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "iris = load_iris(); X = iris.data; y = iris.target; target_names = iris.target_names\n",
    "assert not np.isnan(X).any()\n",
    "\n",
    "X_orig = X[:, :2]  # sepal length, sepal width\n",
    "scaler_ir = StandardScaler(); X_scaled = scaler_ir.fit_transform(X)\n",
    "pca_ir = PCA(n_components=2); X_pca = pca_ir.fit_transform(X_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,6))\n",
    "colors=['red','green','blue']; markers=['o','s','^']\n",
    "for i,name in enumerate(target_names):\n",
    "    mask = y==i\n",
    "    axes[0].scatter(X_orig[mask,0], X_orig[mask,1], c=colors[i], marker=markers[i], s=70, edgecolors='black', alpha=0.7, label=name)\n",
    "axes[0].set_title('Spazio originale (2 feature)'); axes[0].set_xlabel('Sepal length'); axes[0].set_ylabel('Sepal width'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "for i,name in enumerate(target_names):\n",
    "    mask = y==i\n",
    "    axes[1].scatter(X_pca[mask,0], X_pca[mask,1], c=colors[i], marker=markers[i], s=70, edgecolors='black', alpha=0.7, label=name)\n",
    "axes[1].set_title(f'Spazio PCA (varianza {pca_ir.explained_variance_ratio_.sum()*100:.1f}%)'); axes[1].set_xlabel('PC1'); axes[1].set_ylabel('PC2'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "def dist_centroidi(Xmat, yvec):\n",
    "    centroids=[]\n",
    "    for c in np.unique(yvec):\n",
    "        centroids.append(Xmat[yvec==c].mean(axis=0))\n",
    "    centroids=np.array(centroids)\n",
    "    d=cdist(centroids, centroids)\n",
    "    vals=d[np.triu_indices_from(d, k=1)]\n",
    "    return vals.mean()\n",
    "\n",
    "dist_orig = dist_centroidi(X_orig, y)\n",
    "dist_pca = dist_centroidi(X_pca, y)\n",
    "print(f\"Distanza media centroidi - originale: {dist_orig:.3f} | PCA: {dist_pca:.3f} | Delta: {(dist_pca-dist_orig)/dist_orig*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a7946",
   "metadata": {},
   "source": [
    "## Sezione 6 - Conclusione operativa\n\n### Cosa portarsi a casa\n- PCA comprime l'informazione ordinando le direzioni per varianza: le prime componenti spiegano quasi tutto se le feature sono correlate.\n- Scaling e obbligatorio: senza StandardScaler una feature con range piu grande domina la PCA.\n- Scegli n_components con soglie di varianza o scree plot; annota sempre quanta varianza conservi.\n- Leggi i loadings per collegare le PC al significato delle feature originali.\n- Per classificazione/clustering valuta se la riduzione migliora tempo e separabilita.\n\n### Methods explained (uso, input/output, errori tipici)\n- `StandardScaler`: standardizza feature (input array n_samples x n_features, output stessa shape); errori: NaN o colonne costanti.\n- `PCA`: calcola componenti principali; input X scalato 2D; output trasformato X_pca e attributi `explained_variance_ratio_`, `components_`; errori: dati non scalati o n_components troppo alto (>n_features).\n- `explained_variance_ratio_`: quota di varianza per PC; serve per scree plot e soglia cumulativa.\n- `components_` (loadings): righe=PC, colonne=feature; errori: interpretazione senza considerare il segno.\n- `StandardScaler` + `PCA` in pipeline: evita leakage fit/transform su train e poi transform su test.\n\n### Common errors and quick debug\n- Dati non scalati: PC distorte; fix: applica StandardScaler prima di PCA.\n- Troppe poche PC: perdita di informazione e calo accuracy; fix: aumenta soglia varianza e ricontrolla.\n- Interpretazione errata dei loadings: confusione su segni e magnitudini; fix: guarda valori assoluti per importanza e il segno per la direzione.\n- Applicare PCA a dati categorici: componenti senza senso; fix: usa tecniche per categorie (one-hot + eventualmente altra riduzione come MCA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891fa629",
   "metadata": {},
   "source": [
    "## Sezione 7 - End-of-lesson checklist e glossario\n\n### Checklist finale\n- [ ] Dati numerici e scalati con `StandardScaler`.\n- [ ] Varianza spiegata cumulativa valutata (soglia 90-95%).\n- [ ] Numero di componenti scelto e motivato (scree plot/gomito o n_components=valore).\n- [ ] Loadings letti per interpretare le PC.\n- [ ] Se usata in pipeline ML: verificato impatto su tempo e accuracy.\n- [ ] Silhouette/metriche calcolate nel nuovo spazio se usato per clustering.\n\n### Glossario (termini usati)\n- Componente Principale (PC): nuova direzione di massima varianza.\n- Varianza spiegata: quota di varianza catturata da una PC.\n- Scree plot: grafico autovalori/varianza per PC.\n- Loadings: pesi delle feature nella PC.\n- Kaiser criterion: tieni PC con autovalore>1 (dopo scaling).\n- Denoising: riduzione rumore ricostruendo con poche PC.\n- n_components: parametro PCA per fissare numero o soglia di varianza.\n- StandardScaler: standardizza feature a media 0 e dev. std 1.\n- Curse of dimensionality: difficolta dovute ad alto numero di feature.\n- Eigenvalue/eigenvector: autovalore/autovettore di covarianza (varianza e direzione).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Sezione 8 - Didactic changelog (max 10 voci)\n1. Riorganizzate le 8 sezioni con obiettivi, teoria, schema mentale, demo ed esercizi guidati.\n2. Ripulito testo da emoji/non ASCII e aggiunte rationale per demo/esercizi.\n3. Commentati i code cell con scaling, assert e note su silhouette/varianza.\n4. Inseriti Methods explained, errori comuni, checklist e glossario nelle sezioni conclusive.\n"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}