{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21eea501",
   "metadata": {},
   "source": [
    "# Lezione 38 — Introduzione ai Modelli Generativi\n",
    "\n",
    "**Obiettivi della lezione**\n",
    "- Distinguere generazione vs predizione nel contesto dei modelli statistici.\n",
    "- Capire LLM come modelli probabilistici di sequenze (token).\n",
    "- Introdurre la probabilità condizionata e il ruolo della catena di Markov testuale.\n",
    "- Riconoscere hallucinations e rischi operativi/interpretativi per un data analyst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb396c",
   "metadata": {},
   "source": [
    "## Teoria concettuale approfondita\n",
    "\n",
    "**Generazione vs predizione**\n",
    "- Predizione: stimare $y$ dato $x$ (es. regressione, classificazione). Output è etichetta/valore.\n",
    "- Generazione: produrre nuove istanze coerenti con la distribuzione dei dati ($p(x)$ o $p(x|c)$). Output è una sequenza/oggetto.\n",
    "\n",
    "**LLM come modelli probabilistici**\n",
    "- Vettore di token: il testo è scomposto in unità (token). Un LLM modella $p(t_i | t_1, ..., t_{i-1})$.\n",
    "- Autoregressione: la sequenza è generata un token alla volta campionando dalla distribuzione condizionata precedente.\n",
    "\n",
    "**Probabilità condizionata e catena**\n",
    "- Catena: $p(t_1, ..., t_n) = \\prod_{i=1}^n p(t_i | t_1, ..., t_{i-1})$.\n",
    "- In pratica, i modelli moderni usano contesto ampio (finestra) e embedding per rappresentare i token.\n",
    "\n",
    "**Hallucinations**\n",
    "- Output plausibile ma non supportato dai dati di addestramento o dal prompt; deriva dal campionamento di probabilità alte ma non verificate.\n",
    "- Fattori: temperature alte, prompt ambiguo, mancanza di grounding in fonti affidabili.\n",
    "\n",
    "**Rischi operativi**\n",
    "- Informazioni non verificate, bias, fuga di dati sensibili se input non filtrati.\n",
    "- Interpretazione errata di probabilità: alta probabilità non implica verità fattuale.\n",
    "\n",
    "**Perché non fare deep learning operativo qui**\n",
    "- Obiettivo: capire il concetto probabilistico, non addestrare o fine-tunare reti complesse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706cbdde",
   "metadata": {},
   "source": [
    "## Schema mentale / mappa logica\n",
    "- **Quando usare modelli generativi**: sintesi di testo, summarization, assistenza alla scrittura, prototipazione di idee; generazione di esempi sintetici per test.\n",
    "- **Quando NON usare**: decisioni regolamentate, report ufficiali senza verifica, dati sensibili non filtrati, domini dove l’errore costa caro.\n",
    "- **Segnali pratici**: output “plausibile ma non verificato” → serve controllo umano; temperature alte amplificano creatività e rischio; prompt ambiguo aumenta la variabilità.\n",
    "- **Pattern operativo**: definire il contesto, restringere il dominio, aggiungere fonti di verità (grounding), valutare l’output con checklist di qualità."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a8936",
   "metadata": {},
   "source": [
    "## Notebook dimostrativo (modello generativo minimale con probabilità condizionata)\n",
    "Costruiamo un semplice modello bigram (catena di Markov di ordine 1) su un piccolo corpus testuale per mostrare:\n",
    "- stima di $p(t_i | t_{i-1})$;\n",
    "- generazione autoregressiva token-per-token;\n",
    "- effetti di temperatura/rumore sulla variabilità e sulle “hallucinations”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b932272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiamo librerie base per contare frequenze e generare testo\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "random.seed(42)  # riproducibilità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e220f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo un piccolo corpus testuale su un dominio specifico (assistenza clienti)\n",
    "corpus = [\n",
    "    \"spedizione standard richiede tre giorni lavorativi\",\n",
    "    \"spedizione express richiede un giorno lavorativo\",\n",
    "    \"resi accettati entro trenta giorni con ricevuta\",\n",
    "    \"rimborsi gestiti dal reparto finance con approvazione\",\n",
    "]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deaea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizziamo in modo minimale e costruiamo frequenze bigram\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "bigrams = defaultdict(Counter)\n",
    "for sent in corpus:\n",
    "    tokens = [\"<s>\"] + tokenize(sent) + [\"</s>\"]  # aggiungiamo start/end\n",
    "    for w1, w2 in zip(tokens, tokens[1:]):\n",
    "        bigrams[w1][w2] += 1\n",
    "\n",
    "# Calcoliamo distribuzioni condizionate p(w2|w1)\n",
    "bigram_probs = {}\n",
    "for w1, counter in bigrams.items():\n",
    "    total = sum(counter.values())\n",
    "    bigram_probs[w1] = {w2: cnt / total for w2, cnt in counter.items()}\n",
    "\n",
    "# Mostriamo le probabilità di alcune transizioni\n",
    "bigram_probs[\"spedizione\"], bigram_probs[\"richiede\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba69313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di generazione autoregressiva con temperatura\n",
    "\n",
    "def sample_next(probs: dict, temperature: float = 1.0):\n",
    "    # Applichiamo temperatura ai log-prob per controllare la randomicità\n",
    "    items = list(probs.items())\n",
    "    tokens, p = zip(*items)\n",
    "    if temperature != 1.0:\n",
    "        import math\n",
    "\n",
    "        logits = [math.log(pi + 1e-12) / temperature for pi in p]\n",
    "        # Normalizziamo per ottenere una distribuzione valida\n",
    "        max_logit = max(logits)\n",
    "        exp_logits = [math.exp(l - max_logit) for l in logits]\n",
    "        total = sum(exp_logits)\n",
    "        p = [v / total for v in exp_logits]\n",
    "    return random.choices(tokens, weights=p, k=1)[0]\n",
    "\n",
    "\n",
    "def generate(max_len=15, temperature=1.0):\n",
    "    token = \"<s>\"\n",
    "    output = []\n",
    "    for _ in range(max_len):\n",
    "        probs = bigram_probs.get(token, None)\n",
    "        if not probs:\n",
    "            break\n",
    "        nxt = sample_next(probs, temperature=temperature)\n",
    "        if nxt == \"</s>\":\n",
    "            break\n",
    "        output.append(nxt)\n",
    "        token = nxt\n",
    "    return \" \".join(output)\n",
    "\n",
    "# Generiamo con due temperature per confrontare coerenza vs variabilità\n",
    "gen_cool = generate(temperature=0.7)\n",
    "gen_hot = generate(temperature=1.5)\n",
    "(gen_cool, gen_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea6fb4",
   "metadata": {},
   "source": [
    "### Osservazioni sul risultato\n",
    "- Temperatura più bassa (0.7) privilegia i token più probabili ⇒ frasi più prevedibili e coerenti.\n",
    "- Temperatura alta (1.5) aumenta l’esplorazione ⇒ più variabilità, maggiore rischio di incoerenza (hallucination in scala toy).\n",
    "- Il modello bigram ignora contesto lungo: può produrre frasi grammaticali ma semantica limitata. LLM usano contesti più lunghi e rappresentazioni dense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17481f8a",
   "metadata": {},
   "source": [
    "## Esercizi svolti (step-by-step)\n",
    "Esercizi per consolidare: stima di probabilità condizionate, effetto della temperatura, gestione di contesto limitato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81353828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 1: calcolare la probabilità di una frase corta secondo il modello bigram\n",
    "\n",
    "sentence = \"spedizione standard richiede\".split()\n",
    "log_prob = 0.0\n",
    "for w1, w2 in zip([\"<s>\"] + sentence, sentence + [\"</s>\"]):\n",
    "    p = bigram_probs.get(w1, {}).get(w2, 1e-12)  # smoothing minimale per zero-freq\n",
    "    import math\n",
    "\n",
    "    log_prob += math.log(p)\n",
    "log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe783632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 2: osservare l'effetto della temperatura su più campioni\n",
    "for temp in [0.6, 1.0, 1.4]:\n",
    "    samples = [generate(temperature=temp) for _ in range(3)]\n",
    "    print(f\"T={temp}: {samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 3: aggiungere un nuovo documento per ridurre hallucinations nel dominio\n",
    "nuovo_doc = \"spedizione express spesso richiede approvazione speciale\"\n",
    "corpus_esteso = corpus + [nuovo_doc]\n",
    "\n",
    "# Ricostruiamo bigrammi con il nuovo documento\n",
    "bigrams_ext = defaultdict(Counter)\n",
    "for sent in corpus_esteso:\n",
    "    tokens = [\"<s>\"] + tokenize(sent) + [\"</s>\"]\n",
    "    for w1, w2 in zip(tokens, tokens[1:]):\n",
    "        bigrams_ext[w1][w2] += 1\n",
    "\n",
    "bigram_probs_ext = {}\n",
    "for w1, counter in bigrams_ext.items():\n",
    "    total = sum(counter.values())\n",
    "    bigram_probs_ext[w1] = {w2: cnt / total for w2, cnt in counter.items()}\n",
    "\n",
    "# Generiamo dal modello esteso per vedere se compaiono termini più coerenti con express\n",
    "def generate_ext(max_len=15, temperature=1.0):\n",
    "    token = \"<s>\"\n",
    "    output = []\n",
    "    for _ in range(max_len):\n",
    "        probs = bigram_probs_ext.get(token, None)\n",
    "        if not probs:\n",
    "            break\n",
    "        nxt = sample_next(probs, temperature=temperature)\n",
    "        if nxt == \"</s>\":\n",
    "            break\n",
    "        output.append(nxt)\n",
    "        token = nxt\n",
    "    return \" \".join(output)\n",
    "\n",
    "[generate_ext(temperature=0.9) for _ in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d6c760",
   "metadata": {},
   "source": [
    "## Conclusione operativa\n",
    "- Portarsi a casa: un modello generativo produce sequenze campionando da distribuzioni condizionate; la temperatura controlla creatività vs coerenza.\n",
    "- Errori da evitare: confondere alta probabilità con verità; usare output senza verifica; non limitare il dominio o aggiungere grounding.\n",
    "- Ponte verso la prossima lezione: AI nel mondo reale — decision support, automazione e ruolo umano nel controllo di qualità."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
