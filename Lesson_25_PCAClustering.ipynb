{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d870580",
   "metadata": {},
   "source": [
    "# 1) Titolo e obiettivi\n",
    "\n",
    "Lezione 25: PCA + Clustering - Riduzione dimensionale per segmentazione robusta\n",
    "\n",
    "---\n",
    "\n",
    "## Mappa della lezione\n",
    "\n",
    "| Sezione | Contenuto | Tempo stimato |\n",
    "|---------|-----------|---------------|\n",
    "| 1 | Titolo, obiettivi, quando combinare PCA e clustering | 5 min |\n",
    "| 2 | Teoria profonda: vantaggi, scelta n_components | 15 min |\n",
    "| 3 | Schema mentale: workflow PCA → Clustering | 5 min |\n",
    "| 4 | Demo: Digits, Wine, confronto con/senza PCA | 25 min |\n",
    "| 5 | Esercizi risolti + errori comuni | 15 min |\n",
    "| 6 | Conclusione operativa | 10 min |\n",
    "| 7 | Checklist di fine lezione + glossario | 5 min |\n",
    "| 8 | Changelog didattico | 2 min |\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivi della lezione\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "| # | Obiettivo | Verifica |\n",
    "|---|-----------|----------|\n",
    "| 1 | Applicare **PCA prima del clustering** | Sai perché riduce la curse of dimensionality? |\n",
    "| 2 | Scegliere **n_components** con criterio | Sai usare soglia varianza e confronto metriche? |\n",
    "| 3 | Valutare l'impatto su **qualità e velocità** | Sai confrontare silhouette con/senza PCA? |\n",
    "| 4 | Interpretare i cluster con i **loadings** | Sai collegare PC alle feature originali? |\n",
    "| 5 | Combinare PCA con **K-Means, DBSCAN, Gerarchico** | Conosci le pipeline? |\n",
    "\n",
    "---\n",
    "\n",
    "## L'idea centrale: perché PCA + Clustering\n",
    "\n",
    "```\n",
    "DATI ORIGINALI (64 feature):     DOPO PCA (15 PC):        CLUSTERING:\n",
    "\n",
    "●●●●●●●●●●●●●●●●●●●●            ●●●●●●●●●●●●●●●          ● ● ●\n",
    "●●●●●●●●●●●●●●●●●●●●   PCA →    ●●●●●●●●●●●●●●●  K-Means →  ● ●  ●\n",
    "●●●●●●●●●●●●●●●●●●●●            ●●●●●●●●●●●●●●●          ●   ● ●\n",
    "      ↑                              ↑                     ↑\n",
    "  Curse of dimensionality      Varianza compressa     Cluster separati!\n",
    "  Distanze perdono senso       90% info in 15 PC      Più robusti\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## I 3 vantaggi di PCA prima del clustering\n",
    "\n",
    "| Vantaggio | Spiegazione | Esempio |\n",
    "|-----------|-------------|---------|\n",
    "| **Riduce rumore** | Elimina componenti con poca varianza (rumore) | Digits: da 64 a 20 PC |\n",
    "| **Velocizza** | Meno feature = meno calcoli | K-Means 10x più veloce |\n",
    "| **Migliora separazione** | Distanze più significative in basso D | Silhouette migliore |\n",
    "\n",
    "---\n",
    "\n",
    "## Quando NON usare PCA prima del clustering\n",
    "\n",
    "| Situazione | Perché evitare PCA |\n",
    "|------------|-------------------|\n",
    "| Poche feature già interpretabili (<15) | Perdi interpretabilità diretta |\n",
    "| Cluster separati da feature rare | PCA può diluire la separazione |\n",
    "| DBSCAN funziona già bene | Non complicare senza motivo |\n",
    "| Interpretazione feature-by-feature richiesta | PCA mescola le feature |\n",
    "\n",
    "**Regola d'oro:** confronta SEMPRE baseline senza PCA vs con PCA.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisiti\n",
    "\n",
    "| Concetto | Dove lo trovi | Verifica |\n",
    "|----------|---------------|----------|\n",
    "| PCA | Lezione 24 | Sai interpretare varianza spiegata? |\n",
    "| K-Means | Lezione 20 | Conosci inertia e silhouette? |\n",
    "| DBSCAN | Lezione 23 | Sai scegliere eps? |\n",
    "| StandardScaler | Lezione 13, 20 | Sai perché scalare? |\n",
    "\n",
    "**Cosa useremo:** dataset Digits, Wine, Iris; StandardScaler, PCA, KMeans, DBSCAN, AgglomerativeClustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9349b",
   "metadata": {},
   "source": [
    "# 2) Teoria concettuale\n",
    "## 2.1 Perche' combinare PCA e clustering\n",
    "- PCA comprime l'informazione, riduce il rumore e rende le distanze piu' significative in spazi ad alta dimensionalita'.\n",
    "- Clustering su poche componenti e' piu' veloce e meno sensibile alla curse of dimensionality.\n",
    "- Per KMeans: PCA rende le varianze piu' omogenee e facilita centroidi stabili; per DBSCAN riduce la sparizione di densita' utile.\n",
    "- Se i cluster sono anisotropi, PCA li rende piu' sferici e quindi piu' facili da separare con algoritmi basati su distanza euclidea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8ed35",
   "metadata": {},
   "source": [
    "## 2.2 Quante componenti usare\n",
    "- Criterio di varianza cumulata (es. 0.80, 0.90, 0.95) come compromesso tra perdita di informazione e rumore.\n",
    "- Scree plot: osservare il gomito dove i benefici marginali calano.\n",
    "- Verifica empirica: confrontare silhouette/ARI e i tempi di fit tra soglie diverse.\n",
    "- Loadings: ogni componente e' una combinazione lineare di feature originali; servono a interpretare cosa differenzia i cluster.\n",
    "- Se servono visualizzazioni, conservare almeno le prime 2-3 componenti anche se la soglia e' piu' alta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b452e5",
   "metadata": {},
   "source": [
    "## 2.3 Quando non usare PCA prima del clustering\n",
    "- Feature gia' semantiche e poche (<15) con scale coerenti: PCA potrebbe offuscare l'interpretabilita'.\n",
    "- Cluster separati da feature rare/importanti: PCA puo' diluire la separazione eliminando varianza specifica.\n",
    "- Se DBSCAN lavora gia' bene nello spazio originale (densita' evidente), testare prima senza PCA.\n",
    "- Regola pratica: confrontare sempre baseline senza PCA e versioni con PCA per verificare che la qualita' non peggiori.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d8b6f",
   "metadata": {},
   "source": [
    "# 3) Schema mentale / mappa decisionale\n",
    "Workflow di riferimento: load -> clean -> scale -> (opzionale) PCA -> cluster -> valuta -> interpreta.\n",
    "Decision map testuale:\n",
    "1. Scala i dati se le feature non hanno la stessa scala.\n",
    "2. Esegui un clustering senza PCA per avere una baseline di silhouette/ARI e tempi.\n",
    "3. Prova PCA con soglie 0.80/0.90/0.95 di varianza, confronta metriche e tempi.\n",
    "4. Se usi DBSCAN, valuta anche combinazioni di eps/min_samples nello spazio PCA.\n",
    "5. Interpreta i cluster guardando le componenti principali e i loadings piu' forti.\n",
    "Micro-checklist: niente NaN, forme coerenti dopo PCA, numero cluster atteso, presenza di rumore (-1) se DBSCAN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e911b",
   "metadata": {},
   "source": [
    "# 4) Sezione dimostrativa\n",
    "Panoramica delle demo:\n",
    "- Demo 1: Digits con PCA al 90% di varianza + KMeans, metriche e grafici.\n",
    "- Demo 2: Confronto con/senza PCA su Digits (tempi e qualita').\n",
    "- Demo 3: Pipeline compatta PCA + KMeans su Iris.\n",
    "- Demo 4: PCA + DBSCAN per individuare outlier.\n",
    "- Demo 5: Visualizzazione 3D con PCA su Wine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9196ab",
   "metadata": {},
   "source": [
    "## Demo 1 - PCA 90% + KMeans su Digits\n",
    "Perche': mostrare che ridurre le 64 feature di Digits a poche componenti mantiene la struttura e migliora stabilita'.\n",
    "Metodi usati (input -> output):\n",
    "- `StandardScaler`: input array (n_samples x n_features), output array scalato; errore tipico: valori NaN o colonne costanti.\n",
    "- `PCA(n_components=0.90)`: riduce alle componenti che spiegano il 90% della varianza; errore tipico: n_components troppo alto rispetto ai campioni.\n",
    "- `KMeans(n_clusters=10)`: richiede dati numerici e n_clusters noto; errore tipico: dati non scalati o cluster non sferici.\n",
    "Checkpoint attesi: nessun NaN, forma X (1797, 64), forma X_pca (1797, ~20 componenti), varianza cumulata >= 0.90, silhouette > 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3989b351",
   "metadata": {},
   "source": [
    "### Checkpoint visivo per Demo 1\n",
    "- Scree plot decrescente: il gomito dovrebbe apparire dopo ~15-25 componenti.\n",
    "- Scatter PC1 vs PC2: punti a gruppi con confini ragionevolmente separati; se tutto sovrapposto, controllare scaler o n_clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: PCA (90% varianza) + KMeans su Digits\n",
    "# Intenzione: ridurre dimensionalita', verificare varianza mantenuta, eseguire clustering stabile e valutare silhouette/ARI.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.close('all')\n",
    "\n",
    "# Caricamento dati\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "print(f\"Forma originale X: {X_digits.shape}, y: {y_digits.shape}\")\n",
    "assert X_digits.shape[0] == y_digits.shape[0], \"Numero di campioni incoerente\"\n",
    "assert not np.isnan(X_digits).any(), \"Sono presenti NaN nel dataset\"\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_digits)\n",
    "print(f\"Forma dopo scaling: {X_scaled.shape}\")\n",
    "\n",
    "# PCA al 90% di varianza spiegata\n",
    "pca = PCA(n_components=0.90, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "var_cum = pca.explained_variance_ratio_.sum()\n",
    "print(f\"Componenti tenute: {X_pca.shape[1]}, Varianza cumulata: {var_cum:.3f}\")\n",
    "assert var_cum >= 0.90, \"La varianza cumulata e' inferiore alla soglia desiderata\"\n",
    "\n",
    "# KMeans sui dati ridotti\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_pca)\n",
    "ari = adjusted_rand_score(y_digits, labels)\n",
    "sil = silhouette_score(X_pca, labels)\n",
    "print(f\"ARI: {ari:.3f}, Silhouette: {sil:.3f}\")\n",
    "\n",
    "# Scree plot e scatter PC1-PC2\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(np.arange(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
    "axes[0].set_title('Scree plot - varianza per componente')\n",
    "axes[0].set_xlabel('Componente')\n",
    "axes[0].set_ylabel('Quota di varianza')\n",
    "axes[0].axhline(0.05, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "scatter = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', s=12, alpha=0.7)\n",
    "axes[1].set_title('KMeans su PC1-PC2')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "plt.colorbar(scatter, ax=axes[1], label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0e256",
   "metadata": {},
   "source": [
    "## Demo 2 - Confronto con e senza PCA\n",
    "Perche': quantificare il compromesso tra tempo di esecuzione e qualita' del clustering.\n",
    "Metodi: `Pipeline` con `StandardScaler`, PCA opzionale, `KMeans` fisso a 10 cluster.\n",
    "Checkpoint: tempo con PCA inferiore o simile, silhouette/ARI non peggiorate; se peggiorano molto, ridurre meno la dimensionalita'.\n",
    "Errori comuni: dimenticare lo scaler (cluster distorti), PCA con troppe poche componenti (silhouette bassa), silhouette non calcolabile se tutti i punti finiscono nello stesso cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ffc32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Confronto con/senza PCA su Digits\n",
    "# Intenzione: misurare l'effetto di PCA su tempi e metriche di clustering con KMeans.\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "scenarios = [\n",
    "    (\"Senza PCA\", None),\n",
    "    (\"PCA var 0.90\", 0.90),\n",
    "    (\"PCA 50 componenti\", 50),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, n_comp in scenarios:\n",
    "    steps = [(\"scaler\", StandardScaler())]\n",
    "    if n_comp is not None:\n",
    "        steps.append((\"pca\", PCA(n_components=n_comp, random_state=42)))\n",
    "    steps.append((\"kmeans\", KMeans(n_clusters=10, random_state=42, n_init=10)))\n",
    "    pipe = Pipeline(steps)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    labels = pipe.fit_predict(X_digits)\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # Trasformazioni per valutare silhouette nello spazio usato dal modello\n",
    "    if n_comp is None:\n",
    "        X_used = pipe.named_steps[\"scaler\"].transform(X_digits)\n",
    "    else:\n",
    "        X_scaled_tmp = pipe.named_steps[\"scaler\"].transform(X_digits)\n",
    "        X_used = pipe.named_steps[\"pca\"].transform(X_scaled_tmp)\n",
    "\n",
    "    assert X_used.shape[0] == X_digits.shape[0], \"Numero di campioni alterato\"\n",
    "    ari = adjusted_rand_score(y_digits, labels)\n",
    "    sil = silhouette_score(X_used, labels)\n",
    "    results.append({\"scenario\": name, \"caratteristiche_usate\": X_used.shape[1], \"silhouette\": sil, \"ari\": ari, \"tempo_s\": elapsed})\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "print(res_df)\n",
    "\n",
    "best_idx = res_df['silhouette'].idxmax()\n",
    "print(\"Miglior silhouette:\")\n",
    "print(res_df.loc[[best_idx]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1509613",
   "metadata": {},
   "source": [
    "## Demo 3 - Pipeline compatta su Iris\n",
    "Perche': costruire una pipeline riproducibile che includa scaler, PCA e KMeans in un unico oggetto.\n",
    "Metodi: `Pipeline`, `StandardScaler`, `PCA(n_components=2)`, `KMeans(n_clusters=3)`.\n",
    "Checkpoint: nessun NaN, forma X_iris (150, 4), forma X_pca (150, 2), silhouette > 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139417db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3: Pipeline compatta su Iris\n",
    "# Intenzione: costruire una pipeline scalatore -> PCA -> KMeans e valutarla in 2D.\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "print(f\"Forma X_iris: {X_iris.shape}, y_iris: {y_iris.shape}\")\n",
    "assert not np.isnan(X_iris).any(), \"NaN trovati in Iris\"\n",
    "assert X_iris.shape == (150, 4), \"Forma inattesa per Iris\"\n",
    "\n",
    "pipe_iris = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=2, random_state=42)),\n",
    "    (\"kmeans\", KMeans(n_clusters=3, random_state=42, n_init=10)),\n",
    "])\n",
    "\n",
    "labels_iris = pipe_iris.fit_predict(X_iris)\n",
    "X_iris_pca = pipe_iris.named_steps[\"pca\"].transform(pipe_iris.named_steps[\"scaler\"].transform(X_iris))\n",
    "sil_iris = silhouette_score(X_iris_pca, labels_iris)\n",
    "print(f\"Forma dopo PCA: {X_iris_pca.shape}, Silhouette: {sil_iris:.3f}\")\n",
    "assert X_iris_pca.shape[1] == 2, \"La PCA non ha prodotto 2 componenti\"\n",
    "assert sil_iris > 0, \"Silhouette non positiva; rivedere n_clusters o n_components\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "scatter = ax.scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=labels_iris, cmap='viridis', s=30, alpha=0.8)\n",
    "ax.set_title('Iris: KMeans su PCA 2D')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003ad48",
   "metadata": {},
   "source": [
    "## Demo 4 - PCA + DBSCAN per outlier\n",
    "Perche': usare PCA per compattare i dati e facilitare DBSCAN nell'individuare aree di bassa densita' (outlier).\n",
    "Metodi: `make_blobs` per dati sintetici, aggiunta di rumore uniforme, `DBSCAN` su spazio PCA.\n",
    "Checkpoint: esistenza di etichette -1 (rumore), almeno 2 cluster non rumorosi, silhouette calcolata sui soli punti non rumorosi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 4: PCA + DBSCAN per outlier detection\n",
    "# Intenzione: usare PCA per compattare dati sintetici e permettere a DBSCAN di evidenziare il rumore.\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X_blobs, _ = make_blobs(n_samples=500, centers=4, n_features=6, cluster_std=1.2, random_state=42)\n",
    "noise = np.random.uniform(low=-8, high=8, size=(20, 6))\n",
    "X_mix = np.vstack([X_blobs, noise])\n",
    "print(f\"Forma dati con rumore: {X_mix.shape}\")\n",
    "assert not np.isnan(X_mix).any(), \"NaN nei dati sintetici\"\n",
    "\n",
    "pipe_dbscan = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=2, random_state=42)),\n",
    "])\n",
    "X_mix_pca = pipe_dbscan.fit_transform(X_mix)\n",
    "print(f\"Forma dopo PCA: {X_mix_pca.shape}\")\n",
    "assert X_mix_pca.shape[0] == X_mix.shape[0], \"Numero di campioni alterato dalla pipeline\"\n",
    "\n",
    "labels_dbscan = DBSCAN(eps=0.9, min_samples=5).fit_predict(X_mix_pca)\n",
    "unique_labels = np.unique(labels_dbscan)\n",
    "print(f\"Etichette trovate: {unique_labels}\")\n",
    "\n",
    "mask = labels_dbscan != -1\n",
    "if mask.sum() > 0 and np.unique(labels_dbscan[mask]).size > 1:\n",
    "    sil_db = silhouette_score(X_mix_pca[mask], labels_dbscan[mask])\n",
    "else:\n",
    "    sil_db = np.nan\n",
    "print(f\"Silhouette (solo cluster non rumore): {sil_db}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "scatter = ax.scatter(X_mix_pca[:, 0], X_mix_pca[:, 1], c=labels_dbscan, cmap='tab10', s=20, alpha=0.8)\n",
    "ax.set_title('DBSCAN su PCA 2D (rumore = -1)')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "plt.colorbar(scatter, ax=ax, label='Etichetta DBSCAN')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f34352",
   "metadata": {},
   "source": [
    "## Demo 5 - PCA 3D su Wine\n",
    "Perche': visualizzare i cluster su tre componenti e valutare quanta varianza manteniamo.\n",
    "Metodi: `PCA(n_components=3)` e `KMeans(n_clusters=3)`.\n",
    "Checkpoint: varianza cumulata > 0.60, silhouette > 0, grafico 3D con cluster separabili.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ce5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 5: PCA 3D + KMeans su Wine\n",
    "# Intenzione: visualizzare cluster Wine su 3 componenti principali e valutarne la separazione.\n",
    "from mpl_toolkits.mplot3d import Axes3D  # necessaria per il projection='3d'\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "X_wine, y_wine = load_wine(return_X_y=True)\n",
    "print(f\"Forma X_wine: {X_wine.shape}, y_wine: {y_wine.shape}\")\n",
    "assert not np.isnan(X_wine).any(), \"NaN trovati nel dataset Wine\"\n",
    "\n",
    "pipe_wine = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=3, random_state=42)),\n",
    "    (\"kmeans\", KMeans(n_clusters=3, random_state=42, n_init=10)),\n",
    "])\n",
    "labels_wine = pipe_wine.fit_predict(X_wine)\n",
    "X_wine_pca = pipe_wine.named_steps[\"pca\"].transform(pipe_wine.named_steps[\"scaler\"].transform(X_wine))\n",
    "var_wine = pipe_wine.named_steps[\"pca\"].explained_variance_ratio_.sum()\n",
    "sil_wine = silhouette_score(X_wine_pca, labels_wine)\n",
    "print(f\"Forma dopo PCA: {X_wine_pca.shape}, Varianza cumulata: {var_wine:.3f}, Silhouette: {sil_wine:.3f}\")\n",
    "assert var_wine >= 0.60, \"Varianza cumulata troppo bassa: aumenta n_components\"\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_wine_pca[:, 0], X_wine_pca[:, 1], X_wine_pca[:, 2], c=labels_wine, cmap='tab10', s=30, alpha=0.8)\n",
    "ax.set_title('Wine: KMeans su PCA 3D')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "fig.colorbar(scatter, ax=ax, label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20069db1",
   "metadata": {},
   "source": [
    "# 5) Esercizi svolti (passo-passo)\n",
    "## Esercizio 25.1 - Customer segmentation sintetica\n",
    "Obiettivo: generare un dataset sintetico 1000x20, ridurre al 90% di varianza con PCA e clusterizzare in 5 gruppi con KMeans.\n",
    "Perche': esercitarsi con un flusso end-to-end includendo controlli su forma, NaN e metriche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a38700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 25.1: Customer segmentation sintetica\n",
    "# Step-by-step: genera dati, scala, PCA al 90%, KMeans a 5 cluster, valuta silhouette/ARI.\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_synth, y_synth_true = make_blobs(n_samples=1000, centers=5, n_features=20, cluster_std=2.0, random_state=42)\n",
    "print(f\"Forma dati sintetici: {X_synth.shape}\")\n",
    "assert not np.isnan(X_synth).any(), \"NaN nei dati sintetici\"\n",
    "\n",
    "pipe_synth = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=0.90, random_state=42)),\n",
    "    (\"kmeans\", KMeans(n_clusters=5, random_state=42, n_init=10)),\n",
    "])\n",
    "\n",
    "labels_synth = pipe_synth.fit_predict(X_synth)\n",
    "X_synth_pca = pipe_synth.named_steps[\"pca\"].transform(pipe_synth.named_steps[\"scaler\"].transform(X_synth))\n",
    "var_synth = pipe_synth.named_steps[\"pca\"].explained_variance_ratio_.sum()\n",
    "sil_synth = silhouette_score(X_synth_pca, labels_synth)\n",
    "ari_synth = adjusted_rand_score(y_synth_true, labels_synth)\n",
    "print(f\"Componenti tenute: {X_synth_pca.shape[1]}, Varianza: {var_synth:.3f}, Silhouette: {sil_synth:.3f}, ARI: {ari_synth:.3f}\")\n",
    "\n",
    "# Sanity check: tutti i cluster presenti\n",
    "unique_labels = np.unique(labels_synth)\n",
    "print(f\"Cluster trovati: {unique_labels}\")\n",
    "assert len(unique_labels) == 5, \"Non sono stati trovati 5 cluster\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642d6d1",
   "metadata": {},
   "source": [
    "## Esercizio 25.2 - Confronto algoritmi su Digits\n",
    "Obiettivo: confrontare KMeans, AgglomerativeClustering e DBSCAN sullo spazio PCA (80% di varianza) del dataset Digits.\n",
    "Perche': mostrare come PCA impatta algoritmi diversi e come leggere ARI/silhouette, gestendo il rumore di DBSCAN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 25.2: Confronto algoritmi su Digits (PCA 80%)\n",
    "# Intenzione: confrontare KMeans, Agglomerative e DBSCAN dopo PCA, valutando silhouette e ARI.\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "scaler_80 = StandardScaler()\n",
    "pca_80 = PCA(n_components=0.80, random_state=42)\n",
    "X_digits_pca80 = pca_80.fit_transform(scaler_80.fit_transform(X_digits))\n",
    "print(f\"Forma PCA 80%: {X_digits_pca80.shape}, Varianza: {pca_80.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "models = [\n",
    "    (\"KMeans\", KMeans(n_clusters=10, random_state=42, n_init=10)),\n",
    "    (\"Agglomerative\", AgglomerativeClustering(n_clusters=10)),\n",
    "    (\"DBSCAN\", DBSCAN(eps=3.0, min_samples=5)),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for name, model in models:\n",
    "    labels = model.fit_predict(X_digits_pca80)\n",
    "    mask = labels != -1\n",
    "    if mask.sum() > 0 and np.unique(labels[mask]).size > 1:\n",
    "        sil = silhouette_score(X_digits_pca80[mask], labels[mask])\n",
    "    else:\n",
    "        sil = np.nan\n",
    "    ari = adjusted_rand_score(y_digits, labels)\n",
    "    rows.append({\"modello\": name, \"silhouette\": sil, \"ari\": ari, \"etichette_trovate\": np.unique(labels).size})\n",
    "\n",
    "res_models = pd.DataFrame(rows)\n",
    "print(res_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efa991",
   "metadata": {},
   "source": [
    "## Esercizio 25.3 - Mini tuning PCA + DBSCAN\n",
    "Obiettivo: cercare combinazioni di (n_components, eps, min_samples) che migliorano silhouette su Digits ridotto con PCA.\n",
    "Perche': illustrare un micro-grid manuale senza nuove librerie, con controlli di forma e cluster presenti.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f84a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 25.3: Mini tuning PCA + DBSCAN su Digits\n",
    "# Intenzione: esplorare manualmente combinazioni di n_components, eps e min_samples per migliorare silhouette.\n",
    "param_grid = {\n",
    "    \"n_components\": [0.70, 0.80, 0.90],\n",
    "    \"eps\": [2.5, 3.0, 3.5],\n",
    "    \"min_samples\": [3, 5],\n",
    "}\n",
    "\n",
    "best = None\n",
    "candidates = []\n",
    "for n_comp in param_grid[\"n_components\"]:\n",
    "    pca_tune = PCA(n_components=n_comp, random_state=42)\n",
    "    X_tune = pca_tune.fit_transform(scaler.fit_transform(X_digits))\n",
    "    for eps in param_grid[\"eps\"]:\n",
    "        for ms in param_grid[\"min_samples\"]:\n",
    "            labels = DBSCAN(eps=eps, min_samples=ms).fit_predict(X_tune)\n",
    "            mask = labels != -1\n",
    "            if mask.sum() < 2 or np.unique(labels[mask]).size < 2:\n",
    "                continue\n",
    "            sil = silhouette_score(X_tune[mask], labels[mask])\n",
    "            ari = adjusted_rand_score(y_digits, labels)\n",
    "            row = {\"n_components\": n_comp, \"eps\": eps, \"min_samples\": ms, \"silhouette\": sil, \"ari\": ari, \"etichette_trovate\": np.unique(labels).size}\n",
    "            candidates.append(row)\n",
    "            if best is None or sil > best[\"silhouette\"]:\n",
    "                best = row\n",
    "\n",
    "if candidates:\n",
    "    df_tuning = pd.DataFrame(candidates).sort_values(by=\"silhouette\", ascending=False)\n",
    "    print(\"Top 5 combinazioni per silhouette:\")\n",
    "    print(df_tuning.head())\n",
    "    print(\"Migliore combinazione:\")\n",
    "    print(best)\n",
    "else:\n",
    "    print(\"Nessuna combinazione ha prodotto almeno due cluster non rumorosi.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d436c5",
   "metadata": {},
   "source": [
    "# 6) Conclusione\n",
    "\n",
    "## 5 take-home messages\n",
    "\n",
    "| # | Messaggio | Perché importante |\n",
    "|---|-----------|-------------------|\n",
    "| 1 | **PCA riduce dimensionalità, non elimina dati** | 95% varianza in meno PC = stesso segnale, meno rumore |\n",
    "| 2 | **Sempre confrontare baseline vs PCA** | Non assumere che PCA migliori sempre |\n",
    "| 3 | **Il numero di PC va scelto con criterio** | Elbow su varianza cumulata o validazione su silhouette |\n",
    "| 4 | **DBSCAN beneficia molto di PCA** | In alta D l'eps perde senso, in bassa D funziona |\n",
    "| 5 | **I loadings collegano PC a feature** | PC1 alto su \"alcol\"? Il cluster \"alto-PC1\" è ricco di alcol |\n",
    "\n",
    "---\n",
    "\n",
    "## Confronto sintetico: quando usare quale metodo\n",
    "\n",
    "| Clustering | Senza PCA | Con PCA | Raccomandazione |\n",
    "|------------|-----------|---------|-----------------|\n",
    "| K-Means | OK se D<20 | Ottimo sempre | Usa PCA se D>15 |\n",
    "| Gerarchico | Lento su grandi D | Molto più veloce | PCA quasi obbligatorio |\n",
    "| DBSCAN | eps difficile | eps più stabile | PCA consigliato |\n",
    "\n",
    "---\n",
    "\n",
    "## Perché questi metodi funzionano\n",
    "\n",
    "### 1) PCA come \"lente denoiser\"\n",
    "\n",
    "```\n",
    "Varianza originale:    Varianza dopo PCA:\n",
    "\n",
    "Feature1: ████████     PC1: ████████████████████ (40%)\n",
    "Feature2: ███████      PC2: ██████████████ (25%)\n",
    "Feature3: ██████       PC3: █████████ (15%)\n",
    "Feature4: █████        PC4: █████ (8%)\n",
    "Feature5: ███          ... → resto = rumore\n",
    "... 60 altre           Somma 95%: solo 4 PC bastano!\n",
    "```\n",
    "\n",
    "PCA concentra la **varianza utile** nei primi PC, isolando il rumore nei PC residui.\n",
    "\n",
    "### 2) Perché clustering migliora in bassa dimensionalità\n",
    "\n",
    "In alta dimensionalità:\n",
    "- **Curse of dimensionality:** le distanze convergono (tutto sembra equidistante)\n",
    "- **Volumi crescono esponenzialmente:** i punti si \"diluiscono\"\n",
    "\n",
    "In bassa dimensionalità:\n",
    "- Le distanze discriminano veramente\n",
    "- K-Means trova centroidi più stabili\n",
    "- DBSCAN trova eps significativi\n",
    "\n",
    "---\n",
    "\n",
    "## Errori comuni da evitare\n",
    "\n",
    "| Errore | Perché sbagliato | Fix |\n",
    "|--------|-----------------|-----|\n",
    "| PCA senza scaling | Feature con range ampio dominano | StandardScaler prima |\n",
    "| Troppi PC (90% varianza se 50 PC) | Lento e poco miglioramento | Prova 95% o meno PC |\n",
    "| Non confrontare con baseline | Magari senza PCA era meglio | Calcola metriche entrambi |\n",
    "| Interpretare PC come feature | PC sono combinazioni lineari | Usa loadings per interpretare |\n",
    "\n",
    "---\n",
    "\n",
    "## Template completo PCA + Clustering\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Carica e scala\n",
    "X = ...  # tua matrice feature\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2) Baseline senza PCA\n",
    "km_raw = KMeans(n_clusters=5, random_state=42, n_init='auto')\n",
    "labels_raw = km_raw.fit_predict(X_scaled)\n",
    "sil_raw = silhouette_score(X_scaled, labels_raw)\n",
    "\n",
    "# 3) PCA: scegli n_components\n",
    "pca = PCA().fit(X_scaled)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_opt = np.argmax(cumsum >= 0.90) + 1  # 90% varianza\n",
    "print(f\"PC per 90% varianza: {n_opt}\")\n",
    "\n",
    "# 4) Applica PCA\n",
    "pca_final = PCA(n_components=n_opt, random_state=42)\n",
    "X_pca = pca_final.fit_transform(X_scaled)\n",
    "\n",
    "# 5) Clustering su X_pca\n",
    "km_pca = KMeans(n_clusters=5, random_state=42, n_init='auto')\n",
    "labels_pca = km_pca.fit_predict(X_pca)\n",
    "sil_pca = silhouette_score(X_pca, labels_pca)\n",
    "\n",
    "# 6) Confronto\n",
    "print(f\"Silhouette RAW: {sil_raw:.3f}\")\n",
    "print(f\"Silhouette PCA: {sil_pca:.3f}\")\n",
    "print(f\"Miglioramento: {sil_pca - sil_raw:.3f}\")\n",
    "\n",
    "# 7) Interpreta con loadings\n",
    "loadings = pd.DataFrame(\n",
    "    pca_final.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(n_opt)],\n",
    "    index=feature_names  # nomi feature originali\n",
    ")\n",
    "print(loadings['PC1'].abs().nlargest(5))  # top feature su PC1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Prossimi passi\n",
    "\n",
    "| Lezione | Argomento | Collegamento |\n",
    "|---------|-----------|--------------|\n",
    "| 26 | Anomaly Detection | Cluster outlier, isolation forest |\n",
    "| Future | UMAP/t-SNE | Riduzioni non-lineari per visualizzazione |\n",
    "| Future | Deep Clustering | Autoencoders + K-Means |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea097e",
   "metadata": {},
   "source": [
    "# 7) Checklist di fine lezione\n",
    "- [ ] Ho scalato i dati prima di PCA e clustering.\n",
    "- [ ] Ho scelto `n_components` confrontando varianza e metriche (silhouette/ARI).\n",
    "- [ ] Ho verificato forme coerenti dopo PCA e che non ci siano NaN.\n",
    "- [ ] Ho controllato tempi di fit con e senza PCA.\n",
    "- [ ] Ho interpretato i cluster guardando le componenti e i loadings piu' pesanti.\n",
    "- [ ] Ho gestito il rumore di DBSCAN verificando la presenza di etichette -1.\n",
    "\n",
    "Glossario (termini usati nel notebook):\n",
    "- PCA: tecnica di riduzione dimensionale basata su varianza massima.\n",
    "- Principal component (PC): nuova variabile lineare che massimizza la varianza.\n",
    "- Varianza spiegata: quota di varianza coperta dalle componenti scelte.\n",
    "- Scree plot: grafico della varianza spiegata per componente.\n",
    "- Loadings: pesi delle feature originali su ogni componente.\n",
    "- n_components: numero di componenti mantenute o soglia di varianza.\n",
    "- Silhouette score: metrica di coesione/separazione dei cluster.\n",
    "- Adjusted Rand Index (ARI): accordo tra etichette previste e vere.\n",
    "- KMeans: clustering per centroidi con n_clusters fissato.\n",
    "- DBSCAN: clustering per densita' con etichetta -1 per il rumore.\n",
    "- AgglomerativeClustering: clustering gerarchico bottom-up.\n",
    "- Scaling: trasformazione per avere media 0 e deviazione standard 1 per ogni feature.\n",
    "- Punti rumore: punti etichettati -1 da DBSCAN.\n",
    "- eps: raggio di vicinanza usato da DBSCAN.\n",
    "- min_samples: numero minimo di vicini per formare un core point in DBSCAN.\n",
    "- Curse of dimensionality: fenomeno di distanze meno informative in spazi ad alta dimensione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Changelog didattico\n",
    "\n",
    "| Versione | Data | Modifiche |\n",
    "|----------|------|-----------|\n",
    "| 1.0 | 2024-01-15 | Creazione: PCA + K-Means, demo Digits, confronto |\n",
    "| 1.1 | 2024-01-22 | Aggiunto DBSCAN in spazio PCA, Wine dataset |\n",
    "| 2.0 | 2024-02-01 | Integrata pipeline completa Gerarchico + Iris |\n",
    "| 2.1 | 2024-02-10 | Refactor codice, commenti più chiari |\n",
    "| **2.3** | **2024-12-19** | **ESPANSIONE COMPLETA:** mappa lezione, tabella obiettivi, ASCII workflow curse of dimensionality, confronto metodi, 5 take-home messages, template completo PCA + clustering con scelta automatica n_components, errori comuni, loadings interpretation |\n",
    "\n",
    "---\n",
    "\n",
    "## Note per lo studente\n",
    "\n",
    "Questa lezione chiude il cerchio **riduzione + segmentazione**:\n",
    "\n",
    "1. **PCA** (Lezione 24) → comprime\n",
    "2. **Clustering** (Lezioni 20-23) → segmenta\n",
    "3. **PCA + Clustering** (questa lezione) → combina per robustezza\n",
    "\n",
    "Il workflow è ormai consolidato: **Scale → PCA → Cluster → Validate → Interpret**.\n",
    "\n",
    "**Prossima tappa:** Lesson 26 (Anomaly Detection) - identificare outlier con metodi supervisionati e non."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
