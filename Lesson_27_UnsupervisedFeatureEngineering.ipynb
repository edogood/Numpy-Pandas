{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ae6d78",
   "metadata": {},
   "source": [
    "# ğŸ“ Lezione 27 â€” Unsupervised Feature Engineering\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Obiettivi di Apprendimento\n",
    "\n",
    "| Obiettivo | Descrizione |\n",
    "|-----------|-------------|\n",
    "| **Comprendere** | Il ruolo del Feature Engineering nell'Unsupervised Learning |\n",
    "| **Applicare** | Tecniche di trasformazione e creazione feature |\n",
    "| **Combinare** | PCA, clustering, e altre tecniche per features migliori |\n",
    "| **Valutare** | L'impatto delle nuove features sulla qualitÃ  dei modelli |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Concetti Chiave\n",
    "\n",
    "> **Feature Engineering Unsupervised**: Creare nuove feature usando tecniche non supervisionate, senza bisogno di etichette.\n",
    "\n",
    "### ğŸ”— Prerequisiti\n",
    "- Lezione 24: PCA (riduzione dimensionalitÃ )\n",
    "- Lezione 25: PCA + Clustering\n",
    "- Lezione 26: Anomaly Detection\n",
    "\n",
    "### ğŸ› ï¸ Cosa Costruiremo\n",
    "1. Features basate su PCA (componenti, varianza spiegata)\n",
    "2. Features basate su clustering (distanze, appartenenza)\n",
    "3. Features basate su anomaly detection (scores)\n",
    "4. Feature Selection automatica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462abee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“– 1. Teoria\n",
    "\n",
    "## 1.1 Cos'Ã¨ il Feature Engineering Unsupervised?\n",
    "\n",
    "### Il Problema\n",
    "Nel Machine Learning, le **feature originali** raramente sono nella forma ottimale per i modelli.\n",
    "\n",
    "```\n",
    "Feature Engineering = Arte di trasformare i dati grezzi in feature informative\n",
    "```\n",
    "\n",
    "### Supervisionato vs Unsupervised\n",
    "\n",
    "| Aspetto | Supervisionato | Unsupervised |\n",
    "|---------|----------------|--------------|\n",
    "| **Obiettivo** | Predire target | Scoprire struttura |\n",
    "| **Guida** | Label y | Nessuna label |\n",
    "| **Validazione** | Error vs target | Metriche interne |\n",
    "| **Rischio** | Overfitting su y | Pattern spurii |\n",
    "\n",
    "### PerchÃ© Unsupervised per Feature Engineering?\n",
    "\n",
    "1. **Nessuna label necessaria** â†’ Utilizzabile su dati non etichettati\n",
    "2. **Scoperta automatica** â†’ Il modello trova pattern nascosti\n",
    "3. **Feature-dense** â†’ Una componente PCA cattura info da molte feature\n",
    "4. **Preprocessing universale** â†’ Utile sia per classificazione che regressione\n",
    "\n",
    "### Tipologie di Features Derivate\n",
    "\n",
    "| Fonte | Tipo di Feature | Esempio |\n",
    "|-------|-----------------|---------|\n",
    "| PCA | Componenti | PC1, PC2, varianza cumulata |\n",
    "| Clustering | Appartenenza | Cluster label, distanza da centroide |\n",
    "| Anomaly | Score | Isolation score, LOF score |\n",
    "| Autoencoders | Encoding | Rappresentazione latente |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d2fc9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 Features da PCA\n",
    "\n",
    "### Concetto Base\n",
    "PCA proietta i dati in un nuovo spazio dove le **componenti catturano massima varianza**.\n",
    "\n",
    "### Features Derivabili\n",
    "\n",
    "| Feature | Formula | Uso |\n",
    "|---------|---------|-----|\n",
    "| PC_i | X @ V[:, i] | Nuove feature decorrelate |\n",
    "| Varianza cumulata | sum(Î»_1:k) / sum(Î») | QualitÃ  della riduzione |\n",
    "| Reconstruction Error | ||X - X_reconstructed|| | Anomaly detection |\n",
    "| Loadings | V[j, i] | Importanza feature j per PC_i |\n",
    "\n",
    "### Strategia: Componenti Come Features\n",
    "\n",
    "```\n",
    "Dati originali: 100 feature correlate\n",
    "â†“ PCA\n",
    "Nuove feature: 10 componenti decorrelate\n",
    "â†“ Modello\n",
    "Migliore generalizzazione\n",
    "```\n",
    "\n",
    "### Vantaggi\n",
    "1. **Decorrelazione** â†’ Migliora modelli sensibili a multicollinearitÃ \n",
    "2. **Rumore ridotto** â†’ Ultime PC catturano rumore\n",
    "3. **InterpretabilitÃ ** â†’ Poche componenti spiegano molto\n",
    "4. **VelocitÃ ** â†’ Meno feature = training piÃ¹ veloce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf62c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 Features da Clustering\n",
    "\n",
    "### Concetto Base\n",
    "Il clustering raggruppa dati simili. Possiamo usare questa informazione per creare feature.\n",
    "\n",
    "### Features Derivabili\n",
    "\n",
    "| Feature | Descrizione | Calcolo |\n",
    "|---------|-------------|---------|\n",
    "| **Cluster Label** | Appartenenza al cluster | kmeans.predict(X) |\n",
    "| **Distance to Centroid** | Distanza dal centro del proprio cluster | dist(x, centroid_i) |\n",
    "| **Distance to All** | Distanza da tutti i centroidi | [dist(x, c_0), dist(x, c_1), ...] |\n",
    "| **Soft Assignment** | ProbabilitÃ  di appartenenza | GaussianMixture.predict_proba() |\n",
    "| **Nearest Neighbor** | Distanza dal vicino piÃ¹ vicino | NearestNeighbors |\n",
    "\n",
    "### PerchÃ© Funziona?\n",
    "\n",
    "```\n",
    "Dati clienti â†’ K-Means con k=5\n",
    "â†“\n",
    "Cluster 0: \"Alto spenditori\"\n",
    "Cluster 1: \"Occasionali\"\n",
    "...\n",
    "â†“\n",
    "Nuova feature categorica: customer_segment\n",
    "```\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "1. **Segmentazione clienti** â†’ Feature per propensity models\n",
    "2. **Anomaly context** â†’ Anomalo rispetto al proprio cluster\n",
    "3. **Multi-grain analysis** â†’ Cluster a diversi livelli k\n",
    "4. **Semi-supervised** â†’ Propagare poche label al cluster intero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8fc5ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 Features da Anomaly Detection\n",
    "\n",
    "### Concetto Base\n",
    "Gli algoritmi di anomaly detection producono **score** che indicano quanto un'osservazione Ã¨ anomala.\n",
    "\n",
    "### Features Derivabili\n",
    "\n",
    "| Algoritmo | Feature | Interpretazione |\n",
    "|-----------|---------|-----------------|\n",
    "| Isolation Forest | `decision_function()` | Score isolamento (basso = anomalo) |\n",
    "| LOF | `negative_outlier_factor_` | LOF score (basso = anomalo) |\n",
    "| One-Class SVM | `decision_function()` | Distanza dal boundary |\n",
    "| Autoencoder | Reconstruction Error | Errore ricostruzione |\n",
    "\n",
    "### Uso Come Feature\n",
    "\n",
    "```python\n",
    "# Creare feature da anomaly score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(contamination=0.05)\n",
    "iso.fit(X)\n",
    "\n",
    "# Nuova feature: anomaly_score\n",
    "X['anomaly_score'] = iso.decision_function(X)\n",
    "X['is_anomaly'] = (iso.predict(X) == -1).astype(int)\n",
    "```\n",
    "\n",
    "### PerchÃ© Ã¨ Utile?\n",
    "\n",
    "1. **Outlier-aware models** â†’ Il modello \"sa\" quali punti sono strani\n",
    "2. **Feature importance** â†’ Anomaly score correlato con errori\n",
    "3. **Data quality** â†’ Identificare dati problematici\n",
    "4. **Ensemble diversity** â†’ Combinare con altre feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73be78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 Feature Selection Unsupervised\n",
    "\n",
    "### Il Problema\n",
    "Troppe feature â‰  migliori risultati. Alcune feature sono **ridondanti** o **rumorose**.\n",
    "\n",
    "### Metodi di Selezione Unsupervised\n",
    "\n",
    "| Metodo | Criterio | Pro | Contro |\n",
    "|--------|----------|-----|--------|\n",
    "| **Variance Threshold** | Rimuove bassa varianza | Semplice | Ignora correlazioni |\n",
    "| **Correlation Filter** | Rimuove altamente correlate | Riduce ridondanza | Soglia arbitraria |\n",
    "| **PCA Loadings** | Seleziona per importanza in PC | Data-driven | Meno interpretabile |\n",
    "| **Clustering Features** | Rappresentante per cluster | Riduce gruppi simili | Scelta rappresentante |\n",
    "\n",
    "### Variance Threshold\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Rimuove feature con varianza < 0.1\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "X_selected = selector.fit_transform(X)\n",
    "```\n",
    "\n",
    "### Correlation Filter\n",
    "\n",
    "```python\n",
    "# Matrice correlazione\n",
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# Upper triangle\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Colonne da rimuovere (correlazione > 0.95)\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "```\n",
    "\n",
    "### Schema Mentale\n",
    "\n",
    "```\n",
    "1. Rimuovi costanti (var=0)\n",
    "2. Rimuovi quasi-costanti (var<threshold)\n",
    "3. Rimuovi altamente correlate (>0.95)\n",
    "4. (Opzionale) Seleziona top-k per loadings PCA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6cc58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§  Schema Mentale â€” Workflow Feature Engineering Unsupervised\n",
    "\n",
    "```\n",
    "                    DATI GREZZI\n",
    "                         â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â–¼                â–¼                â–¼\n",
    "   Preprocessing    Feature Selection  Missing/Outliers\n",
    "   (Scaling)        (VarianceThreshold)  (handling)\n",
    "        â”‚                â”‚                â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  FEATURE GENERATION  â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼                    â–¼                    â–¼\n",
    "  PCA               Clustering            Anomaly\n",
    "  - Components      - Labels              - Scores\n",
    "  - Loadings        - Distances           - Binary flags\n",
    "  - Rec. Error      - Probabilities\n",
    "    â”‚                    â”‚                    â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚   FEATURE FUSION     â”‚\n",
    "              â”‚  (Concatenazione)    â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  POST-SELECTION      â”‚\n",
    "              â”‚  (Correlation Filter)â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "               FEATURES FINALI\n",
    "```\n",
    "\n",
    "### âœ… Checklist\n",
    "\n",
    "- [ ] Dati scalati (StandardScaler)?\n",
    "- [ ] Feature a bassa varianza rimosse?\n",
    "- [ ] Componenti PCA aggiunte?\n",
    "- [ ] Feature cluster aggiunte?\n",
    "- [ ] Anomaly scores aggiunti?\n",
    "- [ ] Correlazioni elevate rimosse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ’» DEMO 1: Features da PCA â€” Componenti e Loadings\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Creare nuove feature usando PCA\n",
    "- Componenti principali come features\n",
    "- Analisi dei loadings per interpretabilitÃ \n",
    "- Reconstruction error come feature\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Configurazione\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. CREAZIONE DATASET CON MOLTE FEATURES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š CREAZIONE DATASET CON MOLTE FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset con 20 feature, ma solo alcune informative\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=20,\n",
    "    n_informative=5,\n",
    "    n_redundant=10,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "feature_names = [f'feature_{i}' for i in range(20)]\n",
    "df_original = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "print(f\"\\nğŸ“ Shape originale: {df_original.shape}\")\n",
    "print(f\"ğŸ“‹ Features: {list(df_original.columns)[:5]}... (20 totali)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. SCALING E PCA\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”„ SCALING E PCA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_original)\n",
    "\n",
    "# PCA completa per analisi\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Varianza spiegata\n",
    "print(\"\\nğŸ“ˆ Varianza spiegata per componente:\")\n",
    "for i, (var, cum_var) in enumerate(zip(\n",
    "    pca_full.explained_variance_ratio_[:10],\n",
    "    np.cumsum(pca_full.explained_variance_ratio_)[:10]\n",
    ")):\n",
    "    print(f\"  PC{i+1}: {var:.3f} (cumulata: {cum_var:.3f})\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. FEATURE ENGINEERING: COMPONENTI PCA\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”§ CREAZIONE FEATURES DA PCA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Seleziono 5 componenti (spiegano ~80% varianza)\n",
    "n_components = 5\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Nuove feature: componenti\n",
    "pca_features = pd.DataFrame(\n",
    "    X_pca,\n",
    "    columns=[f'PC_{i+1}' for i in range(n_components)]\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Create {n_components} features PCA:\")\n",
    "print(pca_features.head())\n",
    "\n",
    "# Varianza spiegata totale\n",
    "print(f\"\\nğŸ“Š Varianza spiegata totale: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4. FEATURE ENGINEERING: LOADINGS (IMPORTANZA)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š ANALISI LOADINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Loadings matrix\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC_{i+1}' for i in range(n_components)],\n",
    "    index=feature_names\n",
    ")\n",
    "\n",
    "# Top 5 feature per PC1\n",
    "print(\"\\nğŸ” Top 5 feature per importanza in PC1:\")\n",
    "top_features_pc1 = loadings['PC_1'].abs().sort_values(ascending=False)\n",
    "print(top_features_pc1.head())\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5. FEATURE ENGINEERING: RECONSTRUCTION ERROR\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”„ RECONSTRUCTION ERROR COME FEATURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ricostruzione\n",
    "X_reconstructed = pca.inverse_transform(X_pca)\n",
    "\n",
    "# Errore per ogni sample\n",
    "reconstruction_error = np.sqrt(np.sum((X_scaled - X_reconstructed) ** 2, axis=1))\n",
    "\n",
    "print(f\"\\nğŸ“Š Reconstruction Error Statistics:\")\n",
    "print(f\"  Min: {reconstruction_error.min():.3f}\")\n",
    "print(f\"  Max: {reconstruction_error.max():.3f}\")\n",
    "print(f\"  Mean: {reconstruction_error.mean():.3f}\")\n",
    "print(f\"  Std: {reconstruction_error.std():.3f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 6. DATASET FINALE CON NUOVE FEATURES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“¦ DATASET FINALE ARRICCHITO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combino tutto\n",
    "df_enriched = pca_features.copy()\n",
    "df_enriched['reconstruction_error'] = reconstruction_error\n",
    "\n",
    "print(f\"\\nğŸ“ Shape originale: {df_original.shape}\")\n",
    "print(f\"ğŸ“ Shape arricchito: {df_enriched.shape}\")\n",
    "print(f\"\\nğŸ“‹ Nuove features: {list(df_enriched.columns)}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# VISUALIZZAZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Varianza spiegata\n",
    "ax1 = axes[0]\n",
    "ax1.bar(range(1, 11), pca_full.explained_variance_ratio_[:10], alpha=0.7, label='Individuale')\n",
    "ax1.plot(range(1, 11), np.cumsum(pca_full.explained_variance_ratio_)[:10], \n",
    "         'ro-', label='Cumulata')\n",
    "ax1.axhline(y=0.8, color='g', linestyle='--', label='80%')\n",
    "ax1.set_xlabel('Componente')\n",
    "ax1.set_ylabel('Varianza Spiegata')\n",
    "ax1.set_title('ğŸ“Š Varianza Spiegata per Componente')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Loadings PC1\n",
    "ax2 = axes[1]\n",
    "colors = ['green' if x > 0 else 'red' for x in loadings['PC_1']]\n",
    "ax2.barh(range(len(loadings)), loadings['PC_1'], color=colors, alpha=0.7)\n",
    "ax2.set_yticks(range(len(loadings)))\n",
    "ax2.set_yticklabels(loadings.index, fontsize=8)\n",
    "ax2.set_xlabel('Loading')\n",
    "ax2.set_title('ğŸ“ˆ Loadings della Prima Componente (PC1)')\n",
    "\n",
    "# Plot 3: Reconstruction Error Distribution\n",
    "ax3 = axes[2]\n",
    "ax3.hist(reconstruction_error, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax3.axvline(x=reconstruction_error.mean(), color='red', linestyle='--', \n",
    "            label=f'Media: {reconstruction_error.mean():.2f}')\n",
    "ax3.set_xlabel('Reconstruction Error')\n",
    "ax3.set_ylabel('Frequenza')\n",
    "ax3.set_title('ğŸ”„ Distribuzione Reconstruction Error')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… DEMO 1 COMPLETATA!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "ğŸ“ RIEPILOGO - Features create da PCA:\n",
    "1. PC_1, PC_2, ... PC_5 â†’ Componenti principali (decorrelate)\n",
    "2. reconstruction_error â†’ Quanto bene PCA ricostruisce il punto\n",
    "\n",
    "ğŸ’¡ USO: Sostituisci le 20 feature originali con 6 features derivate!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a1703",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ’» Demo 2: Features da Clustering â€” Labels e Distanze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da6b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ’» DEMO 2: Features da Clustering â€” Labels e Distanze\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Creare nuove feature usando K-Means\n",
    "- Cluster labels come feature categorica\n",
    "- Distanze dai centroidi\n",
    "- ProbabilitÃ  di appartenenza (soft clustering)\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. DATASET E CLUSTERING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š FEATURES DA CLUSTERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Uso lo stesso dataset scalato dalla demo precedente\n",
    "# X_scaled giÃ  disponibile\n",
    "\n",
    "# K-Means con 4 cluster\n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"\\nğŸ“Š K-Means con k={n_clusters}\")\n",
    "print(f\"ğŸ“‹ Distribuzione cluster:\")\n",
    "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "for c, cnt in zip(unique, counts):\n",
    "    print(f\"  Cluster {c}: {cnt} samples ({cnt/len(cluster_labels)*100:.1f}%)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. FEATURE: CLUSTER LABEL (One-Hot Encoding)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ·ï¸ FEATURE 1: CLUSTER LABEL (ONE-HOT)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# One-hot encoding del cluster\n",
    "cluster_onehot = pd.get_dummies(cluster_labels, prefix='cluster')\n",
    "print(\"\\nğŸ“‹ One-Hot Encoding:\")\n",
    "print(cluster_onehot.head(10))\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. FEATURE: DISTANZA DA OGNI CENTROIDE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ FEATURE 2: DISTANZA DAI CENTROIDI\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calcolo distanze da tutti i centroidi\n",
    "centroids = kmeans.cluster_centers_\n",
    "distances_to_centroids = cdist(X_scaled, centroids, metric='euclidean')\n",
    "\n",
    "dist_features = pd.DataFrame(\n",
    "    distances_to_centroids,\n",
    "    columns=[f'dist_to_cluster_{i}' for i in range(n_clusters)]\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“‹ Distanze dai centroidi:\")\n",
    "print(dist_features.head())\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4. FEATURE: DISTANZA DAL PROPRIO CENTROIDE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ FEATURE 3: DISTANZA DAL PROPRIO CENTROIDE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Distanza dal centroide del proprio cluster\n",
    "own_centroid_distance = np.array([\n",
    "    distances_to_centroids[i, cluster_labels[i]] \n",
    "    for i in range(len(cluster_labels))\n",
    "])\n",
    "\n",
    "print(f\"\\nğŸ“Š Statistiche distanza dal proprio centroide:\")\n",
    "print(f\"  Min: {own_centroid_distance.min():.3f}\")\n",
    "print(f\"  Max: {own_centroid_distance.max():.3f}\")\n",
    "print(f\"  Mean: {own_centroid_distance.mean():.3f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5. FEATURE: PROBABILITÃ€ SOFT (GAUSSIAN MIXTURE)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š FEATURE 4: SOFT CLUSTERING (GMM)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Gaussian Mixture per probabilitÃ \n",
    "gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "gmm.fit(X_scaled)\n",
    "\n",
    "# ProbabilitÃ  di appartenenza\n",
    "probs = gmm.predict_proba(X_scaled)\n",
    "prob_features = pd.DataFrame(\n",
    "    probs,\n",
    "    columns=[f'prob_cluster_{i}' for i in range(n_clusters)]\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“‹ ProbabilitÃ  di appartenenza (GMM):\")\n",
    "print(prob_features.head())\n",
    "print(f\"\\nâœ… Verifica: somma righe = {prob_features.sum(axis=1).mean():.2f} (dovrebbe essere 1.0)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 6. MULTI-GRAIN CLUSTERING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” FEATURE 5: MULTI-GRAIN CLUSTERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cluster a diversi livelli di granularitÃ \n",
    "multi_grain = {}\n",
    "for k in [2, 4, 8]:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    multi_grain[f'cluster_k{k}'] = km.fit_predict(X_scaled)\n",
    "    \n",
    "multi_grain_df = pd.DataFrame(multi_grain)\n",
    "print(\"\\nğŸ“‹ Clustering a diverse granularitÃ :\")\n",
    "print(multi_grain_df.head(10))\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 7. DATASET FINALE CON FEATURES CLUSTERING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“¦ DATASET ARRICCHITO CON FEATURES CLUSTERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combino features piÃ¹ utili\n",
    "df_cluster_features = pd.DataFrame({\n",
    "    'cluster_label': cluster_labels,\n",
    "    'dist_to_own_centroid': own_centroid_distance,\n",
    "    'dist_to_cluster_0': distances_to_centroids[:, 0],\n",
    "    'dist_to_cluster_1': distances_to_centroids[:, 1],\n",
    "    'dist_to_cluster_2': distances_to_centroids[:, 2],\n",
    "    'dist_to_cluster_3': distances_to_centroids[:, 3],\n",
    "    'prob_cluster_0': probs[:, 0],\n",
    "    'prob_cluster_1': probs[:, 1],\n",
    "    'prob_cluster_2': probs[:, 2],\n",
    "    'prob_cluster_3': probs[:, 3],\n",
    "    'cluster_k2': multi_grain['cluster_k{}'.format(2)],\n",
    "    'cluster_k8': multi_grain['cluster_k{}'.format(8)]\n",
    "})\n",
    "\n",
    "print(f\"\\nğŸ“ Shape features clustering: {df_cluster_features.shape}\")\n",
    "print(f\"\\nğŸ“‹ Colonne: {list(df_cluster_features.columns)}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# VISUALIZZAZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Cluster in 2D (prime 2 PC)\n",
    "ax1 = axes[0]\n",
    "scatter = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, \n",
    "                       cmap='viridis', alpha=0.7, s=30)\n",
    "# Centroidi proiettati\n",
    "centroids_pca = pca.transform(centroids)\n",
    "ax1.scatter(centroids_pca[:, 0], centroids_pca[:, 1], \n",
    "            c='red', marker='X', s=200, edgecolors='black', linewidths=2)\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.set_title('ğŸ¯ Cluster in Spazio PCA')\n",
    "plt.colorbar(scatter, ax=ax1, label='Cluster')\n",
    "\n",
    "# Plot 2: Distanza dal proprio centroide\n",
    "ax2 = axes[1]\n",
    "for i in range(n_clusters):\n",
    "    mask = cluster_labels == i\n",
    "    ax2.hist(own_centroid_distance[mask], bins=20, alpha=0.5, label=f'Cluster {i}')\n",
    "ax2.set_xlabel('Distanza dal Centroide')\n",
    "ax2.set_ylabel('Frequenza')\n",
    "ax2.set_title('ğŸ“ Distribuzione Distanze per Cluster')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: ProbabilitÃ  GMM (primi 20 samples)\n",
    "ax3 = axes[2]\n",
    "prob_sample = prob_features.iloc[:20]\n",
    "bottom = np.zeros(20)\n",
    "for i in range(n_clusters):\n",
    "    ax3.bar(range(20), prob_sample[f'prob_cluster_{i}'], bottom=bottom, \n",
    "            label=f'Cluster {i}', alpha=0.8)\n",
    "    bottom += prob_sample[f'prob_cluster_{i}'].values\n",
    "ax3.set_xlabel('Sample')\n",
    "ax3.set_ylabel('ProbabilitÃ ')\n",
    "ax3.set_title('ğŸ“Š Soft Assignment (primi 20 samples)')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… DEMO 2 COMPLETATA!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "ğŸ“ RIEPILOGO - Features create da Clustering:\n",
    "1. cluster_label â†’ Appartenenza (categorica)\n",
    "2. dist_to_own_centroid â†’ Quanto Ã¨ \"tipico\" nel suo cluster\n",
    "3. dist_to_cluster_X â†’ Distanze da tutti i centroidi\n",
    "4. prob_cluster_X â†’ ProbabilitÃ  soft (GMM)\n",
    "5. cluster_k2, k8 â†’ Multi-grain clustering\n",
    "\n",
    "ğŸ’¡ USO: Aggiunge informazione sulla struttura locale dei dati!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ea4c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ’» Demo 3: Features da Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606cc946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ’» DEMO 3: Features da Anomaly Detection\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Creare features usando Anomaly Scores\n",
    "- Isolation Forest score\n",
    "- LOF score\n",
    "- Binary anomaly flag\n",
    "- Score combination\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. ISOLATION FOREST FEATURES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸŒ² FEATURES DA ISOLATION FOREST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Isolation Forest\n",
    "iso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
    "iso_pred = iso.fit_predict(X_scaled)\n",
    "iso_score = iso.decision_function(X_scaled)\n",
    "\n",
    "print(f\"\\nğŸ“Š Isolation Forest Results:\")\n",
    "print(f\"  Anomalie rilevate: {(iso_pred == -1).sum()} ({(iso_pred == -1).mean()*100:.1f}%)\")\n",
    "print(f\"\\nğŸ“ˆ Score Statistics:\")\n",
    "print(f\"  Min: {iso_score.min():.3f}\")\n",
    "print(f\"  Max: {iso_score.max():.3f}\")\n",
    "print(f\"  Mean: {iso_score.mean():.3f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. LOF FEATURES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” FEATURES DA LOCAL OUTLIER FACTOR\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LOF\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "lof_pred = lof.fit_predict(X_scaled)\n",
    "lof_score = -lof.negative_outlier_factor_  # Convertiamo in positivo per coerenza\n",
    "\n",
    "print(f\"\\nğŸ“Š LOF Results:\")\n",
    "print(f\"  Anomalie rilevate: {(lof_pred == -1).sum()} ({(lof_pred == -1).mean()*100:.1f}%)\")\n",
    "print(f\"\\nğŸ“ˆ Score Statistics:\")\n",
    "print(f\"  Min: {lof_score.min():.3f}\")\n",
    "print(f\"  Max: {lof_score.max():.3f}\")\n",
    "print(f\"  Mean: {lof_score.mean():.3f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. NORMALIZZAZIONE SCORES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ NORMALIZZAZIONE SCORES [0, 1]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def normalize_score(score):\n",
    "    \"\"\"Normalizza score a range [0, 1]\"\"\"\n",
    "    return (score - score.min()) / (score.max() - score.min())\n",
    "\n",
    "# Normalizzati (1 = piÃ¹ anomalo)\n",
    "iso_score_norm = 1 - normalize_score(iso_score)  # Invertiamo perchÃ© basso = anomalo\n",
    "lof_score_norm = normalize_score(lof_score)       # Alto = anomalo giÃ  OK\n",
    "\n",
    "print(f\"\\nğŸ“Š Scores normalizzati:\")\n",
    "print(f\"  ISO range: [{iso_score_norm.min():.3f}, {iso_score_norm.max():.3f}]\")\n",
    "print(f\"  LOF range: [{lof_score_norm.min():.3f}, {lof_score_norm.max():.3f}]\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4. FEATURES COMBINATE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”— FEATURES COMBINATE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combinazioni\n",
    "avg_anomaly_score = (iso_score_norm + lof_score_norm) / 2\n",
    "max_anomaly_score = np.maximum(iso_score_norm, lof_score_norm)\n",
    "consensus_anomaly = ((iso_pred == -1) & (lof_pred == -1)).astype(int)\n",
    "\n",
    "print(f\"\\nğŸ“Š Consensus (entrambi dicono anomalia): {consensus_anomaly.sum()} samples\")\n",
    "print(f\"ğŸ“Š Solo ISO anomalia: {((iso_pred == -1) & (lof_pred == 1)).sum()} samples\")\n",
    "print(f\"ğŸ“Š Solo LOF anomalia: {((iso_pred == 1) & (lof_pred == -1)).sum()} samples\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5. DATASET CON FEATURES ANOMALY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“¦ DATASET CON FEATURES ANOMALY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_anomaly_features = pd.DataFrame({\n",
    "    'iso_score': iso_score,                    # Score raw\n",
    "    'iso_score_norm': iso_score_norm,          # Score normalizzato\n",
    "    'iso_is_anomaly': (iso_pred == -1).astype(int),\n",
    "    'lof_score': lof_score,\n",
    "    'lof_score_norm': lof_score_norm,\n",
    "    'lof_is_anomaly': (lof_pred == -1).astype(int),\n",
    "    'avg_anomaly_score': avg_anomaly_score,    # Media scores\n",
    "    'max_anomaly_score': max_anomaly_score,    # Max (piÃ¹ conservativo)\n",
    "    'consensus_anomaly': consensus_anomaly      # Entrambi d'accordo\n",
    "})\n",
    "\n",
    "print(f\"\\nğŸ“ Shape: {df_anomaly_features.shape}\")\n",
    "print(f\"\\nğŸ“‹ Features create:\")\n",
    "for col in df_anomaly_features.columns:\n",
    "    print(f\"  â€¢ {col}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# VISUALIZZAZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Distribuzione scores\n",
    "ax1 = axes[0]\n",
    "ax1.hist(iso_score_norm, bins=30, alpha=0.5, label='ISO', color='blue')\n",
    "ax1.hist(lof_score_norm, bins=30, alpha=0.5, label='LOF', color='orange')\n",
    "ax1.axvline(x=np.percentile(iso_score_norm, 95), color='blue', linestyle='--')\n",
    "ax1.axvline(x=np.percentile(lof_score_norm, 95), color='orange', linestyle='--')\n",
    "ax1.set_xlabel('Anomaly Score (normalizzato)')\n",
    "ax1.set_ylabel('Frequenza')\n",
    "ax1.set_title('ğŸ“Š Distribuzione Anomaly Scores')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Scatter ISO vs LOF\n",
    "ax2 = axes[1]\n",
    "colors = ['red' if c == 1 else 'blue' for c in consensus_anomaly]\n",
    "ax2.scatter(iso_score_norm, lof_score_norm, c=colors, alpha=0.5, s=20)\n",
    "ax2.axhline(y=0.9, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.axvline(x=0.9, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Isolation Forest Score')\n",
    "ax2.set_ylabel('LOF Score')\n",
    "ax2.set_title('ğŸ” ISO vs LOF (rosso = consensus)')\n",
    "\n",
    "# Plot 3: Anomalie in spazio PCA\n",
    "ax3 = axes[2]\n",
    "normal_mask = consensus_anomaly == 0\n",
    "anomaly_mask = consensus_anomaly == 1\n",
    "ax3.scatter(X_pca[normal_mask, 0], X_pca[normal_mask, 1], \n",
    "            c='blue', alpha=0.3, s=20, label='Normale')\n",
    "ax3.scatter(X_pca[anomaly_mask, 0], X_pca[anomaly_mask, 1], \n",
    "            c='red', alpha=0.8, s=50, marker='X', label='Anomalia')\n",
    "ax3.set_xlabel('PC1')\n",
    "ax3.set_ylabel('PC2')\n",
    "ax3.set_title('ğŸ¯ Anomalie in Spazio PCA')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… DEMO 3 COMPLETATA!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "ğŸ“ RIEPILOGO - Features create da Anomaly Detection:\n",
    "1. iso_score / lof_score â†’ Score raw\n",
    "2. iso_score_norm / lof_score_norm â†’ Score normalizzato [0,1]\n",
    "3. iso_is_anomaly / lof_is_anomaly â†’ Flag binario\n",
    "4. avg_anomaly_score â†’ Media dei due scores\n",
    "5. max_anomaly_score â†’ Score piÃ¹ conservativo\n",
    "6. consensus_anomaly â†’ Entrambi d'accordo\n",
    "\n",
    "ğŸ’¡ USO: Segnala al modello quali punti sono \"strani\"!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252e813",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ’» Demo 4: Feature Selection Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88016f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ’» DEMO 4: Feature Selection Unsupervised\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Selezionare le feature piÃ¹ informative senza usare labels\n",
    "- Variance Threshold\n",
    "- Correlation Filter\n",
    "- PCA-based selection\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. DATASET CON FEATURES PROBLEMATICHE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š DATASET CON FEATURES PROBLEMATICHE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Creo dataset con problemi tipici\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "df_problem = pd.DataFrame({\n",
    "    # Features buone\n",
    "    'good_1': np.random.randn(n),\n",
    "    'good_2': np.random.randn(n) * 2 + 5,\n",
    "    'good_3': np.random.randn(n) * 0.5,\n",
    "    \n",
    "    # Feature costante\n",
    "    'constant': np.ones(n) * 42,\n",
    "    \n",
    "    # Feature quasi-costante\n",
    "    'low_var': np.random.choice([0, 1], size=n, p=[0.99, 0.01]),\n",
    "    \n",
    "    # Features altamente correlate\n",
    "    'corr_1': np.random.randn(n),\n",
    "})\n",
    "df_problem['corr_2'] = df_problem['corr_1'] * 0.95 + np.random.randn(n) * 0.1\n",
    "df_problem['corr_3'] = df_problem['corr_1'] * -0.98 + np.random.randn(n) * 0.05\n",
    "\n",
    "print(f\"\\nğŸ“ Shape iniziale: {df_problem.shape}\")\n",
    "print(f\"\\nğŸ“‹ Colonne: {list(df_problem.columns)}\")\n",
    "print(f\"\\nğŸ“Š Varianze:\")\n",
    "for col in df_problem.columns:\n",
    "    print(f\"  {col}: {df_problem[col].var():.6f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. VARIANCE THRESHOLD\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ STEP 1: VARIANCE THRESHOLD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prima scalo per confrontare varianze\n",
    "scaler_sel = StandardScaler()\n",
    "X_scaled_sel = scaler_sel.fit_transform(df_problem)\n",
    "X_scaled_sel_df = pd.DataFrame(X_scaled_sel, columns=df_problem.columns)\n",
    "\n",
    "# Varianze dopo scaling\n",
    "print(\"\\nğŸ“Š Varianze dopo StandardScaler (dovrebbero essere ~1):\")\n",
    "for col in X_scaled_sel_df.columns:\n",
    "    var = X_scaled_sel_df[col].var()\n",
    "    status = \"âŒ BASSA\" if var < 0.01 else \"âœ…\"\n",
    "    print(f\"  {col}: {var:.6f} {status}\")\n",
    "\n",
    "# Variance threshold\n",
    "var_selector = VarianceThreshold(threshold=0.01)\n",
    "X_var_selected = var_selector.fit_transform(X_scaled_sel_df)\n",
    "selected_mask = var_selector.get_support()\n",
    "selected_cols = df_problem.columns[selected_mask].tolist()\n",
    "\n",
    "print(f\"\\nâœ‚ï¸ Rimosse features con varianza < 0.01:\")\n",
    "removed = [c for c, s in zip(df_problem.columns, selected_mask) if not s]\n",
    "print(f\"  Rimosse: {removed}\")\n",
    "print(f\"  Rimaste: {selected_cols}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. CORRELATION FILTER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”— STEP 2: CORRELATION FILTER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Lavoro solo con le feature sopravvissute\n",
    "X_after_var = pd.DataFrame(X_var_selected, columns=selected_cols)\n",
    "\n",
    "# Matrice correlazione\n",
    "corr_matrix = X_after_var.corr().abs()\n",
    "print(\"\\nğŸ“Š Matrice Correlazione (valori assoluti):\")\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "# Upper triangle\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Trova colonne con correlazione > 0.9\n",
    "threshold_corr = 0.9\n",
    "to_drop_corr = []\n",
    "for col in upper.columns:\n",
    "    highly_corr = upper[col][upper[col] > threshold_corr].index.tolist()\n",
    "    if highly_corr:\n",
    "        print(f\"\\nâš ï¸ {col} altamente correlata con: {highly_corr}\")\n",
    "        # Rimuovi le successive (mantieni la prima)\n",
    "        for hc in highly_corr:\n",
    "            if hc not in to_drop_corr:\n",
    "                to_drop_corr.append(hc)\n",
    "\n",
    "print(f\"\\nâœ‚ï¸ Features da rimuovere (corr > {threshold_corr}): {to_drop_corr}\")\n",
    "\n",
    "# Applica filtro\n",
    "X_after_corr = X_after_var.drop(columns=to_drop_corr)\n",
    "print(f\"\\nâœ… Features finali: {list(X_after_corr.columns)}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4. PIPELINE COMPLETA\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”§ PIPELINE FEATURE SELECTION COMPLETA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def unsupervised_feature_selection(df, var_threshold=0.01, corr_threshold=0.9, verbose=True):\n",
    "    \"\"\"\n",
    "    Pipeline completa di feature selection unsupervised\n",
    "    \n",
    "    Returns:\n",
    "        selected_df: DataFrame con feature selezionate\n",
    "        report: Dizionario con dettagli selezione\n",
    "    \"\"\"\n",
    "    report = {'original_features': list(df.columns)}\n",
    "    \n",
    "    # Step 1: Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    \n",
    "    # Step 2: Variance Threshold\n",
    "    var_sel = VarianceThreshold(threshold=var_threshold)\n",
    "    X_var = var_sel.fit_transform(X_scaled)\n",
    "    var_mask = var_sel.get_support()\n",
    "    var_cols = df.columns[var_mask].tolist()\n",
    "    report['after_variance'] = var_cols\n",
    "    report['removed_low_var'] = [c for c, m in zip(df.columns, var_mask) if not m]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"ğŸ“ Dopo Variance Threshold: {len(var_cols)}/{len(df.columns)} features\")\n",
    "    \n",
    "    # Step 3: Correlation Filter\n",
    "    X_var_df = pd.DataFrame(X_var, columns=var_cols)\n",
    "    corr = X_var_df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    \n",
    "    to_drop = []\n",
    "    for col in upper.columns:\n",
    "        if any(upper[col] > corr_threshold):\n",
    "            to_drop.append(col)\n",
    "    \n",
    "    final_cols = [c for c in var_cols if c not in to_drop]\n",
    "    report['after_correlation'] = final_cols\n",
    "    report['removed_high_corr'] = to_drop\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"ğŸ”— Dopo Correlation Filter: {len(final_cols)}/{len(var_cols)} features\")\n",
    "        print(f\"âœ… Features finali: {final_cols}\")\n",
    "    \n",
    "    return X_var_df[final_cols], report\n",
    "\n",
    "# Eseguo\n",
    "X_selected, report = unsupervised_feature_selection(df_problem)\n",
    "\n",
    "print(f\"\\nğŸ“Š REPORT:\")\n",
    "print(f\"  Originali: {len(report['original_features'])}\")\n",
    "print(f\"  Dopo variance: {len(report['after_variance'])}\")\n",
    "print(f\"  Dopo correlation: {len(report['after_correlation'])}\")\n",
    "print(f\"  Rimosse (bassa var): {report['removed_low_var']}\")\n",
    "print(f\"  Rimosse (alta corr): {report['removed_high_corr']}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# VISUALIZZAZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Varianze originali\n",
    "ax1 = axes[0]\n",
    "variances = df_problem.var().sort_values(ascending=False)\n",
    "colors = ['red' if v < 0.01 else 'green' for v in variances]\n",
    "ax1.barh(range(len(variances)), variances.values, color=colors)\n",
    "ax1.set_yticks(range(len(variances)))\n",
    "ax1.set_yticklabels(variances.index)\n",
    "ax1.axvline(x=0.01, color='black', linestyle='--', label='Threshold')\n",
    "ax1.set_xlabel('Varianza')\n",
    "ax1.set_title('ğŸ“Š Varianze Features (rosso = da rimuovere)')\n",
    "\n",
    "# Plot 2: Correlation Heatmap (prima)\n",
    "ax2 = axes[1]\n",
    "im = ax2.imshow(X_after_var.corr(), cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "ax2.set_xticks(range(len(X_after_var.columns)))\n",
    "ax2.set_yticks(range(len(X_after_var.columns)))\n",
    "ax2.set_xticklabels(X_after_var.columns, rotation=45, ha='right')\n",
    "ax2.set_yticklabels(X_after_var.columns)\n",
    "ax2.set_title('ğŸ”— Correlazioni (prima del filtro)')\n",
    "plt.colorbar(im, ax=ax2)\n",
    "\n",
    "# Plot 3: Features prima e dopo\n",
    "ax3 = axes[2]\n",
    "categories = ['Originali', 'Post-Variance', 'Post-Correlation']\n",
    "counts = [len(report['original_features']), \n",
    "          len(report['after_variance']), \n",
    "          len(report['after_correlation'])]\n",
    "colors = ['lightblue', 'steelblue', 'darkblue']\n",
    "bars = ax3.bar(categories, counts, color=colors)\n",
    "ax3.set_ylabel('Numero Features')\n",
    "ax3.set_title('ğŸ“‰ Riduzione Features')\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             str(count), ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… DEMO 4 COMPLETATA!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "ğŸ“ RIEPILOGO - Feature Selection Unsupervised:\n",
    "1. Variance Threshold â†’ Rimuove costanti e quasi-costanti\n",
    "2. Correlation Filter â†’ Rimuove ridondanti (>0.9 correlate)\n",
    "3. Pipeline â†’ Combina entrambi in un workflow\n",
    "\n",
    "ğŸ’¡ USO: Pulisci il dataset PRIMA di creare nuove features!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274412c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ’» Demo 5: Pipeline Completa di Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸ’» DEMO 5: Pipeline Completa di Feature Engineering\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "OBIETTIVO: Combinare TUTTE le tecniche in una pipeline end-to-end\n",
    "- Dataset originale â†’ Features arricchite\n",
    "- PCA + Clustering + Anomaly + Selection\n",
    "\"\"\"\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. CLASSE FEATURE ENGINEERING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”§ PIPELINE COMPLETA FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class UnsupervisedFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Pipeline completa di Feature Engineering Unsupervised.\n",
    "    \n",
    "    Genera features da:\n",
    "    - PCA (componenti, reconstruction error)\n",
    "    - K-Means (label, distanze)\n",
    "    - Anomaly Detection (scores)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_pca_components=5, n_clusters=4, contamination=0.05):\n",
    "        self.n_pca_components = n_pca_components\n",
    "        self.n_clusters = n_clusters\n",
    "        self.contamination = contamination\n",
    "        \n",
    "        # Modelli\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=n_pca_components)\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        self.iso = IsolationForest(contamination=contamination, random_state=42)\n",
    "        \n",
    "        self.fitted = False\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit tutti i modelli\"\"\"\n",
    "        # Scaling\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Fit modelli\n",
    "        self.pca.fit(X_scaled)\n",
    "        self.kmeans.fit(X_scaled)\n",
    "        self.iso.fit(X_scaled)\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Genera tutte le features\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Chiamare fit() prima di transform()\")\n",
    "        \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        features = {}\n",
    "        \n",
    "        # === PCA Features ===\n",
    "        X_pca = self.pca.transform(X_scaled)\n",
    "        for i in range(self.n_pca_components):\n",
    "            features[f'pca_{i+1}'] = X_pca[:, i]\n",
    "        \n",
    "        # Reconstruction error\n",
    "        X_reconstructed = self.pca.inverse_transform(X_pca)\n",
    "        features['pca_rec_error'] = np.sqrt(np.sum((X_scaled - X_reconstructed) ** 2, axis=1))\n",
    "        \n",
    "        # === Clustering Features ===\n",
    "        features['cluster_label'] = self.kmeans.predict(X_scaled)\n",
    "        \n",
    "        # Distanza dal proprio centroide\n",
    "        distances = cdist(X_scaled, self.kmeans.cluster_centers_)\n",
    "        features['dist_to_centroid'] = np.array([\n",
    "            distances[i, features['cluster_label'][i]] \n",
    "            for i in range(len(X_scaled))\n",
    "        ])\n",
    "        \n",
    "        # Distanze da tutti i centroidi\n",
    "        for i in range(self.n_clusters):\n",
    "            features[f'dist_cluster_{i}'] = distances[:, i]\n",
    "        \n",
    "        # === Anomaly Features ===\n",
    "        features['iso_score'] = self.iso.decision_function(X_scaled)\n",
    "        features['is_anomaly'] = (self.iso.predict(X_scaled) == -1).astype(int)\n",
    "        \n",
    "        return pd.DataFrame(features)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit e transform in un passo\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Ritorna nomi delle features generate\"\"\"\n",
    "        names = []\n",
    "        names += [f'pca_{i+1}' for i in range(self.n_pca_components)]\n",
    "        names.append('pca_rec_error')\n",
    "        names.append('cluster_label')\n",
    "        names.append('dist_to_centroid')\n",
    "        names += [f'dist_cluster_{i}' for i in range(self.n_clusters)]\n",
    "        names += ['iso_score', 'is_anomaly']\n",
    "        return names\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. APPLICAZIONE PIPELINE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\nğŸ“Š Dataset originale:\")\n",
    "print(f\"  Shape: {df_original.shape}\")\n",
    "\n",
    "# Creo pipeline\n",
    "engineer = UnsupervisedFeatureEngineer(\n",
    "    n_pca_components=5,\n",
    "    n_clusters=4,\n",
    "    contamination=0.05\n",
    ")\n",
    "\n",
    "# Applico\n",
    "df_engineered = engineer.fit_transform(df_original)\n",
    "\n",
    "print(f\"\\nâœ¨ Dataset arricchito:\")\n",
    "print(f\"  Shape: {df_engineered.shape}\")\n",
    "print(f\"\\nğŸ“‹ Features generate:\")\n",
    "for col in df_engineered.columns:\n",
    "    print(f\"  â€¢ {col}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. COMBINO CON ORIGINALI (OPZIONALE)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“¦ COMBINAZIONE FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Opzione 1: Solo features generate (sostituzione)\n",
    "df_option1 = df_engineered.copy()\n",
    "print(f\"\\nğŸ“ Opzione 1 (solo generate): {df_option1.shape}\")\n",
    "\n",
    "# Opzione 2: Originali + generate (augmentation)\n",
    "df_option2 = pd.concat([df_original.reset_index(drop=True), df_engineered], axis=1)\n",
    "print(f\"ğŸ“ Opzione 2 (originali + generate): {df_option2.shape}\")\n",
    "\n",
    "# Opzione 3: Selezione mista\n",
    "df_option3 = df_engineered[['pca_1', 'pca_2', 'pca_3', 'cluster_label', \n",
    "                             'dist_to_centroid', 'iso_score']].copy()\n",
    "print(f\"ğŸ“ Opzione 3 (selezione): {df_option3.shape}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4. STATISTICHE FEATURES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š STATISTICHE FEATURES GENERATE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ“ˆ Statistiche descrittive:\")\n",
    "print(df_engineered.describe().round(3))\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# VISUALIZZAZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: PCA space con cluster\n",
    "ax1 = axes[0, 0]\n",
    "scatter = ax1.scatter(df_engineered['pca_1'], df_engineered['pca_2'], \n",
    "                       c=df_engineered['cluster_label'], cmap='viridis', \n",
    "                       alpha=0.7, s=30)\n",
    "ax1.set_xlabel('PCA 1')\n",
    "ax1.set_ylabel('PCA 2')\n",
    "ax1.set_title('ğŸ¯ Spazio PCA colorato per Cluster')\n",
    "plt.colorbar(scatter, ax=ax1, label='Cluster')\n",
    "\n",
    "# Plot 2: Distanza dal centroide per cluster\n",
    "ax2 = axes[0, 1]\n",
    "for c in range(4):\n",
    "    mask = df_engineered['cluster_label'] == c\n",
    "    ax2.hist(df_engineered.loc[mask, 'dist_to_centroid'], bins=15, \n",
    "             alpha=0.5, label=f'Cluster {c}')\n",
    "ax2.set_xlabel('Distanza dal Centroide')\n",
    "ax2.set_ylabel('Frequenza')\n",
    "ax2.set_title('ğŸ“ Distanze per Cluster')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Anomaly score vs reconstruction error\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['red' if a == 1 else 'blue' for a in df_engineered['is_anomaly']]\n",
    "ax3.scatter(df_engineered['iso_score'], df_engineered['pca_rec_error'], \n",
    "            c=colors, alpha=0.5, s=20)\n",
    "ax3.set_xlabel('Isolation Score')\n",
    "ax3.set_ylabel('Reconstruction Error')\n",
    "ax3.set_title('ğŸ” ISO Score vs Rec. Error (rosso=anomalia)')\n",
    "\n",
    "# Plot 4: Correlation heatmap features generate\n",
    "ax4 = axes[1, 1]\n",
    "# Seleziono solo features numeriche continue\n",
    "numeric_cols = ['pca_1', 'pca_2', 'pca_3', 'pca_rec_error', \n",
    "                'dist_to_centroid', 'iso_score']\n",
    "corr = df_engineered[numeric_cols].corr()\n",
    "im = ax4.imshow(corr, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "ax4.set_xticks(range(len(numeric_cols)))\n",
    "ax4.set_yticks(range(len(numeric_cols)))\n",
    "ax4.set_xticklabels(numeric_cols, rotation=45, ha='right', fontsize=8)\n",
    "ax4.set_yticklabels(numeric_cols, fontsize=8)\n",
    "ax4.set_title('ğŸ”— Correlazione Features Generate')\n",
    "plt.colorbar(im, ax=ax4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… DEMO 5 COMPLETATA!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "ğŸ“ RIEPILOGO - Pipeline Completa:\n",
    "\n",
    "CLASSE UnsupervisedFeatureEngineer:\n",
    "â€¢ fit(X) â†’ Allena tutti i modelli\n",
    "â€¢ transform(X) â†’ Genera features\n",
    "â€¢ fit_transform(X) â†’ Entrambi\n",
    "\n",
    "FEATURES GENERATE:\n",
    "â€¢ pca_1...pca_n â†’ Componenti PCA\n",
    "â€¢ pca_rec_error â†’ Errore ricostruzione\n",
    "â€¢ cluster_label â†’ Etichetta cluster\n",
    "â€¢ dist_to_centroid â†’ Distanza dal centro\n",
    "â€¢ dist_cluster_X â†’ Distanza da tutti i centri\n",
    "â€¢ iso_score â†’ Anomaly score\n",
    "â€¢ is_anomaly â†’ Flag binario\n",
    "\n",
    "ğŸ’¡ USO: Una classe riutilizzabile per qualsiasi progetto!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83e75a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# âœï¸ 3. Esercizi Pratici\n",
    "\n",
    "## Esercizio 27.1 â€” Feature Engineering per Segmentazione Clienti\n",
    "\n",
    "### ğŸ“‹ Problema\n",
    "Hai un dataset di comportamento clienti. Devi creare features arricchite per una successiva segmentazione.\n",
    "\n",
    "### ğŸ¯ Tasks\n",
    "1. Carica il dataset simulato (fornito nel codice)\n",
    "2. Applica PCA e estrai le prime 3 componenti\n",
    "3. Esegui K-Means con k=3 ed estrai:\n",
    "   - Cluster label\n",
    "   - Distanza dal proprio centroide\n",
    "4. Aggiungi anomaly score con Isolation Forest\n",
    "5. Crea un DataFrame finale con tutte le nuove features\n",
    "\n",
    "### ğŸ’¡ Suggerimenti\n",
    "- Scala sempre i dati prima di applicare PCA e clustering\n",
    "- Normalizza gli anomaly scores per renderli comparabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb183b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# âœ… SOLUZIONE ESERCIZIO 27.1 â€” Feature Engineering Segmentazione Clienti\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. CREAZIONE DATASET CLIENTI\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š ESERCIZIO 27.1 â€” SEGMENTAZIONE CLIENTI\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_customers = 400\n",
    "\n",
    "# Simulo 3 segmenti di clienti\n",
    "segment_sizes = [150, 150, 100]  # Alto, Medio, Low value\n",
    "\n",
    "df_customers = pd.DataFrame({\n",
    "    # Segment 1: High Value\n",
    "    'monthly_spending': np.concatenate([\n",
    "        np.random.normal(500, 100, segment_sizes[0]),  # Alto\n",
    "        np.random.normal(150, 50, segment_sizes[1]),   # Medio\n",
    "        np.random.normal(30, 15, segment_sizes[2])     # Basso\n",
    "    ]),\n",
    "    'visit_frequency': np.concatenate([\n",
    "        np.random.normal(15, 3, segment_sizes[0]),\n",
    "        np.random.normal(8, 2, segment_sizes[1]),\n",
    "        np.random.normal(2, 1, segment_sizes[2])\n",
    "    ]),\n",
    "    'avg_basket_size': np.concatenate([\n",
    "        np.random.normal(80, 15, segment_sizes[0]),\n",
    "        np.random.normal(40, 10, segment_sizes[1]),\n",
    "        np.random.normal(20, 8, segment_sizes[2])\n",
    "    ]),\n",
    "    'days_since_last_purchase': np.concatenate([\n",
    "        np.random.exponential(5, segment_sizes[0]),    # Frequenti\n",
    "        np.random.exponential(15, segment_sizes[1]),   # Occasionali\n",
    "        np.random.exponential(45, segment_sizes[2])    # Rari\n",
    "    ]),\n",
    "    'online_engagement': np.concatenate([\n",
    "        np.random.normal(70, 15, segment_sizes[0]),\n",
    "        np.random.normal(45, 20, segment_sizes[1]),\n",
    "        np.random.normal(15, 10, segment_sizes[2])\n",
    "    ])\n",
    "})\n",
    "\n",
    "# Aggiungo alcune anomalie\n",
    "anomaly_idx = [0, 50, 200, 350]\n",
    "df_customers.loc[anomaly_idx, 'monthly_spending'] *= 3\n",
    "\n",
    "print(f\"\\nğŸ“ Dataset clienti: {df_customers.shape}\")\n",
    "print(f\"\\nğŸ“‹ Features:\")\n",
    "for col in df_customers.columns:\n",
    "    print(f\"  â€¢ {col}: min={df_customers[col].min():.1f}, max={df_customers[col].max():.1f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. PREPROCESSING: SCALING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”„ SCALING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_customers)\n",
    "\n",
    "print(\"âœ… Dati scalati con StandardScaler\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. FEATURES PCA\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š FEATURES PCA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Features PCA\n",
    "pca_features = pd.DataFrame({\n",
    "    'PC1': X_pca[:, 0],\n",
    "    'PC2': X_pca[:, 1],\n",
    "    'PC3': X_pca[:, 2]\n",
    "})\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Varianza spiegata per componente:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.1%}\")\n",
    "print(f\"  Totale: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4. FEATURES CLUSTERING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ¯ FEATURES CLUSTERING (K-MEANS k=3)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Distanze\n",
    "centroids = kmeans.cluster_centers_\n",
    "distances = cdist(X_scaled, centroids)\n",
    "dist_own = np.array([distances[i, cluster_labels[i]] for i in range(len(X_scaled))])\n",
    "\n",
    "# Features clustering\n",
    "cluster_features = pd.DataFrame({\n",
    "    'customer_segment': cluster_labels,\n",
    "    'dist_to_segment_center': dist_own\n",
    "})\n",
    "\n",
    "print(f\"\\nğŸ“Š Distribuzione segmenti:\")\n",
    "for c in range(3):\n",
    "    count = (cluster_labels == c).sum()\n",
    "    print(f\"  Segmento {c}: {count} clienti ({count/len(cluster_labels)*100:.1f}%)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5. FEATURES ANOMALY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” FEATURES ANOMALY DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso_pred = iso.fit_predict(X_scaled)\n",
    "iso_score = iso.decision_function(X_scaled)\n",
    "\n",
    "# Normalizzazione score [0, 1] dove 1 = piÃ¹ anomalo\n",
    "iso_score_norm = 1 - (iso_score - iso_score.min()) / (iso_score.max() - iso_score.min())\n",
    "\n",
    "# Features anomaly\n",
    "anomaly_features = pd.DataFrame({\n",
    "    'anomaly_score': iso_score_norm,\n",
    "    'is_outlier': (iso_pred == -1).astype(int)\n",
    "})\n",
    "\n",
    "print(f\"\\nâš ï¸ Anomalie rilevate: {anomaly_features['is_outlier'].sum()} clienti\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 6. DATAFRAME FINALE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“¦ DATASET FINALE ARRICCHITO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combino tutto\n",
    "df_enriched = pd.concat([\n",
    "    df_customers.reset_index(drop=True),\n",
    "    pca_features,\n",
    "    cluster_features,\n",
    "    anomaly_features\n",
    "], axis=1)\n",
    "\n",
    "print(f\"\\nğŸ“ Shape originale: {df_customers.shape}\")\n",
    "print(f\"ğŸ“ Shape arricchito: {df_enriched.shape}\")\n",
    "print(f\"\\nğŸ“‹ Nuove features aggiunte:\")\n",
    "new_features = list(pca_features.columns) + list(cluster_features.columns) + list(anomaly_features.columns)\n",
    "for f in new_features:\n",
    "    print(f\"  â€¢ {f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# VISUALIZZAZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Segmenti in spazio PCA\n",
    "ax1 = axes[0]\n",
    "scatter = ax1.scatter(df_enriched['PC1'], df_enriched['PC2'], \n",
    "                       c=df_enriched['customer_segment'], cmap='viridis', alpha=0.6)\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.set_title('ğŸ¯ Segmenti Clienti in Spazio PCA')\n",
    "plt.colorbar(scatter, ax=ax1, label='Segmento')\n",
    "\n",
    "# Plot 2: Anomaly score distribution\n",
    "ax2 = axes[1]\n",
    "for seg in range(3):\n",
    "    mask = df_enriched['customer_segment'] == seg\n",
    "    ax2.hist(df_enriched.loc[mask, 'anomaly_score'], bins=20, alpha=0.5, label=f'Segmento {seg}')\n",
    "ax2.axvline(x=0.9, color='red', linestyle='--', label='Soglia anomalia')\n",
    "ax2.set_xlabel('Anomaly Score')\n",
    "ax2.set_ylabel('Frequenza')\n",
    "ax2.set_title('ğŸ“Š Distribuzione Anomaly Score per Segmento')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Spending vs Anomaly Score\n",
    "ax3 = axes[2]\n",
    "colors = ['red' if o == 1 else 'blue' for o in df_enriched['is_outlier']]\n",
    "ax3.scatter(df_enriched['monthly_spending'], df_enriched['anomaly_score'], \n",
    "            c=colors, alpha=0.5, s=20)\n",
    "ax3.set_xlabel('Monthly Spending (â‚¬)')\n",
    "ax3.set_ylabel('Anomaly Score')\n",
    "ax3.set_title('ğŸ’° Spending vs Anomaly (rosso = outlier)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sample output\n",
    "print(\"\\nğŸ“‹ Esempio prime 5 righe arricchite:\")\n",
    "print(df_enriched[new_features].head().round(3))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… ESERCIZIO 27.1 COMPLETATO!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ea672f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Esercizio 27.2 â€” Feature Selection su Dataset Ridondante\n",
    "\n",
    "### ğŸ“‹ Problema\n",
    "Un dataset contiene molte features ridondanti e correlate. Devi selezionare le features piÃ¹ informative.\n",
    "\n",
    "### ğŸ¯ Tasks\n",
    "1. Crea un dataset con features problematiche (fornito)\n",
    "2. Rimuovi features con varianza < 0.01 (dopo scaling)\n",
    "3. Rimuovi features con correlazione > 0.85\n",
    "4. Confronta dimensionalitÃ  prima e dopo\n",
    "\n",
    "### ğŸ’¡ Suggerimenti\n",
    "- Lavora sui dati scalati per confrontare varianze\n",
    "- Mantieni sempre la \"prima\" delle features correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9fa8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# âœ… SOLUZIONE ESERCIZIO 27.2 â€” Feature Selection\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. CREAZIONE DATASET PROBLEMATICO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š ESERCIZIO 27.2 â€” FEATURE SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n = 300\n",
    "\n",
    "# Features base\n",
    "base_feat = np.random.randn(n)\n",
    "\n",
    "df_redundant = pd.DataFrame({\n",
    "    # Features buone\n",
    "    'feat_a': np.random.randn(n) * 2,\n",
    "    'feat_b': np.random.randn(n) * 1.5 + 3,\n",
    "    'feat_c': np.random.exponential(2, n),\n",
    "    \n",
    "    # Features costanti/quasi-costanti\n",
    "    'const_1': np.ones(n) * 10,\n",
    "    'quasi_const': np.random.choice([0, 1], n, p=[0.995, 0.005]),\n",
    "    \n",
    "    # Features altamente correlate\n",
    "    'corr_base': base_feat,\n",
    "    'corr_1': base_feat * 1.02 + np.random.randn(n) * 0.05,\n",
    "    'corr_2': base_feat * 0.98 + np.random.randn(n) * 0.08,\n",
    "    'corr_3': -base_feat * 1.01 + np.random.randn(n) * 0.03,  # Correlazione negativa\n",
    "    \n",
    "    # Altra feature buona\n",
    "    'feat_d': np.random.randn(n) * 3 - 2\n",
    "})\n",
    "\n",
    "print(f\"\\nğŸ“ Shape iniziale: {df_redundant.shape}\")\n",
    "print(f\"\\nğŸ“‹ Features problematiche attese:\")\n",
    "print(\"  â€¢ const_1: costante\")\n",
    "print(\"  â€¢ quasi_const: varianza quasi zero\")\n",
    "print(\"  â€¢ corr_1, corr_2, corr_3: correlate con corr_base\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. SCALING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”„ STEP 1: SCALING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_redundant)\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=df_redundant.columns)\n",
    "\n",
    "print(\"\\nğŸ“Š Varianze dopo scaling:\")\n",
    "for col in df_scaled.columns:\n",
    "    var = df_scaled[col].var()\n",
    "    status = \"âš ï¸ BASSA\" if var < 0.01 else \"âœ…\"\n",
    "    print(f\"  {col}: {var:.6f} {status}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. VARIANCE THRESHOLD\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ STEP 2: VARIANCE THRESHOLD (< 0.01)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "var_selector = VarianceThreshold(threshold=0.01)\n",
    "X_after_var = var_selector.fit_transform(df_scaled)\n",
    "\n",
    "# Features mantenute\n",
    "var_mask = var_selector.get_support()\n",
    "cols_after_var = df_scaled.columns[var_mask].tolist()\n",
    "cols_removed_var = df_scaled.columns[~var_mask].tolist()\n",
    "\n",
    "print(f\"\\nâœ‚ï¸ Rimosse: {cols_removed_var}\")\n",
    "print(f\"âœ… Mantenute: {cols_after_var}\")\n",
    "print(f\"\\nğŸ“ Shape: {df_scaled.shape} â†’ {X_after_var.shape}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4. CORRELATION FILTER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”— STEP 3: CORRELATION FILTER (> 0.85)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_after_var = pd.DataFrame(X_after_var, columns=cols_after_var)\n",
    "\n",
    "# Matrice correlazione\n",
    "corr_matrix = df_after_var.corr().abs()\n",
    "print(\"\\nğŸ“Š Matrice correlazione (abs):\")\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "# Upper triangle\n",
    "upper_tri = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Trova colonne da rimuovere\n",
    "threshold = 0.85\n",
    "cols_to_remove = set()\n",
    "\n",
    "for col in upper_tri.columns:\n",
    "    highly_corr = upper_tri[col][upper_tri[col] > threshold].index.tolist()\n",
    "    if highly_corr:\n",
    "        print(f\"\\nâš ï¸ {col} correlata con: {highly_corr} (corr > {threshold})\")\n",
    "        for hc in highly_corr:\n",
    "            cols_to_remove.add(hc)\n",
    "\n",
    "print(f\"\\nâœ‚ï¸ Da rimuovere: {list(cols_to_remove)}\")\n",
    "\n",
    "# Applico filtro\n",
    "final_cols = [c for c in cols_after_var if c not in cols_to_remove]\n",
    "df_final = df_after_var[final_cols]\n",
    "\n",
    "print(f\"âœ… Features finali: {final_cols}\")\n",
    "print(f\"\\nğŸ“ Shape: {df_after_var.shape} â†’ {df_final.shape}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5. RIEPILOGO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š RIEPILOGO SELEZIONE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nğŸ“‰ Riduzione features:\")\n",
    "print(f\"  Originali: {df_redundant.shape[1]}\")\n",
    "print(f\"  Dopo variance threshold: {len(cols_after_var)}\")\n",
    "print(f\"  Dopo correlation filter: {len(final_cols)}\")\n",
    "print(f\"\\nğŸ“‰ Riduzione totale: {df_redundant.shape[1]} â†’ {len(final_cols)} ({(1-len(final_cols)/df_redundant.shape[1])*100:.1f}% rimosso)\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# VISUALIZZAZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Varianze\n",
    "ax1 = axes[0]\n",
    "variances = df_scaled.var().sort_values()\n",
    "colors = ['red' if v < 0.01 else 'green' for v in variances]\n",
    "ax1.barh(range(len(variances)), variances.values, color=colors)\n",
    "ax1.set_yticks(range(len(variances)))\n",
    "ax1.set_yticklabels(variances.index)\n",
    "ax1.axvline(x=0.01, color='black', linestyle='--', label='Threshold 0.01')\n",
    "ax1.set_xlabel('Varianza')\n",
    "ax1.set_title('ğŸ“Š Varianze Features')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Correlazione prima\n",
    "ax2 = axes[1]\n",
    "im = ax2.imshow(df_after_var.corr(), cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax2.set_xticks(range(len(df_after_var.columns)))\n",
    "ax2.set_yticks(range(len(df_after_var.columns)))\n",
    "ax2.set_xticklabels(df_after_var.columns, rotation=45, ha='right', fontsize=8)\n",
    "ax2.set_yticklabels(df_after_var.columns, fontsize=8)\n",
    "ax2.set_title('ğŸ”— Correlazioni (dopo variance filter)')\n",
    "plt.colorbar(im, ax=ax2)\n",
    "\n",
    "# Plot 3: Correlazione dopo\n",
    "ax3 = axes[2]\n",
    "im3 = ax3.imshow(df_final.corr(), cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax3.set_xticks(range(len(df_final.columns)))\n",
    "ax3.set_yticks(range(len(df_final.columns)))\n",
    "ax3.set_xticklabels(df_final.columns, rotation=45, ha='right', fontsize=8)\n",
    "ax3.set_yticklabels(df_final.columns, fontsize=8)\n",
    "ax3.set_title('âœ… Correlazioni FINALI')\n",
    "plt.colorbar(im3, ax=ax3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… ESERCIZIO 27.2 COMPLETATO!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876354e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Esercizio 27.3 â€” Multi-Grain Feature Engineering\n",
    "\n",
    "### ğŸ“‹ Problema\n",
    "Vuoi catturare pattern a diverse scale usando clustering a livelli multipli.\n",
    "\n",
    "### ğŸ¯ Tasks\n",
    "1. Applica K-Means con k=2, 4, 8 (3 livelli di granularitÃ )\n",
    "2. Per ogni livello, estrai: cluster label e distanza dal centroide\n",
    "3. Combina tutte le features in un unico DataFrame\n",
    "4. Visualizza come i cluster si annidano tra loro\n",
    "\n",
    "### ğŸ’¡ Suggerimenti\n",
    "- Usa nomi descrittivi: `cluster_k2`, `cluster_k4`, etc.\n",
    "- La distanza dal centroide Ã¨ utile per identificare punti \"borderline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3cd0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# âœ… SOLUZIONE ESERCIZIO 27.3 â€” Multi-Grain Feature Engineering\n",
    "# =============================================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. DATASET\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š ESERCIZIO 27.3 â€” MULTI-GRAIN CLUSTERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Creo dataset con struttura gerarchica\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_blobs, y_blobs = make_blobs(\n",
    "    n_samples=400, \n",
    "    centers=8,  # 8 cluster \"base\"\n",
    "    cluster_std=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_blobs = pd.DataFrame(X_blobs, columns=['x', 'y'])\n",
    "\n",
    "print(f\"\\nğŸ“ Dataset: {df_blobs.shape}\")\n",
    "print(\"ğŸ“‹ Struttura attesa: 8 cluster che si possono raggruppare in 4, poi in 2\")\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_blobs)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. MULTI-GRAIN CLUSTERING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ¯ CLUSTERING A 3 LIVELLI\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "k_values = [2, 4, 8]\n",
    "multi_grain_features = {}\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nğŸ“Š K-Means con k={k}:\")\n",
    "    \n",
    "    # Fit\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_scaled)\n",
    "    \n",
    "    # Distanze dai centroidi\n",
    "    centroids = km.cluster_centers_\n",
    "    distances = cdist(X_scaled, centroids)\n",
    "    dist_own = np.array([distances[i, labels[i]] for i in range(len(X_scaled))])\n",
    "    \n",
    "    # Salvo features\n",
    "    multi_grain_features[f'cluster_k{k}'] = labels\n",
    "    multi_grain_features[f'dist_k{k}'] = dist_own\n",
    "    \n",
    "    # Statistiche\n",
    "    for c in range(k):\n",
    "        count = (labels == c).sum()\n",
    "        print(f\"  Cluster {c}: {count} punti ({count/len(labels)*100:.1f}%)\")\n",
    "\n",
    "# DataFrame features\n",
    "df_multi_grain = pd.DataFrame(multi_grain_features)\n",
    "\n",
    "print(f\"\\nğŸ“ Features generate: {df_multi_grain.shape}\")\n",
    "print(f\"ğŸ“‹ Colonne: {list(df_multi_grain.columns)}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. ANALISI ANNIDAMENTO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” ANALISI ANNIDAMENTO CLUSTER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Come si distribuiscono i cluster k=8 nei cluster k=4?\n",
    "print(\"\\nğŸ“Š Cluster k=8 â†’ Cluster k=4:\")\n",
    "crosstab_8_4 = pd.crosstab(df_multi_grain['cluster_k8'], df_multi_grain['cluster_k4'])\n",
    "print(crosstab_8_4)\n",
    "\n",
    "# Come si distribuiscono i cluster k=4 nei cluster k=2?\n",
    "print(\"\\nğŸ“Š Cluster k=4 â†’ Cluster k=2:\")\n",
    "crosstab_4_2 = pd.crosstab(df_multi_grain['cluster_k4'], df_multi_grain['cluster_k2'])\n",
    "print(crosstab_4_2)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4. FEATURE COMPOSITA: CODICE GERARCHICO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ·ï¸ CODICE GERARCHICO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Creo un codice gerarchico combinando i livelli\n",
    "df_multi_grain['hierarchical_code'] = (\n",
    "    df_multi_grain['cluster_k2'].astype(str) + '_' +\n",
    "    df_multi_grain['cluster_k4'].astype(str) + '_' +\n",
    "    df_multi_grain['cluster_k8'].astype(str)\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“‹ Esempi codici gerarchici:\")\n",
    "print(df_multi_grain['hierarchical_code'].value_counts().head(10))\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5. COMBINO CON DATI ORIGINALI\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“¦ DATASET FINALE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_final = pd.concat([df_blobs.reset_index(drop=True), df_multi_grain], axis=1)\n",
    "\n",
    "print(f\"\\nğŸ“ Shape finale: {df_final.shape}\")\n",
    "print(f\"\\nğŸ“‹ Prime 5 righe:\")\n",
    "print(df_final.head())\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# VISUALIZZAZIONE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Row 1: Cluster a diversi livelli\n",
    "for idx, k in enumerate(k_values):\n",
    "    ax = axes[0, idx]\n",
    "    scatter = ax.scatter(df_final['x'], df_final['y'], \n",
    "                          c=df_final[f'cluster_k{k}'], cmap='viridis', \n",
    "                          alpha=0.6, s=30)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'ğŸ¯ Cluster k={k}')\n",
    "    plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "\n",
    "# Row 2: Distanze\n",
    "for idx, k in enumerate(k_values):\n",
    "    ax = axes[1, idx]\n",
    "    for c in range(k):\n",
    "        mask = df_final[f'cluster_k{k}'] == c\n",
    "        ax.hist(df_final.loc[mask, f'dist_k{k}'], bins=15, alpha=0.5, \n",
    "                label=f'Cluster {c}')\n",
    "    ax.set_xlabel('Distanza dal Centroide')\n",
    "    ax.set_ylabel('Frequenza')\n",
    "    ax.set_title(f'ğŸ“ Distanze k={k}')\n",
    "    if k <= 4:  # Legenda solo per pochi cluster\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualizzazione gerarchica\n",
    "fig2, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Matrice annidamento k8 â†’ k2\n",
    "combined = df_multi_grain[['cluster_k2', 'cluster_k4', 'cluster_k8']].copy()\n",
    "combined['count'] = 1\n",
    "pivot = combined.groupby(['cluster_k2', 'cluster_k4', 'cluster_k8']).count().reset_index()\n",
    "pivot_matrix = pd.crosstab(\n",
    "    [df_multi_grain['cluster_k2'], df_multi_grain['cluster_k4']], \n",
    "    df_multi_grain['cluster_k8']\n",
    ")\n",
    "im = ax.imshow(pivot_matrix.values, cmap='YlOrRd', aspect='auto')\n",
    "ax.set_xlabel('Cluster k=8')\n",
    "ax.set_ylabel('(k=2, k=4)')\n",
    "ax.set_xticks(range(8))\n",
    "ax.set_yticks(range(len(pivot_matrix.index)))\n",
    "ax.set_yticklabels([str(x) for x in pivot_matrix.index])\n",
    "ax.set_title('ğŸ”— Annidamento Gerarchico: Come k=8 si distribuisce in (k=2, k=4)')\n",
    "plt.colorbar(im, ax=ax, label='Conteggio')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… ESERCIZIO 27.3 COMPLETATO!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "ğŸ“ Features Multi-Grain create:\n",
    "â€¢ cluster_k2, cluster_k4, cluster_k8 â†’ Labels a 3 livelli\n",
    "â€¢ dist_k2, dist_k4, dist_k8 â†’ Distanze per ogni livello\n",
    "â€¢ hierarchical_code â†’ Codice combinato \"2_4_8\"\n",
    "\n",
    "ğŸ’¡ USO: Cattura pattern a diverse scale di granularitÃ !\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd219a54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ¯ 4. Conclusione\n",
    "\n",
    "## ğŸ† Concetti Chiave Appresi\n",
    "\n",
    "| Concetto | Descrizione |\n",
    "|----------|-------------|\n",
    "| **Feature Engineering Unsupervised** | Creare features senza usare labels |\n",
    "| **PCA Features** | Componenti, loadings, reconstruction error |\n",
    "| **Clustering Features** | Labels, distanze, probabilitÃ  soft |\n",
    "| **Anomaly Features** | Scores normalizzati, flag binari |\n",
    "| **Feature Selection** | Variance threshold, correlation filter |\n",
    "| **Multi-Grain** | Clustering a piÃ¹ livelli di granularitÃ  |\n",
    "\n",
    "## âš ï¸ Errori Comuni da Evitare\n",
    "\n",
    "| Errore | Problema | Soluzione |\n",
    "|--------|----------|-----------|\n",
    "| Non scalare prima di PCA | Varianze incomparabili | Sempre StandardScaler |\n",
    "| Troppe features correlate | Ridondanza, overfitting | Correlation filter |\n",
    "| Ignorare variance threshold | Features non informative | Rimuovere costanti |\n",
    "| Dimenticare normalizzazione scores | Scores incomparabili | Normalizzare [0,1] |\n",
    "\n",
    "## ğŸš€ Prossimo Passo\n",
    "\n",
    "**Lezione 28**: Applicheremo TUTTO in un progetto end-to-end completo di Unsupervised Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d144cb9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“š 6. Bignami â€” Scheda di Riferimento Rapido\n",
    "\n",
    "## ğŸ“– Tipologie di Features Unsupervised\n",
    "\n",
    "| Fonte | Features | Codice |\n",
    "|-------|----------|--------|\n",
    "| **PCA** | Componenti | `pca.fit_transform(X)` |\n",
    "| **PCA** | Rec. Error | `||X - X_rec||` |\n",
    "| **K-Means** | Cluster Label | `kmeans.predict(X)` |\n",
    "| **K-Means** | Distanze | `cdist(X, centroids)` |\n",
    "| **GMM** | ProbabilitÃ  | `gmm.predict_proba(X)` |\n",
    "| **IsoForest** | Anomaly Score | `iso.decision_function(X)` |\n",
    "| **LOF** | Outlier Score | `-lof.negative_outlier_factor_` |\n",
    "\n",
    "## ğŸ”§ Template Feature Engineering\n",
    "\n",
    "```python\n",
    "# === PIPELINE COMPLETA ===\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# 1. Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. PCA Features\n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 3. Clustering Features\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# 4. Anomaly Features\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomaly_score = iso.decision_function(X_scaled)\n",
    "\n",
    "# 5. Combine\n",
    "features = pd.DataFrame({\n",
    "    'pc1': X_pca[:, 0],\n",
    "    'cluster': cluster_labels,\n",
    "    'anomaly': anomaly_score\n",
    "})\n",
    "```\n",
    "\n",
    "## ğŸ” Feature Selection\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# 1. Variance Threshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_selected = selector.fit_transform(X_scaled)\n",
    "\n",
    "# 2. Correlation Filter\n",
    "corr = X.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.9)]\n",
    "X_final = X.drop(columns=to_drop)\n",
    "```\n",
    "\n",
    "## âœ… Checklist Feature Engineering\n",
    "\n",
    "- [ ] Dati scalati con StandardScaler?\n",
    "- [ ] Features a bassa varianza rimosse?\n",
    "- [ ] Correlazioni elevate gestite?\n",
    "- [ ] PCA componenti aggiunte?\n",
    "- [ ] Cluster features aggiunte?\n",
    "- [ ] Anomaly scores aggiunti?\n",
    "- [ ] Features validate e testate?\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Fine Lezione 27 â€” Unsupervised Feature Engineering Masterclass!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
