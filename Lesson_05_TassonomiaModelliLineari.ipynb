{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "002ae5a4",
      "metadata": {},
      "source": [
        "# SECTION 1 — Titolo e Obiettivo della Lezione\n",
        "\n",
        "Prima di leggere qualsiasi formula, fissiamo un glossario chiaro. Nessun termine tecnico comparirà senza una definizione intuitiva.\n",
        "\n",
        "- **Feature**: variabile di input che descrive un’osservazione (es. età, reddito, numero di clic).\n",
        "- **Collinearità**: quando due o più feature sono quasi combinazioni lineari tra loro; rende instabili i coefficienti.\n",
        "- **Modello lineare**: combina le feature con pesi e bias per produrre un output (iperpiano nello spazio delle feature).\n",
        "- **Decision boundary**: la frontiera geometrica (linea/piano/iperpiano) che separa le classi previste da un classificatore lineare.\n",
        "- **Logit**: trasformazione $\\log \\frac{p}{1-p}$ che converte una probabilità in una quantità illimitata, modellabile linearmente.\n",
        "- **Odds**: rapporto tra probabilità di successo e di insuccesso, $\\frac{p}{1-p}$.\n",
        "- **Odds ratio**: variazione degli odds quando una feature aumenta di 1 unità; è l’esponenziale di un coefficiente logistico.\n",
        "- **Sigmoid**: funzione $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ che schiaccia il logit in un numero tra 0 e 1 (una probabilità).\n",
        "- **Probability model**: modello che restituisce probabilità (non solo classi), es. Logistic Regression stima $P(y=1\\mid x)$.\n",
        "- **Regularizzazione**: penalità sui pesi (L1/L2) per ridurre overfitting e stabilizzare i coefficienti, utile con molte feature o collinearità.\n",
        "\n",
        "Obiettivo della lezione: costruire una mappa mentale delle quattro famiglie di modelli di Scikit-Learn, capire la matematica di base dei modelli lineari e preparare una pipeline minimale di classificazione. Stile: chiarezza matematica, intuizione geometrica, esempi reali e passaggi operativi espliciti.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "473f78dd",
      "metadata": {},
      "source": [
        "# SECTION 2 — Tassonomia delle Quattro Famiglie di Modelli\n",
        "\n",
        "Scikit-Learn offre molti modelli, ma pensarli per **famiglie concettuali** aiuta a scegliere in modo ragionato.\n",
        "\n",
        "Le quattro famiglie principali sono:\n",
        "\n",
        "1. **Lineari**\n",
        "2. **Tree-Based** (alberi e ensemble)\n",
        "3. **Distanza / Vicinanza** (KNN)\n",
        "4. **Probabilistici / Bayesiani**\n",
        "\n",
        "Capire la famiglia significa sapere in anticipo cosa può apprendere, quale pre-processing richiede e quanto è interpretabile.\n",
        "\n",
        "## 2.1 Modelli Lineari\n",
        "\n",
        "- **Esempi**: `LinearRegression`, `LogisticRegression`, `Ridge`, `Lasso`, `SGDClassifier`.\n",
        "- **Idea**: stimano una combinazione lineare delle feature.\n",
        "\n",
        "Formula generale:\n",
        "\n",
        "$$\n",
        "\\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\n",
        "$$\n",
        "\n",
        "- **Punti di forza**: interpretabilità, velocità, ottime baseline.\n",
        "- **Limiti**: relazioni quasi-lineari; sensibilità a outlier e scaling.\n",
        "- **Esempi applicati**: pricing assicurativo base (premio stimato da età e storico sinistri), scoring di rischio di credito rapido, previsione di click per una campagna con poche feature aggregate.\n",
        "- **Metodo di pensiero**: chiediti se un iperpiano può spiegare gran parte della variazione; se sì, parti da qui per avere un riferimento interpretabile.\n",
        "\n",
        "## 2.2 Modelli Tree-Based (Alberi)\n",
        "\n",
        "- **Esempi**: Decision Tree, Random Forest, Gradient Boosting.\n",
        "- **Idea**: suddivisione dello spazio tramite soglie.\n",
        "\n",
        "Esempio:\n",
        "\n",
        "```text\n",
        "if feature_3 < 2.5:\n",
        "    if feature_1 >= 0.7:\n",
        "        -> classe A\n",
        "    else:\n",
        "        -> classe B\n",
        "else:\n",
        "    -> classe C\n",
        "```\n",
        "\n",
        "- **Punti di forza**: catturano non-linearità, robusti, ottime performance.\n",
        "- **Limiti**: rischio overfitting (alberi singoli), interpretabilità minore rispetto ai lineari.\n",
        "- **Esempi applicati**: rilevazione frodi con molte interazioni tra importo e device; previsione di churn combinando uso app, ticket, campagne; scoring industriale con sensori eterogenei.\n",
        "- **Metodo di pensiero**: usali quando sospetti interazioni o soglie non lineari e vuoi performance solide con poco feature engineering.\n",
        "\n",
        "## 2.3 Modelli di Distanza (KNN)\n",
        "\n",
        "- **Idea**: basati sui _vicini più simili_.\n",
        "\n",
        "Distanza euclidea tipica:\n",
        "\n",
        "$$\n",
        "d(x, x_i) = \\sqrt{\\sum_j (x_j - x_{ij})^2}\n",
        "$$\n",
        "\n",
        "- **Pro**: nessuna assunzione forte, intuitivi.\n",
        "- **Contro**: molto sensibili allo scaling; lenti in predizione.\n",
        "- **Esempi applicati**: raccomandazione di prodotti per piccoli cataloghi; classificazione di immagini molto piccole; diagnosi rapida su dataset ridotti e puliti.\n",
        "- **Metodo di pensiero**: funziona se i punti vicini condividono l’etichetta e le feature sono già su scale comparabili (altrimenti standardizza).\n",
        "\n",
        "## 2.4 Modelli Probabilistici / Bayesiani\n",
        "\n",
        "- **Esempi**: GaussianNB, MultinomialNB, BernoulliNB.\n",
        "- **Formula fondamentale (Teorema di Bayes)**:\n",
        "\n",
        "  $$\n",
        "  P(y \\mid x) = \\frac{P(x \\mid y) P(y)}{P(x)}\n",
        "  $$\n",
        "\n",
        "- **Pro**: velocissimi; ottimi su dati testuali.\n",
        "- **Contro**: assumono (spesso) indipendenza tra le feature.\n",
        "- **Esempi applicati**: classificazione spam con conteggi parole; intent detection di frasi corte; triage rapido di ticket testuali.\n",
        "- **Metodo di pensiero**: scegli Bayes se vuoi probabilità immediate con feature quasi indipendenti (o trattate come tali) e dataset testuali leggeri.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa8a6352",
      "metadata": {},
      "source": [
        "# SECTION 3 — Matematica dei Modelli Lineari\n",
        "\n",
        "Questa sezione espone la logica matematica dei modelli lineari e come leggerne i parametri.\n",
        "\n",
        "## 3.1 Combinazione Lineare\n",
        "\n",
        "Formula:\n",
        "\n",
        "$$\n",
        "\\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\n",
        "$$\n",
        "\n",
        "In forma vettoriale:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\mathbf{w}^T \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "Interpretazione dei coefficienti:\n",
        "\n",
        "- segno di \\( w_j \\): direzione dell’effetto;\n",
        "- modulo di \\( w_j \\): forza dell’effetto.\n",
        "\n",
        "**Esempi applicati**: prezzo di una casa come somma pesata di metri quadri e posizione; tempo di consegna come somma di distanza e traffico; punteggio di rischio come combinazione di reddito, debito e storico pagamenti.\n",
        "\n",
        "## 3.2 Logistic Regression\n",
        "\n",
        "**Step 1 — Logit**\n",
        "\n",
        "$$\n",
        "z = \\mathbf{w}^T \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "**Step 2 — Sigmoide**\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Probabilità stimata della classe positiva:\n",
        "\n",
        "$$\n",
        "\\hat{p} = P(y=1 \\mid x) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)\n",
        "$$\n",
        "\n",
        "**Esempi applicati**: rischio di default (classe 1) in funzione di reddito/debito; probabilità di click a una campagna; probabilità che una transazione sia fraudolenta.\n",
        "\n",
        "## 3.3 Interpretazione dei coefficienti\n",
        "\n",
        "Relazione log-odds:\n",
        "\n",
        "$$\n",
        "\\log \\frac{p}{1-p} = \\mathbf{w}^T \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "Ogni coefficiente sposta il logit: $e^{w_j}$ è l’odds ratio associato alla feature $j$ (dopo scaling), cioè quanto cambiano gli odds aumentando quella feature di un’unità.\n",
        "\n",
        "## 3.4 Standardizzazione\n",
        "\n",
        "StandardScaler applica:\n",
        "\n",
        "$$\n",
        "x' = \\frac{x - \\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "Benefici:\n",
        "\n",
        "- stabilità numerica;\n",
        "- convergenza più rapida;\n",
        "- coefficienti comparabili;\n",
        "- assolutamente necessario per modelli lineari, KNN e SVM.\n",
        "\n",
        "**Metodo di pensiero**: prima guarda se le feature hanno scale molto diverse; se sì, standardizza per evitare che una variabile domini l’iperpiano o la distanza. Poi interpreta i coefficienti sapendo che sono riferiti a variazioni di 1 deviazione standard.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9040ed34",
      "metadata": {},
      "source": [
        "# SECTION 4 — Pipeline Logica di Ogni Modello ML\n",
        "\n",
        "Pipeline standard in ML supervisionato:\n",
        "\n",
        "1. **Feature engineering**\n",
        "2. **Train/test split**\n",
        "3. **Scelta del modello**\n",
        "4. **Fit**\n",
        "5. **Predict**\n",
        "6. **Score**\n",
        "\n",
        "Schema ASCII:\n",
        "\n",
        "```text\n",
        "[Feature Eng.] -> [Split] -> [Modello] -> [Fit] -> [Predict] -> [Score]\n",
        "```\n",
        "\n",
        "È un ciclo iterativo: dopo lo score si torna indietro se necessario.\n",
        "\n",
        "**Esempi applicati**\n",
        "\n",
        "- **Churn**: crea feature su uso app, fai split, prova Logistic Regression come baseline, misura precision/recall.\n",
        "- **Fraude**: ingegnerizza frequenze per device/merchant, scala, usa Random Forest, valuta AUROC e richiama feature eng se il recall è basso.\n",
        "- **Domanda energetica**: costruisci lag temporali, split temporale, Linear Regression o Gradient Boosting, controlla RMSE e residui.\n",
        "\n",
        "**Metodo di pensiero**: visualizza la pipeline come un nastro trasportatore. Ogni blocco è verificabile: se le metriche sono basse, chiediti se mancano feature, se il modello è troppo semplice o se la divisione dei dati è adeguata. Ripeti in modo disciplinato, non a tentativi casuali.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1099388a",
      "metadata": {},
      "source": [
        "# SECTION 5 — Come Scegliere un Modello\n",
        "\n",
        "Euristiche pratiche:\n",
        "\n",
        "- dati quasi lineari → Logistic/Linear Regression;\n",
        "- non linearità sospette → RandomForest;\n",
        "- dataset enorme → Gradient Boosting;\n",
        "- NLP classico → Naive Bayes + TF-IDF;\n",
        "- dataset piccolo → KNN.\n",
        "\n",
        "**Esempi applicati**\n",
        "\n",
        "- **Credito retail**: parti con Logistic Regression per interpretabilità, poi confronta con Gradient Boosting se servono interazioni.\n",
        "- **Ticket testuali**: MultinomialNB + TF-IDF come baseline, poi eventualmente modelli più ricchi.\n",
        "- **Telemetria IoT**: Random Forest/Gradient Boosting per catturare soglie e interazioni non lineari tra sensori.\n",
        "\n",
        "**Metodo di pensiero**: scegli la complessità minima che risolve il problema. Usa un modello lineare come riferimento interpretabile; se fallisce, passa a modelli più flessibili controllando overfitting e stabilità.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d67eb62",
      "metadata": {},
      "source": [
        "# SECTION 6 — Esempio Mentale: costruzione di un modello ML\n",
        "\n",
        "Esempio su problema di churn: capire tipo di problema, pre-processing, scelta modello iniziale, valutazione tramite precision/recall, iterazione.\n",
        "\n",
        "**Scenario ragionato**\n",
        "\n",
        "1. Definisci l’obiettivo: prevedere se un utente disdirà (classe 1) nei prossimi 30 giorni.\n",
        "2. Prepara feature: frequenza login, durata sessione, ticket aperti, risposta a campagne.\n",
        "3. Split stratificato: preserva la classe rara (churn) nel test.\n",
        "4. Baseline: Logistic Regression con standardizzazione; valuta precision e recall sulla classe positiva.\n",
        "5. Itera: se intercetti poche churn (recall basso), prova Random Forest/Gradient Boosting e arricchisci le feature (es. trend temporali).\n",
        "\n",
        "**Altri esempi rapidi**\n",
        "\n",
        "- **Fraude**: stessa logica ma metriche focalizzate sul recall della frode e sul tasso di falsi allarmi.\n",
        "- **Marketing**: ottimizza la probabilità di click; usa il modello probabilistico per selezionare top-N utenti con soglia regolata sul budget.\n",
        "\n",
        "**Metodo di pensiero**: ogni passaggio deve avere un motivo. Dopo lo score, chiediti se l’errore viene da feature povere, modello troppo semplice o dati sbilanciati. Cambia un elemento per volta.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f24d5ad",
      "metadata": {},
      "source": [
        "# SECTION 7 — DIMOSTRAZIONE: Logistic Regression Minimal Example\n",
        "\n",
        "Implementiamo una pipeline completa: dataset sintetico → scaling → Logistic Regression → coefficienti → accuracy. Questo è il percorso minimo per ottenere un modello interpretabile e vedere subito come ogni fase impatta sul risultato.\n",
        "\n",
        "**Esempi applicati**\n",
        "\n",
        "- **Spam vs. ham** su poche feature di testo (lunghezza, punteggiatura, frequenze chiave).\n",
        "- **Rischio credito base** con 5–6 variabili numeriche (reddito, debito, anzianità lavorativa).\n",
        "- **Churn rapido** con feature aggregate giornaliere (login, durata, ticket).\n",
        "\n",
        "**Metodo di pensiero**: esegui lo script e osserva shape, bilanciamento classi, stabilizzazione dopo lo scaling e significato dei coefficienti. Considera questa demo come checklist da riusare nei tuoi progetti.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f70d8bc2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape X: (600, 6)\n",
            "Positive class ratio: 0.50\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=600,\n",
        "    n_features=6,\n",
        "    n_informative=4,\n",
        "    n_redundant=0,\n",
        "    class_sep=1.5,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(f\"Shape X: {X.shape}\")\n",
        "print(f\"Positive class ratio: {y.mean():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "eb4fb90d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(450, 6) (150, 6)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2fc16d71",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 7.35214359e-17 -4.49455288e-16  3.50337043e-17  2.91125149e-17\n",
            "  5.21188031e-16 -1.48029737e-18]\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(X_train_scaled.mean(axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5a8283d8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "0",
                  "rawType": "float64",
                  "type": "float"
                }
              ],
              "ref": "bda666bd-a8c6-490b-ab67-e75d201e499f",
              "rows": [
                [
                  "feature_1",
                  "1.1586538968827325"
                ],
                [
                  "feature_4",
                  "0.7823028257283188"
                ],
                [
                  "feature_5",
                  "0.598549870239413"
                ],
                [
                  "feature_2",
                  "0.09697855791010884"
                ],
                [
                  "feature_3",
                  "0.06530627582463851"
                ],
                [
                  "feature_0",
                  "-0.8244110452495775"
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 6
              }
            },
            "text/plain": [
              "feature_1    1.158654\n",
              "feature_4    0.782303\n",
              "feature_5    0.598550\n",
              "feature_2    0.096979\n",
              "feature_3    0.065306\n",
              "feature_0   -0.824411\n",
              "dtype: float64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "coeff = pd.Series(log_reg.coef_.ravel(), index=[f\"feature_{i}\" for i in range(6)])\n",
        "coeff.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "de83218f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.727\n"
          ]
        }
      ],
      "source": [
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.14.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
