{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b56c9e",
   "metadata": {},
   "source": [
    "# Lezione 20 ‚Äî K-Means Clustering\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivi della Lezione\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "1. **Comprendere** la geometria del clustering K-Means\n",
    "2. **Spiegare** il ruolo dei centroidi e come vengono aggiornati\n",
    "3. **Calcolare** la distanza euclidea e capire perch√© √® fondamentale\n",
    "4. **Riconoscere** le assunzioni forti del modello e i loro limiti\n",
    "5. **Applicare** K-Means correttamente con sklearn\n",
    "\n",
    "---\n",
    "\n",
    "## Perch√© questa lezione √® importante\n",
    "\n",
    "K-Means √® l'algoritmo di clustering pi√π usato al mondo. √à semplice, veloce e spesso efficace.\n",
    "\n",
    "Ma questa semplicit√† nasconde **assunzioni forti** che, se ignorate, portano a risultati sbagliati:\n",
    "- Assume che i cluster siano **sferici**\n",
    "- Assume che abbiano **dimensioni simili**\n",
    "- √à **sensibile** ai valori iniziali e agli outlier\n",
    "\n",
    "Capire come funziona *davvero* K-Means ti permette di:\n",
    "- Usarlo quando √® appropriato\n",
    "- Evitarlo quando non lo √®\n",
    "- Interpretare correttamente i risultati\n",
    "\n",
    "---\n",
    "\n",
    "## Ruolo nel percorso\n",
    "\n",
    "| Lezione | Argomento |\n",
    "|---------|-----------|\n",
    "| 19 | Introduzione all'Unsupervised Learning |\n",
    "| **20** | **K-Means Clustering** ‚Üê Sei qui |\n",
    "| 21 | Scelta del numero di cluster (Elbow, Silhouette) |\n",
    "| 22 | Clustering Gerarchico |\n",
    "| 23 | DBSCAN |\n",
    "\n",
    "Questa lezione introduce il **primo algoritmo concreto** di clustering. Nelle lezioni successive vedremo come scegliere K e algoritmi alternativi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da452241",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 1 ‚Äî Teoria Concettuale\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Cos'√® il clustering?\n",
    "\n",
    "Il **clustering** √® il task di raggruppare osservazioni simili tra loro, senza sapere a priori quanti gruppi esistono o quali sono.\n",
    "\n",
    "### L'idea intuitiva\n",
    "\n",
    "Immagina di avere 1000 clienti descritti da:\n",
    "- Frequenza di acquisto\n",
    "- Spesa media\n",
    "- Tempo dall'ultimo acquisto\n",
    "\n",
    "Senza sapere nulla di loro, vuoi trovare **gruppi naturali**: clienti che si comportano in modo simile.\n",
    "\n",
    "Il clustering risponde alla domanda:\n",
    "> \"Quali osservazioni stanno insieme?\"\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 K-Means: l'idea fondamentale\n",
    "\n",
    "K-Means √® l'algoritmo di clustering pi√π semplice e diffuso.\n",
    "\n",
    "### Il concetto in una frase\n",
    "\n",
    "> K-Means divide i dati in **K gruppi** minimizzando la distanza tra ogni punto e il **centro del suo gruppo** (centroide).\n",
    "\n",
    "### Cosa sono i centroidi?\n",
    "\n",
    "Un **centroide** √® il punto medio di un cluster: la media aritmetica di tutte le osservazioni che appartengono a quel gruppo.\n",
    "\n",
    "$$\\mu_k = \\frac{1}{|C_k|} \\sum_{x_i \\in C_k} x_i$$\n",
    "\n",
    "Dove:\n",
    "- $\\mu_k$ = centroide del cluster $k$\n",
    "- $C_k$ = insieme dei punti assegnati al cluster $k$\n",
    "- $|C_k|$ = numero di punti nel cluster $k$\n",
    "\n",
    "**Esempio numerico:**\n",
    "\n",
    "Se un cluster contiene 3 punti in 2D:\n",
    "- Punto A: (2, 4)\n",
    "- Punto B: (4, 6)\n",
    "- Punto C: (3, 5)\n",
    "\n",
    "Il centroide √®:\n",
    "$$\\mu = \\left(\\frac{2+4+3}{3}, \\frac{4+6+5}{3}\\right) = (3, 5)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ded85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 La distanza euclidea\n",
    "\n",
    "K-Means usa la **distanza euclidea** per misurare quanto un punto √® \"lontano\" da un centroide.\n",
    "\n",
    "### Formula in 2 dimensioni\n",
    "\n",
    "Per due punti $A = (x_1, y_1)$ e $B = (x_2, y_2)$:\n",
    "\n",
    "$$d(A, B) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$$\n",
    "\n",
    "√à il teorema di Pitagora: la distanza in linea retta.\n",
    "\n",
    "### Formula generale (n dimensioni)\n",
    "\n",
    "Per due punti con $n$ feature:\n",
    "\n",
    "$$d(A, B) = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}$$\n",
    "\n",
    "### Esempio numerico\n",
    "\n",
    "Punto A: (2, 3, 1)  \n",
    "Punto B: (5, 7, 2)\n",
    "\n",
    "$$d(A, B) = \\sqrt{(5-2)^2 + (7-3)^2 + (2-1)^2} = \\sqrt{9 + 16 + 1} = \\sqrt{26} \\approx 5.1$$\n",
    "\n",
    "---\n",
    "\n",
    "### Perch√© la distanza euclidea √® importante?\n",
    "\n",
    "K-Means assegna ogni punto al centroide **pi√π vicino** in termini di distanza euclidea.\n",
    "\n",
    "Questo ha conseguenze importanti:\n",
    "1. **Scale diverse ‚Üí problemi**: se una feature ha range 0-100 e un'altra 0-100.000, la seconda domina\n",
    "2. **Forma sferica implicita**: la distanza euclidea definisce \"sfere\" attorno ai centroidi\n",
    "3. **Outlier problematici**: punti molto lontani distorcono i centroidi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3815e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 L'algoritmo K-Means passo per passo\n",
    "\n",
    "L'algoritmo √® iterativo e segue questi passi:\n",
    "\n",
    "### Passo 0: Inizializzazione\n",
    "Scegli K punti iniziali come centroidi (random o con metodo k-means++).\n",
    "\n",
    "### Passo 1: Assegnazione\n",
    "Per ogni punto, calcola la distanza da tutti i K centroidi.\n",
    "Assegna il punto al centroide pi√π vicino.\n",
    "\n",
    "### Passo 2: Aggiornamento\n",
    "Ricalcola ogni centroide come la media dei punti assegnati a quel cluster.\n",
    "\n",
    "### Passo 3: Convergenza\n",
    "Se i centroidi non cambiano (o cambiano meno di una soglia), STOP.\n",
    "Altrimenti, torna al Passo 1.\n",
    "\n",
    "---\n",
    "\n",
    "### Visualizzazione dell'algoritmo\n",
    "\n",
    "```\n",
    "Iterazione 0:  ‚óè  ‚óè  ‚óè     (centroidi iniziali random)\n",
    "               ¬∑  ¬∑  ¬∑  ¬∑  ¬∑  (punti da clusterizzare)\n",
    "\n",
    "Iterazione 1:  Assegna ogni punto al centroide pi√π vicino\n",
    "               Ricalcola i centroidi\n",
    "\n",
    "Iterazione 2:  Riassegna, ricalcola...\n",
    "\n",
    "...\n",
    "\n",
    "Convergenza:   I centroidi non si muovono pi√π ‚Üí FINE\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### La funzione obiettivo (Inertia)\n",
    "\n",
    "K-Means minimizza l'**inertia** (o Within-Cluster Sum of Squares, WCSS):\n",
    "\n",
    "$$\\text{Inertia} = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} ||x_i - \\mu_k||^2$$\n",
    "\n",
    "In parole: la somma delle distanze al quadrato di ogni punto dal suo centroide.\n",
    "\n",
    "**Meno inertia = cluster pi√π compatti.**\n",
    "\n",
    "Attenzione: l'inertia diminuisce SEMPRE all'aumentare di K (caso limite: K = n punti ‚Üí inertia = 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4216da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 Le assunzioni forti di K-Means\n",
    "\n",
    "K-Means funziona bene **solo se** i dati rispettano certe condizioni. Se queste assunzioni sono violate, i risultati saranno sbagliati.\n",
    "\n",
    "---\n",
    "\n",
    "### Assunzione 1: Cluster sferici (isotropici)\n",
    "\n",
    "K-Means assume che i cluster abbiano **forma sferica** (o iper-sferica in n dimensioni).\n",
    "\n",
    "Questo perch√© usa la distanza euclidea, che definisce \"cerchi\" di equidistanza attorno ai centroidi.\n",
    "\n",
    "**Problema:** Se i cluster hanno forma allungata, a \"banana\", o irregolare, K-Means li taglia male.\n",
    "\n",
    "---\n",
    "\n",
    "### Assunzione 2: Cluster di dimensioni simili\n",
    "\n",
    "K-Means tende ad assegnare lo stesso numero di punti a ogni cluster.\n",
    "\n",
    "**Problema:** Se un cluster ha 1000 punti e un altro ne ha 50, K-Means potrebbe \"rubare\" punti dal cluster grande per bilanciare.\n",
    "\n",
    "---\n",
    "\n",
    "### Assunzione 3: Varianza simile tra cluster\n",
    "\n",
    "K-Means assume che i cluster abbiano **dispersione simile** attorno al centroide.\n",
    "\n",
    "**Problema:** Se un cluster √® molto compatto e un altro molto disperso, il confine sar√† sbagliato.\n",
    "\n",
    "---\n",
    "\n",
    "### Assunzione 4: Assenza di outlier significativi\n",
    "\n",
    "I centroidi sono **medie**, quindi molto sensibili agli outlier.\n",
    "\n",
    "**Problema:** Un singolo punto anomalo pu√≤ spostare significativamente un centroide.\n",
    "\n",
    "---\n",
    "\n",
    "### Tabella riassuntiva\n",
    "\n",
    "| Assunzione | Cosa assume K-Means | Cosa succede se violata |\n",
    "|------------|---------------------|-------------------------|\n",
    "| Forma | Cluster sferici | Cluster tagliati male |\n",
    "| Dimensione | Cluster bilanciati | Punti assegnati al cluster sbagliato |\n",
    "| Varianza | Dispersione simile | Confini distorti |\n",
    "| Outlier | Nessun outlier | Centroidi distorti |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28272b0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.6 Il problema dell'inizializzazione\n",
    "\n",
    "K-Means √® un algoritmo **greedy**: trova un minimo locale, non globale.\n",
    "\n",
    "Il risultato dipende da **dove partono i centroidi iniziali**.\n",
    "\n",
    "### Il problema\n",
    "\n",
    "```\n",
    "Inizializzazione A:  ‚óè    ‚óè    ‚óè     ‚Üí Converge a soluzione X\n",
    "Inizializzazione B:  ‚óè  ‚óè      ‚óè     ‚Üí Converge a soluzione Y (diversa!)\n",
    "```\n",
    "\n",
    "Con centroidi iniziali diversi, K-Means pu√≤ convergere a soluzioni diverse.\n",
    "\n",
    "### La soluzione: k-means++\n",
    "\n",
    "L'algoritmo **k-means++** (default in sklearn) sceglie i centroidi iniziali in modo intelligente:\n",
    "\n",
    "1. Scegli il primo centroide random\n",
    "2. Per i successivi, scegli punti **lontani** dai centroidi gi√† scelti\n",
    "3. La probabilit√† di scegliere un punto √® proporzionale alla sua distanza dal centroide pi√π vicino\n",
    "\n",
    "Questo riduce la probabilit√† di inizializzazioni sfortunate.\n",
    "\n",
    "### n_init: eseguire pi√π volte\n",
    "\n",
    "Sklearn esegue K-Means **n_init volte** con inizializzazioni diverse e tiene la soluzione con inertia minore.\n",
    "\n",
    "```python\n",
    "KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "```\n",
    "\n",
    "Significa: esegui 10 volte, tieni il risultato migliore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2838adb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 2 ‚Äî Schema Mentale e Mappa Logica\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Quando usare K-Means\n",
    "\n",
    "### Situazioni ideali per K-Means\n",
    "\n",
    "| Condizione | Perch√© K-Means funziona |\n",
    "|------------|-------------------------|\n",
    "| **Cluster sferici** | La distanza euclidea li cattura bene |\n",
    "| **Cluster bilanciati** | Nessun cluster viene \"schiacciato\" |\n",
    "| **Varianza simile** | I confini saranno corretti |\n",
    "| **Dati scalati** | Tutte le feature contribuiscono equamente |\n",
    "| **K noto o stimabile** | Puoi usare Elbow/Silhouette (Lezione 21) |\n",
    "| **Molti dati** | K-Means √® veloce anche su milioni di punti |\n",
    "\n",
    "### Segnali che K-Means √® appropriato\n",
    "\n",
    "- Scatter plot 2D mostra gruppi \"rotondi\" e separati\n",
    "- Le feature hanno scale simili (o le hai scalate)\n",
    "- Non ci sono outlier evidenti\n",
    "- Il business suggerisce un numero ragionevole di segmenti\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Quando NON usare K-Means\n",
    "\n",
    "### Situazioni problematiche\n",
    "\n",
    "| Condizione | Perch√© K-Means fallisce | Alternativa |\n",
    "|------------|-------------------------|-------------|\n",
    "| **Cluster allungati** | Li taglia male | DBSCAN, Spectral |\n",
    "| **Cluster di dimensioni diverse** | Ruba punti | DBSCAN, GMM |\n",
    "| **Outlier** | Distorcono i centroidi | DBSCAN, rimuovi outlier |\n",
    "| **K ignoto** | Risultato arbitrario | Hierarchical per esplorare |\n",
    "| **Forma arbitraria** | Assume sfericit√† | DBSCAN |\n",
    "\n",
    "### Segnali che K-Means √® inappropriato\n",
    "\n",
    "- Scatter plot mostra forme \"a banana\" o irregolari\n",
    "- Un gruppo √® molto pi√π grande degli altri\n",
    "- Ci sono punti isolati lontani da tutto\n",
    "- Non hai idea di quanti cluster cercare\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 Checklist pre-K-Means\n",
    "\n",
    "Prima di applicare K-Means:\n",
    "\n",
    "- [ ] **Scaling**: hai applicato StandardScaler?\n",
    "- [ ] **Outlier**: hai verificato e gestito i punti anomali?\n",
    "- [ ] **K**: hai un'ipotesi su quanti cluster cercare?\n",
    "- [ ] **Forma**: i dati sembrano avere gruppi \"rotondi\"?\n",
    "- [ ] **n_init**: stai usando n_init ‚â• 10?\n",
    "- [ ] **random_state**: hai fissato il seed per riproducibilit√†?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3ce8f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 3 ‚Äî Notebook Dimostrativo\n",
    "\n",
    "---\n",
    "\n",
    "## Demo 1: K-Means base ‚Äî vedere l'algoritmo in azione\n",
    "\n",
    "Generiamo dati con 3 gruppi ben separati e vediamo come K-Means li trova."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fcf7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 1: K-Means base\n",
    "# ============================================\n",
    "# Obiettivo: vedere K-Means in azione su dati ideali\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fissiamo il seed per riproducibilit√†\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 1: Generazione dati sintetici\n",
    "# ============================================\n",
    "# Creiamo 3 cluster ben separati (caso ideale per K-Means)\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=300,       # 300 punti totali\n",
    "    centers=3,           # 3 centri\n",
    "    cluster_std=0.8,     # deviazione standard di ogni cluster\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEMO 1: K-Means su dati ideali\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDimensioni dataset: {X.shape}\")\n",
    "print(f\"Gruppi veri: {np.unique(y_true)}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 2: Applicazione K-Means\n",
    "# ============================================\n",
    "# Nota: in questo caso i dati sono gi√† centrati, ma per buona pratica scaliamo\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Applichiamo K-Means con K=3 (sappiamo che ci sono 3 gruppi)\n",
    "kmeans = KMeans(\n",
    "    n_clusters=3,        # numero di cluster\n",
    "    init='k-means++',    # inizializzazione intelligente\n",
    "    n_init=10,           # esegui 10 volte, tieni il migliore\n",
    "    max_iter=300,        # massimo iterazioni per convergenza\n",
    "    random_state=42      # riproducibilit√†\n",
    ")\n",
    "\n",
    "# fit_predict: addestra e restituisce le etichette\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 3: Analisi del risultato\n",
    "# ============================================\n",
    "print(f\"\\nRisultato K-Means:\")\n",
    "print(f\"  Etichette uniche: {np.unique(labels)}\")\n",
    "print(f\"  Punti per cluster: {np.bincount(labels)}\")\n",
    "print(f\"  Inertia (WCSS): {kmeans.inertia_:.2f}\")\n",
    "print(f\"  Iterazioni per convergere: {kmeans.n_iter_}\")\n",
    "\n",
    "# Silhouette score (metrica di qualit√†)\n",
    "sil = silhouette_score(X_scaled, labels)\n",
    "print(f\"  Silhouette score: {sil:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 4: Visualizzazione\n",
    "# ============================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Gruppi veri (se li conoscessimo)\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.7)\n",
    "axes[0].set_title('Gruppi VERI (ground truth)')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Clustering K-Means\n",
    "scatter = axes[1].scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "\n",
    "# Trasformiamo i centroidi nello spazio originale per visualizzarli\n",
    "centroidi_originali = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "axes[1].scatter(\n",
    "    centroidi_originali[:, 0], \n",
    "    centroidi_originali[:, 1], \n",
    "    c='red', marker='X', s=300, edgecolors='black', linewidth=2,\n",
    "    label='Centroidi'\n",
    ")\n",
    "axes[1].set_title(f'K-Means (K=3) | Silhouette={sil:.3f}')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OSSERVAZIONI\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "‚úÖ K-Means ha trovato esattamente i 3 gruppi\n",
    "‚úÖ I centroidi (stelle rosse) sono nel 'centro' di ogni cluster\n",
    "‚úÖ Silhouette alto (~0.7) indica buona separazione\n",
    "‚úÖ Con dati sferici e ben separati, K-Means funziona perfettamente\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e5ef3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Demo 2: Visualizzare le iterazioni di K-Means\n",
    "\n",
    "Mostriamo come l'algoritmo converge passo dopo passo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d98173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 2: Visualizzare le iterazioni di K-Means\n",
    "# ============================================\n",
    "# Obiettivo: vedere come i centroidi si muovono ad ogni iterazione\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Dataset semplice per visualizzare bene\n",
    "X_demo, _ = make_blobs(n_samples=150, centers=3, cluster_std=1.2, random_state=123)\n",
    "\n",
    "# Inizializziamo i centroidi in modo random (NON k-means++)\n",
    "# per vedere bene il movimento\n",
    "np.random.seed(999)\n",
    "initial_centroids = X_demo[np.random.choice(len(X_demo), 3, replace=False)]\n",
    "\n",
    "# Eseguiamo K-Means manualmente per tracciare le iterazioni\n",
    "def kmeans_step_by_step(X, K, initial_centroids, max_iter=10):\n",
    "    \"\"\"Esegue K-Means tracciando ogni iterazione\"\"\"\n",
    "    centroids = initial_centroids.copy()\n",
    "    history = [centroids.copy()]\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Passo 1: Assegnazione - calcola distanze e assegna al centroide pi√π vicino\n",
    "        distances = np.zeros((len(X), K))\n",
    "        for k in range(K):\n",
    "            distances[:, k] = np.sqrt(np.sum((X - centroids[k])**2, axis=1))\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Passo 2: Aggiornamento - ricalcola i centroidi\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        for k in range(K):\n",
    "            if np.sum(labels == k) > 0:\n",
    "                new_centroids[k] = X[labels == k].mean(axis=0)\n",
    "            else:\n",
    "                new_centroids[k] = centroids[k]\n",
    "        \n",
    "        # Controlla convergenza\n",
    "        if np.allclose(centroids, new_centroids):\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids\n",
    "        history.append(centroids.copy())\n",
    "    \n",
    "    return labels, centroids, history\n",
    "\n",
    "# Eseguiamo\n",
    "labels_final, centroids_final, history = kmeans_step_by_step(X_demo, 3, initial_centroids)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEMO 2: Iterazioni di K-Means\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCentroidi iniziali (random):\")\n",
    "for i, c in enumerate(initial_centroids):\n",
    "    print(f\"  Centroide {i}: ({c[0]:.2f}, {c[1]:.2f})\")\n",
    "\n",
    "print(f\"\\nNumero di iterazioni: {len(history)}\")\n",
    "\n",
    "# Visualizzazione delle iterazioni\n",
    "n_plots = min(4, len(history))\n",
    "fig, axes = plt.subplots(1, n_plots, figsize=(4*n_plots, 4))\n",
    "\n",
    "iterations_to_show = [0, 1, len(history)//2, len(history)-1][:n_plots]\n",
    "\n",
    "for idx, (ax, iter_num) in enumerate(zip(axes, iterations_to_show)):\n",
    "    # Calcola le etichette per questa iterazione\n",
    "    centroids_iter = history[iter_num]\n",
    "    distances = np.zeros((len(X_demo), 3))\n",
    "    for k in range(3):\n",
    "        distances[:, k] = np.sqrt(np.sum((X_demo - centroids_iter[k])**2, axis=1))\n",
    "    labels_iter = np.argmin(distances, axis=1)\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(X_demo[:, 0], X_demo[:, 1], c=labels_iter, cmap='viridis', s=40, alpha=0.6)\n",
    "    ax.scatter(centroids_iter[:, 0], centroids_iter[:, 1], \n",
    "               c='red', marker='X', s=200, edgecolors='black', linewidth=2)\n",
    "    \n",
    "    if iter_num == 0:\n",
    "        ax.set_title(f'Iterazione {iter_num}\\n(centroidi random)')\n",
    "    elif iter_num == len(history)-1:\n",
    "        ax.set_title(f'Iterazione {iter_num}\\n(CONVERGENZA)')\n",
    "    else:\n",
    "        ax.set_title(f'Iterazione {iter_num}')\n",
    "    \n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostra il movimento dei centroidi\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MOVIMENTO DEI CENTROIDI\")\n",
    "print(\"=\"*60)\n",
    "for i in range(3):\n",
    "    start = initial_centroids[i]\n",
    "    end = centroids_final[i]\n",
    "    dist = np.sqrt(np.sum((end - start)**2))\n",
    "    print(f\"\\nCentroide {i}:\")\n",
    "    print(f\"  Partenza: ({start[0]:.2f}, {start[1]:.2f})\")\n",
    "    print(f\"  Arrivo:   ({end[0]:.2f}, {end[1]:.2f})\")\n",
    "    print(f\"  Distanza percorsa: {dist:.2f}\")\n",
    "\n",
    "print(\"\\nüí° I centroidi si spostano verso il 'centro di massa' dei punti assegnati\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972a55c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Demo 3: Quando K-Means fallisce ‚Äî cluster non sferici\n",
    "\n",
    "Mostriamo cosa succede quando i cluster hanno forme che violano le assunzioni di K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f0b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 3: Quando K-Means fallisce\n",
    "# ============================================\n",
    "# Obiettivo: mostrare i limiti di K-Means su dati non ideali\n",
    "\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Creiamo 3 dataset problematici per K-Means\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# ============================================\n",
    "# CASO 1: Cluster a forma di luna (moons)\n",
    "# ============================================\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.08, random_state=42)\n",
    "\n",
    "# K-Means su moons\n",
    "kmeans_moons = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "labels_moons = kmeans_moons.fit_predict(X_moons)\n",
    "\n",
    "# Plot\n",
    "axes[0, 0].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='coolwarm', s=40, alpha=0.7)\n",
    "axes[0, 0].set_title('Moons: Gruppi VERI')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_moons, cmap='coolwarm', s=40, alpha=0.7)\n",
    "axes[1, 0].scatter(kmeans_moons.cluster_centers_[:, 0], kmeans_moons.cluster_centers_[:, 1],\n",
    "                   c='black', marker='X', s=200, edgecolors='white', linewidth=2)\n",
    "axes[1, 0].set_title('Moons: K-Means FALLISCE')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# ============================================\n",
    "# CASO 2: Cerchi concentrici\n",
    "# ============================================\n",
    "X_circles, y_circles = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
    "\n",
    "kmeans_circles = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "labels_circles = kmeans_circles.fit_predict(X_circles)\n",
    "\n",
    "axes[0, 1].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap='coolwarm', s=40, alpha=0.7)\n",
    "axes[0, 1].set_title('Cerchi: Gruppi VERI')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].scatter(X_circles[:, 0], X_circles[:, 1], c=labels_circles, cmap='coolwarm', s=40, alpha=0.7)\n",
    "axes[1, 1].scatter(kmeans_circles.cluster_centers_[:, 0], kmeans_circles.cluster_centers_[:, 1],\n",
    "                   c='black', marker='X', s=200, edgecolors='white', linewidth=2)\n",
    "axes[1, 1].set_title('Cerchi: K-Means FALLISCE')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# ============================================\n",
    "# CASO 3: Cluster di dimensioni molto diverse\n",
    "# ============================================\n",
    "# Un cluster grande, uno piccolo\n",
    "X_big = np.random.randn(280, 2) * 2 + np.array([0, 0])\n",
    "X_small = np.random.randn(20, 2) * 0.3 + np.array([6, 0])\n",
    "X_unbalanced = np.vstack([X_big, X_small])\n",
    "y_unbalanced = np.array([0]*280 + [1]*20)\n",
    "\n",
    "kmeans_unbalanced = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "labels_unbalanced = kmeans_unbalanced.fit_predict(X_unbalanced)\n",
    "\n",
    "axes[0, 2].scatter(X_unbalanced[:, 0], X_unbalanced[:, 1], c=y_unbalanced, cmap='coolwarm', s=40, alpha=0.7)\n",
    "axes[0, 2].set_title('Sbilanciati: Gruppi VERI\\n(280 vs 20 punti)')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 2].scatter(X_unbalanced[:, 0], X_unbalanced[:, 1], c=labels_unbalanced, cmap='coolwarm', s=40, alpha=0.7)\n",
    "axes[1, 2].scatter(kmeans_unbalanced.cluster_centers_[:, 0], kmeans_unbalanced.cluster_centers_[:, 1],\n",
    "                   c='black', marker='X', s=200, edgecolors='white', linewidth=2)\n",
    "# Conta errori\n",
    "errori = min((labels_unbalanced != y_unbalanced).sum(), \n",
    "             (labels_unbalanced != (1-y_unbalanced)).sum())\n",
    "axes[1, 2].set_title(f'Sbilanciati: K-Means\\n(errori: {errori})')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Etichette righe\n",
    "axes[0, 0].set_ylabel('GROUND TRUTH', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('K-MEANS', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# SPIEGAZIONE\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"PERCH√â K-MEANS FALLISCE IN QUESTI CASI?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "üìå CASO 1 - MOONS (lune):\n",
    "   I cluster hanno forma a \"banana\", non sferica.\n",
    "   K-Means taglia verticalmente invece di seguire la curva.\n",
    "   ‚Üí Alternativa: DBSCAN (Lezione 23)\n",
    "\n",
    "üìå CASO 2 - CERCHI CONCENTRICI:\n",
    "   Un cluster √® dentro l'altro.\n",
    "   La distanza euclidea non pu√≤ separare cerchi concentrici.\n",
    "   ‚Üí Alternativa: Spectral Clustering o DBSCAN\n",
    "\n",
    "üìå CASO 3 - CLUSTER SBILANCIATI:\n",
    "   Un cluster ha 280 punti, l'altro 20.\n",
    "   K-Means tende a bilanciare, \"rubando\" punti dal cluster grande.\n",
    "   ‚Üí Alternativa: DBSCAN o algoritmi density-based\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40bce1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Demo 4: L'effetto degli outlier sui centroidi\n",
    "\n",
    "Mostriamo come un singolo outlier pu√≤ distorcere significativamente il risultato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4eb0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMO 4: L'effetto degli outlier\n",
    "# ============================================\n",
    "# Obiettivo: mostrare come gli outlier distorcono i centroidi\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Creiamo 2 cluster puliti\n",
    "X_clean = np.vstack([\n",
    "    np.random.randn(100, 2) * 0.5 + np.array([-2, 0]),\n",
    "    np.random.randn(100, 2) * 0.5 + np.array([2, 0])\n",
    "])\n",
    "y_clean = np.array([0]*100 + [1]*100)\n",
    "\n",
    "# Aggiungiamo alcuni outlier\n",
    "outliers = np.array([\n",
    "    [0, 8],     # outlier lontano in alto\n",
    "    [-1, 7],    # altro outlier\n",
    "    [1, 7.5]    # terzo outlier\n",
    "])\n",
    "\n",
    "X_with_outliers = np.vstack([X_clean, outliers])\n",
    "y_with_outliers = np.array([0]*100 + [1]*100 + [2]*3)  # gli outlier come gruppo 2\n",
    "\n",
    "# K-Means su dati puliti\n",
    "kmeans_clean = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "labels_clean = kmeans_clean.fit_predict(X_clean)\n",
    "\n",
    "# K-Means su dati con outlier\n",
    "kmeans_outliers = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "labels_outliers = kmeans_outliers.fit_predict(X_with_outliers)\n",
    "\n",
    "# Visualizzazione\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Senza outlier\n",
    "axes[0].scatter(X_clean[:, 0], X_clean[:, 1], c=labels_clean, cmap='coolwarm', s=50, alpha=0.7)\n",
    "axes[0].scatter(kmeans_clean.cluster_centers_[:, 0], kmeans_clean.cluster_centers_[:, 1],\n",
    "                c='black', marker='X', s=300, edgecolors='white', linewidth=2, label='Centroidi')\n",
    "axes[0].set_title('SENZA outlier\\n(centroidi corretti)')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(-3, 10)\n",
    "\n",
    "# Plot 2: Con outlier\n",
    "# Coloriamo gli outlier in modo diverso\n",
    "colors_outliers = ['blue' if l == 0 else 'red' for l in labels_outliers[:-3]]\n",
    "axes[1].scatter(X_clean[:, 0], X_clean[:, 1], c=colors_outliers, s=50, alpha=0.7)\n",
    "axes[1].scatter(outliers[:, 0], outliers[:, 1], c='green', s=150, marker='*', \n",
    "                edgecolors='black', linewidth=1, label='Outlier')\n",
    "axes[1].scatter(kmeans_outliers.cluster_centers_[:, 0], kmeans_outliers.cluster_centers_[:, 1],\n",
    "                c='black', marker='X', s=300, edgecolors='white', linewidth=2, label='Centroidi')\n",
    "axes[1].set_title('CON outlier\\n(centroidi DISTORTI)')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(-3, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analisi numerica\n",
    "print(\"=\"*60)\n",
    "print(\"CONFRONTO CENTROIDI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nCentroidi SENZA outlier:\")\n",
    "for i, c in enumerate(kmeans_clean.cluster_centers_):\n",
    "    print(f\"  Cluster {i}: ({c[0]:.2f}, {c[1]:.2f})\")\n",
    "\n",
    "print(\"\\nCentroidi CON outlier:\")\n",
    "for i, c in enumerate(kmeans_outliers.cluster_centers_):\n",
    "    print(f\"  Cluster {i}: ({c[0]:.2f}, {c[1]:.2f})\")\n",
    "\n",
    "# Calcola lo spostamento\n",
    "print(\"\\nSpostamento dei centroidi:\")\n",
    "for i in range(2):\n",
    "    spostamento = np.sqrt(np.sum((kmeans_clean.cluster_centers_[i] - \n",
    "                                   kmeans_outliers.cluster_centers_[i])**2))\n",
    "    print(f\"  Cluster {i}: spostamento = {spostamento:.2f}\")\n",
    "\n",
    "print(\"\"\"\n",
    "‚ö†Ô∏è ATTENZIONE:\n",
    "Gli outlier hanno \"tirato\" uno dei centroidi verso l'alto.\n",
    "Questo pu√≤ causare assegnazioni sbagliate ai confini dei cluster.\n",
    "\n",
    "üí° SOLUZIONI:\n",
    "1. Rimuovere gli outlier prima del clustering\n",
    "2. Usare algoritmi robusti come DBSCAN (identifica outlier come rumore)\n",
    "3. Usare K-Medians invece di K-Means (meno sensibile)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0421e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Parte 4 ‚Äî Esercizi Svolti\n",
    "\n",
    "---\n",
    "\n",
    "## Esercizio 20.1 ‚Äî Calcolo manuale della distanza euclidea e del centroide\n",
    "\n",
    "**Consegna:**\n",
    "Dati i seguenti punti in 2D appartenenti a un cluster:\n",
    "- A = (1, 2)\n",
    "- B = (3, 4)\n",
    "- C = (2, 1)\n",
    "- D = (4, 3)\n",
    "\n",
    "1. Calcola il centroide del cluster\n",
    "2. Calcola la distanza euclidea di ogni punto dal centroide\n",
    "3. Calcola l'inertia (somma delle distanze al quadrato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de2a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 20.1 ‚Äî SOLUZIONE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ESERCIZIO 20.1 ‚Äî Calcolo manuale\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Definiamo i punti\n",
    "punti = np.array([\n",
    "    [1, 2],  # A\n",
    "    [3, 4],  # B\n",
    "    [2, 1],  # C\n",
    "    [4, 3]   # D\n",
    "])\n",
    "nomi = ['A', 'B', 'C', 'D']\n",
    "\n",
    "print(\"\\nPunti del cluster:\")\n",
    "for nome, punto in zip(nomi, punti):\n",
    "    print(f\"  {nome} = ({punto[0]}, {punto[1]})\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 1: Calcolo del centroide\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PASSO 1: Calcolo del centroide\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Il centroide √® la media delle coordinate\n",
    "centroide_x = np.mean(punti[:, 0])\n",
    "centroide_y = np.mean(punti[:, 1])\n",
    "centroide = np.array([centroide_x, centroide_y])\n",
    "\n",
    "print(f\"\"\"\n",
    "Formula: Œº = (1/n) * Œ£ x_i\n",
    "\n",
    "Calcolo x:\n",
    "  Œº_x = (1 + 3 + 2 + 4) / 4 = 10 / 4 = {centroide_x}\n",
    "\n",
    "Calcolo y:\n",
    "  Œº_y = (2 + 4 + 1 + 3) / 4 = 10 / 4 = {centroide_y}\n",
    "\n",
    "‚úÖ Centroide: ({centroide_x}, {centroide_y})\n",
    "\"\"\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 2: Calcolo delle distanze\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"PASSO 2: Distanza di ogni punto dal centroide\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nFormula: d(P, Œº) = ‚àö[(x_p - Œº_x)¬≤ + (y_p - Œº_y)¬≤]\\n\")\n",
    "\n",
    "distanze = []\n",
    "for nome, punto in zip(nomi, punti):\n",
    "    dx = punto[0] - centroide[0]\n",
    "    dy = punto[1] - centroide[1]\n",
    "    d = np.sqrt(dx**2 + dy**2)\n",
    "    distanze.append(d)\n",
    "    \n",
    "    print(f\"Punto {nome} = ({punto[0]}, {punto[1]}):\")\n",
    "    print(f\"  dx = {punto[0]} - {centroide[0]} = {dx}\")\n",
    "    print(f\"  dy = {punto[1]} - {centroide[1]} = {dy}\")\n",
    "    print(f\"  d = ‚àö[({dx})¬≤ + ({dy})¬≤] = ‚àö[{dx**2} + {dy**2}] = ‚àö{dx**2 + dy**2} = {d:.3f}\")\n",
    "    print()\n",
    "\n",
    "# ============================================\n",
    "# PASSO 3: Calcolo dell'inertia\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"PASSO 3: Calcolo dell'Inertia (WCSS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nFormula: Inertia = Œ£ ||x_i - Œº||¬≤\\n\")\n",
    "\n",
    "distanze_quadrato = [d**2 for d in distanze]\n",
    "inertia = sum(distanze_quadrato)\n",
    "\n",
    "print(\"Distanze al quadrato:\")\n",
    "for nome, d, d2 in zip(nomi, distanze, distanze_quadrato):\n",
    "    print(f\"  {nome}: {d:.3f}¬≤ = {d2:.3f}\")\n",
    "\n",
    "print(f\"\\nInertia = {' + '.join([f'{d2:.3f}' for d2 in distanze_quadrato])}\")\n",
    "print(f\"        = {inertia:.3f}\")\n",
    "print(f\"\\n‚úÖ Inertia del cluster: {inertia:.3f}\")\n",
    "\n",
    "# Verifica con numpy\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFICA CON NUMPY\")\n",
    "print(\"=\"*70)\n",
    "inertia_numpy = np.sum((punti - centroide)**2)\n",
    "print(f\"Inertia calcolata con numpy: {inertia_numpy:.3f}\")\n",
    "print(\"‚úÖ I calcoli sono corretti!\" if np.isclose(inertia, inertia_numpy) else \"‚ùå Errore nei calcoli\")\n",
    "\n",
    "# Visualizzazione\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(punti[:, 0], punti[:, 1], c='blue', s=100, label='Punti')\n",
    "ax.scatter(centroide[0], centroide[1], c='red', marker='X', s=200, label='Centroide')\n",
    "\n",
    "# Linee dal centroide ai punti\n",
    "for nome, punto, d in zip(nomi, punti, distanze):\n",
    "    ax.plot([centroide[0], punto[0]], [centroide[1], punto[1]], 'k--', alpha=0.5)\n",
    "    ax.annotate(f'{nome}\\nd={d:.2f}', (punto[0], punto[1]), textcoords=\"offset points\", \n",
    "                xytext=(10, 5), fontsize=10)\n",
    "\n",
    "ax.set_title(f'Cluster con centroide\\nInertia = {inertia:.3f}')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c0fc2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Esercizio 20.2 ‚Äî Clustering di clienti e-commerce\n",
    "\n",
    "**Consegna:**\n",
    "Un e-commerce ha i seguenti dati sui clienti:\n",
    "- `frequenza`: numero di acquisti negli ultimi 12 mesi\n",
    "- `spesa_media`: valore medio per acquisto (‚Ç¨)\n",
    "- `recency`: giorni dall'ultimo acquisto\n",
    "\n",
    "Applica K-Means per segmentare i clienti in 3 gruppi. Interpreta i cluster trovati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f76db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 20.2 ‚Äî SOLUZIONE\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ESERCIZIO 20.2 ‚Äî Segmentazione clienti e-commerce\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# PASSO 1: Generazione dataset realistico\n",
    "# ============================================\n",
    "np.random.seed(42)\n",
    "n_clienti = 300\n",
    "\n",
    "# Creiamo 3 profili di clienti (che l'algoritmo dovr√† scoprire)\n",
    "# Profilo 1: Clienti PREMIUM - frequenti, alta spesa, recenti\n",
    "premium = pd.DataFrame({\n",
    "    'frequenza': np.random.normal(25, 5, 80).clip(10, 40),\n",
    "    'spesa_media': np.random.normal(150, 30, 80).clip(80, 250),\n",
    "    'recency': np.random.normal(10, 5, 80).clip(1, 30)\n",
    "})\n",
    "\n",
    "# Profilo 2: Clienti OCCASIONALI - rari, spesa media, recenti\n",
    "occasionali = pd.DataFrame({\n",
    "    'frequenza': np.random.normal(5, 2, 120).clip(1, 12),\n",
    "    'spesa_media': np.random.normal(50, 15, 120).clip(20, 100),\n",
    "    'recency': np.random.normal(45, 15, 120).clip(15, 90)\n",
    "})\n",
    "\n",
    "# Profilo 3: Clienti DORMIENTI - erano attivi, ora inattivi\n",
    "dormienti = pd.DataFrame({\n",
    "    'frequenza': np.random.normal(12, 4, 100).clip(3, 25),\n",
    "    'spesa_media': np.random.normal(80, 25, 100).clip(30, 150),\n",
    "    'recency': np.random.normal(150, 40, 100).clip(90, 250)\n",
    "})\n",
    "\n",
    "# Combiniamo\n",
    "df_clienti = pd.concat([premium, occasionali, dormienti], ignore_index=True)\n",
    "\n",
    "print(\"\\nüìä Dataset generato:\")\n",
    "print(df_clienti.describe().round(1))\n",
    "\n",
    "# ============================================\n",
    "# PASSO 2: Preprocessing - SCALING\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PASSO 2: Scaling delle feature\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nRange PRIMA dello scaling:\")\n",
    "for col in df_clienti.columns:\n",
    "    print(f\"  {col}: {df_clienti[col].min():.1f} - {df_clienti[col].max():.1f}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_clienti)\n",
    "\n",
    "print(\"\\nDopo StandardScaler: media‚âà0, std‚âà1 per tutte le feature\")\n",
    "print(\"‚úÖ Le feature hanno ora lo stesso peso nelle distanze\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 3: Applicazione K-Means\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PASSO 3: K-Means con K=3\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df_clienti['cluster'] = labels\n",
    "\n",
    "print(f\"\\nRisultato:\")\n",
    "print(f\"  Punti per cluster: {np.bincount(labels)}\")\n",
    "print(f\"  Inertia: {kmeans.inertia_:.2f}\")\n",
    "print(f\"  Silhouette: {silhouette_score(X_scaled, labels):.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 4: Interpretazione dei cluster\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PASSO 4: Interpretazione dei cluster\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Statistiche per cluster\n",
    "cluster_stats = df_clienti.groupby('cluster').agg({\n",
    "    'frequenza': ['mean', 'std'],\n",
    "    'spesa_media': ['mean', 'std'],\n",
    "    'recency': ['mean', 'std']\n",
    "}).round(1)\n",
    "\n",
    "print(\"\\nStatistiche per cluster:\")\n",
    "print(cluster_stats)\n",
    "\n",
    "# Interpretazione automatica basata sulle caratteristiche\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETAZIONE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for cluster_id in sorted(df_clienti['cluster'].unique()):\n",
    "    cluster_data = df_clienti[df_clienti['cluster'] == cluster_id]\n",
    "    freq_media = cluster_data['frequenza'].mean()\n",
    "    spesa_media = cluster_data['spesa_media'].mean()\n",
    "    recency_media = cluster_data['recency'].mean()\n",
    "    n_clienti_cluster = len(cluster_data)\n",
    "    \n",
    "    print(f\"\\nüìå CLUSTER {cluster_id} ({n_clienti_cluster} clienti):\")\n",
    "    print(f\"   Frequenza media: {freq_media:.1f} acquisti/anno\")\n",
    "    print(f\"   Spesa media: ‚Ç¨{spesa_media:.1f}\")\n",
    "    print(f\"   Recency media: {recency_media:.1f} giorni\")\n",
    "    \n",
    "    # Interpretazione\n",
    "    if freq_media > 20 and recency_media < 30:\n",
    "        print(\"   ‚Üí PROFILO: üåü CLIENTI PREMIUM (fedeli, alto valore, attivi)\")\n",
    "        print(\"   ‚Üí AZIONE: Programma loyalty, accesso anticipato, offerte esclusive\")\n",
    "    elif recency_media > 100:\n",
    "        print(\"   ‚Üí PROFILO: üò¥ CLIENTI DORMIENTI (erano attivi, ora inattivi)\")\n",
    "        print(\"   ‚Üí AZIONE: Campagna win-back, email personalizzata, sconto rientro\")\n",
    "    else:\n",
    "        print(\"   ‚Üí PROFILO: üîÑ CLIENTI OCCASIONALI (acquisti sporadici)\")\n",
    "        print(\"   ‚Üí AZIONE: Aumentare engagement, cross-sell, newsletter\")\n",
    "\n",
    "# ============================================\n",
    "# PASSO 5: Visualizzazione\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PASSO 5: Visualizzazione\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Frequenza vs Spesa\n",
    "scatter1 = axes[0].scatter(df_clienti['frequenza'], df_clienti['spesa_media'], \n",
    "                            c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "axes[0].set_xlabel('Frequenza (acquisti/anno)')\n",
    "axes[0].set_ylabel('Spesa media (‚Ç¨)')\n",
    "axes[0].set_title('Frequenza vs Spesa')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Frequenza vs Recency\n",
    "scatter2 = axes[1].scatter(df_clienti['frequenza'], df_clienti['recency'], \n",
    "                            c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "axes[1].set_xlabel('Frequenza (acquisti/anno)')\n",
    "axes[1].set_ylabel('Recency (giorni)')\n",
    "axes[1].set_title('Frequenza vs Recency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Spesa vs Recency\n",
    "scatter3 = axes[2].scatter(df_clienti['spesa_media'], df_clienti['recency'], \n",
    "                            c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "axes[2].set_xlabel('Spesa media (‚Ç¨)')\n",
    "axes[2].set_ylabel('Recency (giorni)')\n",
    "axes[2].set_title('Spesa vs Recency')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.colorbar(scatter3, ax=axes, label='Cluster', shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ La segmentazione ha identificato 3 profili distinti di clienti\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09934a85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìù Esercizio 20.3 ‚Äî Confronto: Con e senza Scaling\n",
    "\n",
    "**Consegna:**\n",
    "Hai un dataset con feature su scale molto diverse. Dimostra visivamente e quantitativamente la differenza tra applicare K-Means con e senza StandardScaler.\n",
    "\n",
    "**Cosa deve emergere:**\n",
    "1. Quanto cambiano le assegnazioni ai cluster\n",
    "2. Quanto cambia la silhouette score\n",
    "3. Perch√© lo scaling √® cruciale\n",
    "\n",
    "**Dataset:**\n",
    "```python\n",
    "feature_1 = [2, 3, 8, 9, 100]     # Scala: 2-100\n",
    "feature_2 = [0.01, 0.02, 0.08, 0.09, 0.5]  # Scala: 0.01-0.5\n",
    "```\n",
    "\n",
    "**Hint:** Senza scaling, quale feature \"domina\" la distanza euclidea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e38c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ESERCIZIO 20.3 ‚Äî SOLUZIONE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ESERCIZIO 20.3 ‚Äî Effetto dello scaling su K-Means\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dataset con scale diverse\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generiamo dati con 3 cluster \"reali\" ma scale molto diverse\n",
    "# Cluster 1\n",
    "c1_f1 = np.random.normal(10, 2, 30)    # Scala grande\n",
    "c1_f2 = np.random.normal(0.1, 0.02, 30) # Scala piccola\n",
    "\n",
    "# Cluster 2\n",
    "c2_f1 = np.random.normal(50, 5, 30)\n",
    "c2_f2 = np.random.normal(0.5, 0.05, 30)\n",
    "\n",
    "# Cluster 3\n",
    "c3_f1 = np.random.normal(90, 3, 30)\n",
    "c3_f2 = np.random.normal(0.9, 0.03, 30)\n",
    "\n",
    "feature_1 = np.concatenate([c1_f1, c2_f1, c3_f1])\n",
    "feature_2 = np.concatenate([c1_f2, c2_f2, c3_f2])\n",
    "true_labels = np.array([0]*30 + [1]*30 + [2]*30)\n",
    "\n",
    "X = np.column_stack([feature_1, feature_2])\n",
    "\n",
    "print(\"\\nüìä Range delle feature:\")\n",
    "print(f\"  Feature 1: {feature_1.min():.1f} - {feature_1.max():.1f} (range: {feature_1.max()-feature_1.min():.1f})\")\n",
    "print(f\"  Feature 2: {feature_2.min():.3f} - {feature_2.max():.3f} (range: {feature_2.max()-feature_2.min():.3f})\")\n",
    "print(f\"\\n‚ö†Ô∏è  Feature 1 ha un range ~100x pi√π grande di Feature 2!\")\n",
    "\n",
    "# ============================================\n",
    "# SENZA SCALING\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST A: K-Means SENZA Scaling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "kmeans_no_scale = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_no_scale = kmeans_no_scale.fit_predict(X)\n",
    "\n",
    "silhouette_no_scale = silhouette_score(X, labels_no_scale)\n",
    "\n",
    "print(f\"\\nRisultato SENZA scaling:\")\n",
    "print(f\"  Distribuzione: {np.bincount(labels_no_scale)}\")\n",
    "print(f\"  Silhouette: {silhouette_no_scale:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# CON SCALING\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST B: K-Means CON StandardScaler\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans_scaled = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_scaled = kmeans_scaled.fit_predict(X_scaled)\n",
    "\n",
    "silhouette_scaled = silhouette_score(X_scaled, labels_scaled)\n",
    "\n",
    "print(f\"\\nRisultato CON scaling:\")\n",
    "print(f\"  Distribuzione: {np.bincount(labels_scaled)}\")\n",
    "print(f\"  Silhouette: {silhouette_scaled:.3f}\")\n",
    "\n",
    "# ============================================\n",
    "# CONFRONTO\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFRONTO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Quanti punti cambiano cluster?\n",
    "n_changed = np.sum(labels_no_scale != labels_scaled)\n",
    "print(f\"\\nüîÑ Punti che cambiano cluster: {n_changed}/{len(X)} ({100*n_changed/len(X):.1f}%)\")\n",
    "print(f\"\\nüìà Silhouette SENZA scaling: {silhouette_no_scale:.3f}\")\n",
    "print(f\"üìà Silhouette CON scaling:   {silhouette_scaled:.3f}\")\n",
    "print(f\"üìà Miglioramento:            +{100*(silhouette_scaled-silhouette_no_scale)/silhouette_no_scale:.1f}%\")\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZZAZIONE\n",
    "# ============================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Dati originali con cluster veri\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=true_labels, cmap='viridis', s=60, alpha=0.7)\n",
    "axes[0].set_xlabel('Feature 1 (scala grande)')\n",
    "axes[0].set_ylabel('Feature 2 (scala piccola)')\n",
    "axes[0].set_title('Cluster VERI (ground truth)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: SENZA scaling\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=labels_no_scale, cmap='viridis', s=60, alpha=0.7)\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].set_title(f'SENZA Scaling\\nSilhouette: {silhouette_no_scale:.3f}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: CON scaling (visualizziamo i dati originali con le label scalate)\n",
    "axes[2].scatter(X[:, 0], X[:, 1], c=labels_scaled, cmap='viridis', s=60, alpha=0.7)\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "axes[2].set_title(f'CON Scaling\\nSilhouette: {silhouette_scaled:.3f}')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# LEZIONE CHIAVE\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üí° LEZIONE CHIAVE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "SENZA SCALING:\n",
    "  ‚Ä¢ Feature 1 (range ~80) DOMINA completamente la distanza euclidea\n",
    "  ‚Ä¢ Feature 2 (range ~0.8) viene praticamente IGNORATA\n",
    "  ‚Ä¢ K-Means crea cluster basati quasi solo su Feature 1\n",
    "  \n",
    "CON SCALING:\n",
    "  ‚Ä¢ Entrambe le feature contribuiscono equamente\n",
    "  ‚Ä¢ I cluster riflettono la vera struttura dei dati\n",
    "  ‚Ä¢ La silhouette score migliora\n",
    "  \n",
    "REGOLA: StandardScaler PRIMA di K-Means √® quasi sempre necessario!\n",
    "  Eccezione: quando le scale diverse sono significative (es. metri vs km)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacefcf1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Cosa Portarsi a Casa\n",
    "\n",
    "### ‚úÖ Concetti Fondamentali\n",
    "\n",
    "| Concetto | Definizione |\n",
    "|----------|-------------|\n",
    "| **K-Means** | Algoritmo che partiziona i dati in K cluster minimizzando l'inertia |\n",
    "| **Centroide** | Punto medio di un cluster, calcolato come media delle coordinate |\n",
    "| **Inertia (WCSS)** | Somma delle distanze quadrate dai punti ai centroidi |\n",
    "| **k-means++** | Inizializzazione intelligente che distribuisce i centroidi iniziali |\n",
    "\n",
    "### ‚ö†Ô∏è Errori Comuni da Evitare\n",
    "\n",
    "| Errore | Problema | Soluzione |\n",
    "|--------|----------|-----------|\n",
    "| Non scalare i dati | Le feature con range grande dominano | Sempre `StandardScaler` prima |\n",
    "| Scegliere K a caso | Cluster non significativi | Usare Elbow + Silhouette (Lezione 21) |\n",
    "| Ignorare la forma dei cluster | K-Means fallisce con forme non convesse | Usare DBSCAN per forme arbitrarie (Lezione 23) |\n",
    "| Una sola inizializzazione | Risultato dipende dal caso | Usare `n_init=10` o superiore |\n",
    "| Non interpretare i cluster | Clustering inutile senza significato | Sempre analizzare le caratteristiche |\n",
    "\n",
    "### üîó Ponte verso la Lezione 21\n",
    "\n",
    "**Problema aperto:** Come scegliere K in modo sistematico?\n",
    "\n",
    "In questa lezione abbiamo sempre \"saputo\" K a priori. Ma nella realt√†:\n",
    "- Non sappiamo quanti cluster ci siano\n",
    "- Diverse scelte di K danno risultati diversi\n",
    "- Servono metriche oggettive per decidere\n",
    "\n",
    "**Nella prossima lezione:**\n",
    "- Metodo del Gomito (Elbow Method)\n",
    "- Silhouette Analysis approfondita\n",
    "- Gap Statistic\n",
    "- Trade-off interpretabilit√† vs performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382a2ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã BIGNAMI ‚Äî Lezione 20: K-Means Clustering\n",
    "\n",
    "### Definizioni Essenziali\n",
    "\n",
    "| Termine | Definizione |\n",
    "|---------|-------------|\n",
    "| **K-Means** | Algoritmo di clustering che partiziona N punti in K cluster, assegnando ogni punto al cluster con centroide pi√π vicino |\n",
    "| **Centroide** | Centro geometrico di un cluster: $\\mathbf{c}_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\mathbf{x}_i$ |\n",
    "| **Inertia** | Within-Cluster Sum of Squares: $\\text{WCSS} = \\sum_{k=1}^{K} \\sum_{i \\in C_k} \\|\\mathbf{x}_i - \\mathbf{c}_k\\|^2$ |\n",
    "| **Silhouette** | Misura di qualit√† del clustering: $s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$ |\n",
    "| **k-means++** | Inizializzazione che sceglie centroidi iniziali distanti tra loro per evitare minimi locali |\n",
    "\n",
    "---\n",
    "\n",
    "### Formule Chiave\n",
    "\n",
    "**Distanza Euclidea:**\n",
    "$$d(\\mathbf{x}, \\mathbf{c}) = \\sqrt{\\sum_{j=1}^{d} (x_j - c_j)^2}$$\n",
    "\n",
    "**Centroide di un cluster:**\n",
    "$$\\mathbf{c}_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\mathbf{x}_i$$\n",
    "\n",
    "**Inertia (WCSS):**\n",
    "$$J = \\sum_{k=1}^{K} \\sum_{i \\in C_k} \\|\\mathbf{x}_i - \\mathbf{c}_k\\|^2$$\n",
    "\n",
    "---\n",
    "\n",
    "### Checklist K-Means\n",
    "\n",
    "```\n",
    "‚ñ° Feature selezionate (solo numeriche rilevanti)\n",
    "‚ñ° StandardScaler applicato\n",
    "‚ñ° K scelto con criterio (Elbow/Silhouette - Lezione 21)\n",
    "‚ñ° n_init >= 10 per evitare minimi locali\n",
    "‚ñ° Cluster interpretati e nominati\n",
    "‚ñ° Visualizzazione per validare i risultati\n",
    "‚ñ° Outlier gestiti se necessario\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Template di Codice\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 1. Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. K-Means\n",
    "kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# 3. Valutazione\n",
    "print(f\"Inertia: {kmeans.inertia_:.2f}\")\n",
    "print(f\"Silhouette: {silhouette_score(X_scaled, labels):.3f}\")\n",
    "\n",
    "# 4. Interpretazione\n",
    "df['cluster'] = labels\n",
    "df.groupby('cluster').mean()  # Profilo dei cluster\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Quando K-Means Funziona vs Quando Fallisce\n",
    "\n",
    "| ‚úÖ Funziona Bene | ‚ùå Fallisce |\n",
    "|-----------------|-------------|\n",
    "| Cluster sferici/convessi | Forme arbitrarie (mezzalune, anelli) |\n",
    "| Cluster di dimensioni simili | Cluster molto sbilanciati |\n",
    "| Dati ben separati | Cluster sovrapposti |\n",
    "| Pochi outlier | Molti outlier (spostano i centroidi) |\n",
    "\n",
    "---\n",
    "\n",
    "### Parametri Chiave di KMeans\n",
    "\n",
    "| Parametro | Default | Significato |\n",
    "|-----------|---------|-------------|\n",
    "| `n_clusters` | 8 | Numero di cluster K |\n",
    "| `init` | 'k-means++' | Metodo di inizializzazione |\n",
    "| `n_init` | 10 | Numero di inizializzazioni diverse |\n",
    "| `max_iter` | 300 | Iterazioni massime per convergenza |\n",
    "| `random_state` | None | Seed per riproducibilit√† |\n",
    "\n",
    "---\n",
    "\n",
    "### Flusso Mentale\n",
    "\n",
    "```\n",
    "DATI ‚Üí StandardScaler ‚Üí KMeans(K) ‚Üí Valutazione ‚Üí Interpretazione\n",
    "         ‚Üì                 ‚Üì           ‚Üì              ‚Üì\n",
    "      Normalizza        Itera      Silhouette    Significato\n",
    "      le scale       converge?     Inertia       business\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "*\"K-Means trova K centroidi che minimizzano le distanze intra-cluster. \n",
    " Funziona meglio con cluster sferici, dati scalati, e K scelto con criterio.\"*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
