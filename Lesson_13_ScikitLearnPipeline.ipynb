{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57abc05c",
   "metadata": {},
   "source": [
    "# LEZIONE 13 - Pipeline di Scikit-Learn\n",
    "\n",
    "## Obiettivi della Lezione\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "1. **Comprendere** perche le Pipeline sono essenziali nei progetti reali\n",
    "2. **Costruire** Pipeline che combinano preprocessing e modello\n",
    "3. **Usare** ColumnTransformer per gestire feature diverse\n",
    "4. **Evitare** il data leakage con preprocessing corretto\n",
    "5. **Strutturare** workflow di ML riproducibili e manutenibili\n",
    "\n",
    "## Prerequisiti\n",
    "\n",
    "| Concetto | Dove lo trovi |\n",
    "|----------|---------------|\n",
    "| Train/Test Split | Lezione 9 |\n",
    "| Cross-Validation | Lezione 10 |\n",
    "| Overfitting e Data Leakage | Lezione 12 |\n",
    "| StandardScaler e preprocessing | Lezione 9 |\n",
    "\n",
    "## Indice\n",
    "\n",
    "1. SEZIONE 1 - Teoria\n",
    "2. SEZIONE 2 - Mappa Mentale\n",
    "3. SEZIONE 3 - Quaderno Dimostrativo\n",
    "4. SEZIONE 4 - Metodi Principali\n",
    "5. SEZIONE 5 - Glossario\n",
    "6. SEZIONE 6 - Errori Comuni\n",
    "7. SEZIONE 7 - Conclusione\n",
    "8. SEZIONE 8 - Checklist\n",
    "9. SEZIONE 9 - Changelog\n",
    "\n",
    "---\n",
    "\n",
    "## Librerie Utilizzate\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f54f30",
   "metadata": {},
   "source": [
    "# SEZIONE 1 - Teoria\n",
    "\n",
    "## 1.1 Il Problema: Perche Servono le Pipeline?\n",
    "\n",
    "### Scenario Tipico (SBAGLIATO)\n",
    "\n",
    "```python\n",
    "# ERRORE COMUNE: preprocessing su TUTTI i dati prima dello split\n",
    "\n",
    "# 1. Carichiamo i dati\n",
    "X = load_data()\n",
    "\n",
    "# 2. Preprocessing su TUTTO il dataset\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # ERRORE! fit su tutti i dati\n",
    "\n",
    "# 3. Poi facciamo lo split\n",
    "X_train, X_test = train_test_split(X_scaled)\n",
    "\n",
    "# 4. Alleniamo il modello\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Perche e Sbagliato? DATA LEAKAGE\n",
    "\n",
    "```\n",
    "            PRIMA DELLO SPLIT\n",
    "            -----------------\n",
    "               +----------+\n",
    "    Tutti i -> |  Scaler  | -> fit() vede ANCHE i dati di test!\n",
    "    dati       |   fit()  |\n",
    "               +----------+\n",
    "                    |\n",
    "            +-------+-------+\n",
    "            v               v\n",
    "         Train            Test\n",
    "         \n",
    "Il modello \"bara\": ha visto informazioni sul test set!\n",
    "```\n",
    "\n",
    "**Conseguenze:**\n",
    "- Le performance sul test set sono **ottimistiche**\n",
    "- In produzione il modello avra performance **peggiori**\n",
    "- Hai violato la regola fondamentale del ML\n",
    "\n",
    "### Scenario Corretto (MA SCOMODO)\n",
    "\n",
    "```python\n",
    "# CORRETTO ma verboso e error-prone\n",
    "\n",
    "# 1. Prima lo split\n",
    "X_train, X_test = train_test_split(X)\n",
    "\n",
    "# 2. Fit SOLO su train\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# 3. Transform su entrambi\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Stessa cosa per ogni step di preprocessing...\n",
    "imputer = SimpleImputer()\n",
    "imputer.fit(X_train_scaled)\n",
    "X_train_imputed = imputer.transform(X_train_scaled)\n",
    "X_test_imputed = imputer.transform(X_test_scaled)\n",
    "\n",
    "# 5. Allena modello\n",
    "model.fit(X_train_imputed, y_train)\n",
    "\n",
    "# 6. Per predizioni su nuovi dati: ripeti tutti i transform!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 La Soluzione: sklearn.pipeline.Pipeline\n",
    "\n",
    "Una **Pipeline** e una sequenza ordinata di trasformazioni + un modello finale, incapsulati in un unico oggetto.\n",
    "\n",
    "```\n",
    "               PIPELINE\n",
    "    +----------------------------------+\n",
    "    |                                  |\n",
    "    |  Step 1      Step 2      Step 3  |\n",
    "    | +--------+  +--------+  +-------+|\n",
    "X ->| |Imputer | ->|Scaler | ->| Model ||-> y_pred\n",
    "    | +--------+  +--------+  +-------+|\n",
    "    |                                  |\n",
    "    +----------------------------------+\n",
    "    \n",
    "    pipe.fit(X_train, y_train)  # allena TUTTO\n",
    "    pipe.predict(X_test)        # applica TUTTO\n",
    "```\n",
    "\n",
    "### Confronto: Senza vs Con Pipeline\n",
    "\n",
    "| Senza Pipeline | Con Pipeline |\n",
    "|----------------|--------------|\n",
    "| Codice frammentato | Oggetto unico |\n",
    "| Facile dimenticare step | Tutti gli step garantiti |\n",
    "| Data leakage possibile | Data leakage impossibile |\n",
    "| Difficile salvare/caricare | Un solo file da salvare |\n",
    "| Cross-validation manuale | CV automatica e corretta |\n",
    "\n",
    "### Sintassi Base\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Metodo 1: esplicito con nomi\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Metodo 2: senza nomi (make_pipeline)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "```\n",
    "\n",
    "### Come Funziona Internamente\n",
    "\n",
    "**Quando chiami pipe.fit(X, y):**\n",
    "1. Ogni transformer chiama `fit_transform()` sui dati\n",
    "2. L'output viene passato al passo successivo\n",
    "3. L'ultimo step (modello) chiama solo `fit()`\n",
    "\n",
    "**Quando chiami pipe.predict(X):**\n",
    "1. Ogni transformer chiama solo `transform()` (NO fit!)\n",
    "2. L'ultimo step chiama `predict()`\n",
    "\n",
    "```\n",
    "fit(X_train, y_train):\n",
    "----------------------\n",
    "X_train -> [Imputer.fit_transform] -> [Scaler.fit_transform] -> [Model.fit]\n",
    "\n",
    "predict(X_test):\n",
    "----------------\n",
    "X_test -> [Imputer.transform] -> [Scaler.transform] -> [Model.predict] -> y_pred\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 ColumnTransformer: Pipeline per Dati Eterogenei\n",
    "\n",
    "Nei dataset reali abbiamo spesso:\n",
    "- **Colonne numeriche** -> SimpleImputer(mean) + StandardScaler\n",
    "- **Colonne categoriche** -> SimpleImputer(most_frequent) + OneHotEncoder\n",
    "- **Colonne ordinali** -> OrdinalEncoder\n",
    "- **Colonne da ignorare** -> passthrough o drop\n",
    "\n",
    "### Sintassi ColumnTransformer\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('name_1', transformer_1, columns_1),\n",
    "    ('name_2', transformer_2, columns_2),\n",
    "    ...\n",
    "], remainder='drop')  # cosa fare con le altre colonne\n",
    "```\n",
    "\n",
    "### Parametro remainder\n",
    "\n",
    "| Valore | Comportamento |\n",
    "|--------|---------------|\n",
    "| `'drop'` (default) | Scarta le colonne non specificate |\n",
    "| `'passthrough'` | Mantiene le colonne non trasformate |\n",
    "| transformer | Applica un transformer alle colonne rimanenti |\n",
    "\n",
    "### Schema Completo con ColumnTransformer\n",
    "\n",
    "```\n",
    "+--------------------------------------------------------+\n",
    "|                    ColumnTransformer                    |\n",
    "|  +-----------------+    +------------------+            |\n",
    "|  | numeric_pipe    |    | categorical_pipe |            |\n",
    "|  |  Imputer(mean)  |    |  Imputer(freq)   |            |\n",
    "|  |  StandardScaler |    |  OneHotEncoder   |            |\n",
    "|  +--------+--------+    +--------+---------+            |\n",
    "|           |                      |                      |\n",
    "|           +----------+-----------+                      |\n",
    "|                      | concatena                        |\n",
    "|                      v                                  |\n",
    "|            [features trasformate]                       |\n",
    "+--------------------------------------------------------+\n",
    "                       |\n",
    "                       v\n",
    "              +---------------+\n",
    "              |    Modello    |\n",
    "              | (RandomForest)|\n",
    "              +---------------+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4 Pipeline con GridSearchCV\n",
    "\n",
    "Quando usi GridSearchCV con una Pipeline, puoi ottimizzare parametri di **qualsiasi step**.\n",
    "\n",
    "### Sintassi per Accedere ai Parametri\n",
    "\n",
    "```python\n",
    "# Con Pipeline esplicita\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svc__C': [0.1, 1, 10],        # stepname__param\n",
    "    'svc__kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Con make_pipeline (nomi automatici lowercase)\n",
    "pipe = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "param_grid = {\n",
    "    'standardscaler__with_mean': [True, False],\n",
    "    'svc__C': [0.1, 1, 10]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1.5 Salvare e Caricare Pipeline\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Salva TUTTA la pipeline (preprocessing + modello)\n",
    "joblib.dump(full_pipeline, 'model.pkl')\n",
    "\n",
    "# Carica e predici su nuovi dati\n",
    "loaded_pipe = joblib.load('model.pkl')\n",
    "predictions = loaded_pipe.predict(new_data)\n",
    "```\n",
    "\n",
    "Un solo file contiene tutto il workflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da3a2e2",
   "metadata": {},
   "source": [
    "# SEZIONE 2 - Mappa Mentale\n",
    "\n",
    "## Flusso Decisionale: Quando Usare Pipeline\n",
    "\n",
    "```\n",
    "Ho piu di uno step di preprocessing?\n",
    "            |\n",
    "    +-------+-------+\n",
    "    |               |\n",
    "   NO              SI\n",
    "    |               |\n",
    "    v               v\n",
    "Transformer      Farai training/prediction?\n",
    "singolo OK              |\n",
    "                +-------+-------+\n",
    "                |               |\n",
    "               NO              SI\n",
    "                |               |\n",
    "                v               v\n",
    "          Step-by-step      USA PIPELINE!\n",
    "          per EDA\n",
    "```\n",
    "\n",
    "## Checklist: Quando Pipeline e Obbligatoria\n",
    "\n",
    "| Situazione | Pipeline? | Motivo |\n",
    "|------------|-----------|--------|\n",
    "| Cross-validation | SI | Previeni data leakage automaticamente |\n",
    "| Hyperparameter tuning | SI | GridSearchCV lavora con pipeline |\n",
    "| Modello in produzione | SI | Un solo oggetto da serializzare |\n",
    "| Lavoro in team | SI | Codice piu leggibile e manutenibile |\n",
    "| Solo EDA esplorativa | NO | Lavori step-by-step per capire |\n",
    "| Debug preprocessing | NO | Isoli il problema senza pipeline |\n",
    "\n",
    "## Tipi di Pipeline\n",
    "\n",
    "```\n",
    "1. PIPELINE SEMPLICE (solo numeriche)\n",
    "+--------------------------------------------+\n",
    "|  SimpleImputer -> StandardScaler -> Model  |\n",
    "+--------------------------------------------+\n",
    "\n",
    "2. PIPELINE CON COLUMNTRANSFORMER (dati misti)\n",
    "+------------------------------------------------+\n",
    "| ColumnTransformer                              |\n",
    "|   +------------------+  +------------------+   |\n",
    "|   | num_pipeline     |  | cat_pipeline     |   |\n",
    "|   | Imputer->Scaler  |  | Imputer->OneHot  |   |\n",
    "|   +------------------+  +------------------+   |\n",
    "|               |                   |            |\n",
    "|               +--------+----------+            |\n",
    "|                        v                       |\n",
    "+------------------------|------------------------+\n",
    "                         v\n",
    "                     [Model]\n",
    "```\n",
    "\n",
    "## Pattern da Ricordare\n",
    "\n",
    "```\n",
    "PATTERN 1: Pipeline Base\n",
    "--------------------------\n",
    "pipe = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "PATTERN 2: Dati Misti\n",
    "--------------------------\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipe, num_cols),\n",
    "    ('cat', cat_pipe, cat_cols)\n",
    "])\n",
    "\n",
    "full_pipe = Pipeline([\n",
    "    ('prep', preprocessor),\n",
    "    ('model', RandomForest())\n",
    "])\n",
    "\n",
    "PATTERN 3: Con GridSearchCV\n",
    "--------------------------\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "best_model = grid.best_estimator_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13acb57",
   "metadata": {},
   "source": [
    "# SEZIONE 3 - Quaderno Dimostrativo\n",
    "\n",
    "In questa sezione vedremo 5 esercizi pratici:\n",
    "\n",
    "1. **Esercizio 1**: Pipeline Base con Dati Numerici\n",
    "2. **Esercizio 2**: ColumnTransformer per Dati Misti\n",
    "3. **Esercizio 3**: Pipeline con GridSearchCV\n",
    "4. **Esercizio 4**: Confronto Con/Senza Pipeline\n",
    "5. **Esercizio 5**: Pipeline Completa per Produzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ede1c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ESERCIZIO 1: Pipeline Base con Dati Numerici\n",
      "============================================================\n",
      "\n",
      "Dataset: (500, 8)\n",
      "Valori mancanti: 426 (10.7%)\n",
      "\n",
      "Pipeline creata:\n",
      "Pipeline(steps=[('simpleimputer', SimpleImputer()),\n",
      "                ('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(random_state=42))])\n",
      "\n",
      "Cross-Validation Results:\n",
      "  Scores per fold: ['0.870', '0.970', '0.880', '0.910', '0.860']\n",
      "  Mean Accuracy: 0.8980 +/- 0.0397\n",
      "\n",
      "Test Accuracy: 0.8960\n",
      "\n",
      "Micro-checkpoint: Pipeline funziona correttamente!\n"
     ]
    }
   ],
   "source": [
    "# === ESERCIZIO 1: PIPELINE BASE CON DATI NUMERICI ===\n",
    "# Perche: mostriamo come costruire una pipeline semplice per preprocessing + modello\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Creiamo un dataset con valori mancanti\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=8,\n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    flip_y=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Aggiungiamo 10% di valori mancanti\n",
    "mask = np.random.random(X.shape) < 0.10\n",
    "X[mask] = np.nan\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ESERCIZIO 1: Pipeline Base con Dati Numerici\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset: {X.shape}\")\n",
    "print(f\"Valori mancanti: {np.isnan(X).sum()} ({np.isnan(X).mean()*100:.1f}%)\")\n",
    "\n",
    "# Split dei dati\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Creiamo la pipeline: Imputer -> Scaler -> SVC\n",
    "pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    SVC(kernel='rbf', random_state=42)\n",
    ")\n",
    "\n",
    "print(f\"\\nPipeline creata:\")\n",
    "print(pipe)\n",
    "\n",
    "# Cross-validation (internamente applica fit solo su train di ogni fold)\n",
    "cv_scores = cross_val_score(pipe, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"\\nCross-Validation Results:\")\n",
    "print(f\"  Scores per fold: {[f'{s:.3f}' for s in cv_scores]}\")\n",
    "print(f\"  Mean Accuracy: {cv_scores.mean():.4f} +/- {cv_scores.std():.4f}\")\n",
    "\n",
    "# Fit finale e test\n",
    "pipe.fit(X_train, y_train)\n",
    "test_score = pipe.score(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_score:.4f}\")\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert cv_scores.mean() > 0.70, \"CV accuracy troppo bassa\"\n",
    "assert test_score > 0.70, \"Test accuracy troppo bassa\"\n",
    "print(\"\\nMicro-checkpoint: Pipeline funziona correttamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800df815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ESERCIZIO 2: ColumnTransformer per Dati Misti\n",
      "============================================================\n",
      "\n",
      "Dataset shape: (600, 7)\n",
      "\n",
      "Valori mancanti:\n",
      "Age            50\n",
      "Income         30\n",
      "CreditScore     0\n",
      "Employment     20\n",
      "Education       0\n",
      "HomeOwner       0\n",
      "Approved        0\n",
      "dtype: int64\n",
      "\n",
      "Dtypes:\n",
      "Age            float64\n",
      "Income         float64\n",
      "CreditScore    float64\n",
      "Employment      object\n",
      "Education       object\n",
      "HomeOwner       object\n",
      "Approved         int64\n",
      "dtype: object\n",
      "\n",
      "Pipeline Completa:\n",
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='median')),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['Age', 'Income',\n",
      "                                                   'CreditScore']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('encoder',\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                                 sparse_output=False))]),\n",
      "                                                  ['Employment', 'Education',\n",
      "                                                   'HomeOwner'])])),\n",
      "                ('classifier', RandomForestClassifier(random_state=42))])\n",
      "\n",
      "Risultati:\n",
      "  Train samples: 450\n",
      "  Test samples: 150\n",
      "  Test Accuracy: 0.9933\n",
      "\n",
      "Features trasformate (14):\n",
      "  1. num__Age\n",
      "  2. num__Income\n",
      "  3. num__CreditScore\n",
      "  4. cat__Employment_employed\n",
      "  5. cat__Employment_retired\n",
      "  6. cat__Employment_self-employed\n",
      "  7. cat__Employment_unemployed\n",
      "  8. cat__Education_bachelor\n",
      "  9. cat__Education_high_school\n",
      "  10. cat__Education_master\n",
      "  11. cat__Education_phd\n",
      "  12. cat__HomeOwner_no\n",
      "  13. cat__HomeOwner_rent\n",
      "  14. cat__HomeOwner_yes\n",
      "\n",
      "Micro-checkpoint: ColumnTransformer funziona correttamente!\n"
     ]
    }
   ],
   "source": [
    "# === ESERCIZIO 2: COLUMNTRANSFORMER PER DATI MISTI ===\n",
    "# Perche: nei dataset reali abbiamo colonne numeriche e categoriche che richiedono preprocessing diverso\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ESERCIZIO 2: ColumnTransformer per Dati Misti\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Creiamo un dataset simil-Titanic con dati misti\n",
    "np.random.seed(42)\n",
    "n_samples = 600\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Age': np.random.normal(30, 12, n_samples).clip(1, 80),\n",
    "    'Income': np.abs(np.random.normal(50000, 20000, n_samples)),\n",
    "    'CreditScore': np.random.randint(300, 850, n_samples).astype(float),\n",
    "    'Employment': np.random.choice(['employed', 'self-employed', 'unemployed', 'retired'], n_samples),\n",
    "    'Education': np.random.choice(['high_school', 'bachelor', 'master', 'phd'], n_samples),\n",
    "    'HomeOwner': np.random.choice(['yes', 'no', 'rent'], n_samples)\n",
    "})\n",
    "\n",
    "# Target binario\n",
    "df['Approved'] = ((df['Income'] > 40000) & (df['CreditScore'] > 600)).astype(int)\n",
    "\n",
    "# Aggiungiamo valori mancanti realistici\n",
    "df.loc[np.random.choice(n_samples, 50, replace=False), 'Age'] = np.nan\n",
    "df.loc[np.random.choice(n_samples, 30, replace=False), 'Income'] = np.nan\n",
    "df.loc[np.random.choice(n_samples, 20, replace=False), 'Employment'] = np.nan\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nValori mancanti:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nDtypes:\\n{df.dtypes}\")\n",
    "\n",
    "# Identifichiamo le colonne per tipo\n",
    "numeric_features = ['Age', 'Income', 'CreditScore']\n",
    "categorical_features = ['Employment', 'Education', 'HomeOwner']\n",
    "\n",
    "# Pipeline per features numeriche\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline per features categoriche\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# ColumnTransformer che combina tutto\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Pipeline completa con modello\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"\\nPipeline Completa:\")\n",
    "print(full_pipeline)\n",
    "\n",
    "# Training e valutazione\n",
    "X = df.drop('Approved', axis=1)\n",
    "y = df['Approved']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "accuracy = full_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nRisultati:\")\n",
    "print(f\"  Train samples: {len(X_train)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Feature names dopo trasformazione\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "print(f\"\\nFeatures trasformate ({len(feature_names)}):\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"  {i+1}. {name}\")\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert accuracy > 0.80, \"Accuracy troppo bassa per dati misti\"\n",
    "assert len(feature_names) > len(numeric_features), \"One-hot encoding non applicato\"\n",
    "print(\"\\nMicro-checkpoint: ColumnTransformer funziona correttamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ESERCIZIO 3: PIPELINE CON GRIDSEARCHCV ===\n",
    "# Perche: possiamo ottimizzare iperparametri di qualsiasi step della pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ESERCIZIO 3: Pipeline con GridSearchCV\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dataset\n",
    "np.random.seed(42)\n",
    "X_grid, y_grid = make_classification(\n",
    "    n_samples=400,\n",
    "    n_features=10,\n",
    "    n_informative=6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_g, X_test_g, y_train_g, y_test_g = train_test_split(\n",
    "    X_grid, y_grid, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {X_grid.shape}\")\n",
    "print(f\"Train: {X_train_g.shape}, Test: {X_test_g.shape}\")\n",
    "\n",
    "# Creiamo la pipeline\n",
    "pipe_grid = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(random_state=42)\n",
    ")\n",
    "\n",
    "print(f\"\\nPipeline:\")\n",
    "print(pipe_grid)\n",
    "\n",
    "# Definiamo la griglia di iperparametri\n",
    "# NOTA: con make_pipeline i nomi sono lowercase del transformer\n",
    "param_grid = {\n",
    "    'svc__C': [0.1, 1, 10, 100],\n",
    "    'svc__kernel': ['linear', 'rbf'],\n",
    "    'svc__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "print(f\"\\nGriglia iperparametri:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipe_grid,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nEseguendo GridSearchCV con 5-fold CV...\")\n",
    "grid_search.fit(X_train_g, y_train_g)\n",
    "\n",
    "# Risultati\n",
    "print(f\"\\nMiglior CV Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"\\nMigliori Parametri:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Test con migliori parametri\n",
    "test_score_grid = grid_search.score(X_test_g, y_test_g)\n",
    "print(f\"\\nTest Accuracy (best params): {test_score_grid:.4f}\")\n",
    "\n",
    "# Top 5 combinazioni\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results = results.sort_values('rank_test_score')\n",
    "print(f\"\\nTop 5 combinazioni:\")\n",
    "print(results[['rank_test_score', 'mean_test_score', 'std_test_score', 'params']].head())\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert grid_search.best_score_ > 0.80, \"Best CV score troppo basso\"\n",
    "assert test_score_grid > 0.80, \"Test score troppo basso\"\n",
    "print(\"\\nMicro-checkpoint: GridSearchCV ottimizza tutta la pipeline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feeb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ESERCIZIO 4: CONFRONTO CON/SENZA PIPELINE ===\n",
    "# Perche: dimostriamo che la pipeline produce risultati identici ma con codice piu pulito\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ESERCIZIO 4: Confronto Con/Senza Pipeline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dataset con valori mancanti\n",
    "np.random.seed(42)\n",
    "X_cmp, y_cmp = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=10,\n",
    "    n_informative=6,\n",
    "    flip_y=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Aggiungiamo valori mancanti\n",
    "mask = np.random.random(X_cmp.shape) < 0.10\n",
    "X_cmp[mask] = np.nan\n",
    "\n",
    "# Split\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_cmp, y_cmp, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {X_cmp.shape}\")\n",
    "print(f\"Valori mancanti: {np.isnan(X_cmp).sum()}\")\n",
    "\n",
    "# --- METODO 1: SENZA PIPELINE ---\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"METODO 1: Senza Pipeline\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Step 1: Imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(X_train_c)\n",
    "X_train_imputed = imputer.transform(X_train_c)\n",
    "X_test_imputed = imputer.transform(X_test_c)\n",
    "\n",
    "# Step 2: Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imputed)\n",
    "X_train_scaled = scaler.transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Step 3: Model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_scaled, y_train_c)\n",
    "score_no_pipe = model.score(X_test_scaled, y_test_c)\n",
    "\n",
    "print(f\"Test Accuracy: {score_no_pipe:.4f}\")\n",
    "print(\"Righe di codice: ~12\")\n",
    "print(\"Variabili intermedie: 6\")\n",
    "print(\"Oggetti da salvare: 3\")\n",
    "\n",
    "# --- METODO 2: CON PIPELINE ---\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"METODO 2: Con Pipeline\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "pipe_cmp = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "pipe_cmp.fit(X_train_c, y_train_c)\n",
    "score_pipe = pipe_cmp.score(X_test_c, y_test_c)\n",
    "\n",
    "print(f\"Test Accuracy: {score_pipe:.4f}\")\n",
    "print(\"Righe di codice: 2\")\n",
    "print(\"Variabili intermedie: 0\")\n",
    "print(\"Oggetti da salvare: 1\")\n",
    "\n",
    "# Confronto\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"CONFRONTO\")\n",
    "print(\"-\"*40)\n",
    "print(f\"Risultati identici? {np.isclose(score_no_pipe, score_pipe)}\")\n",
    "print(f\"Differenza: {abs(score_no_pipe - score_pipe):.6f}\")\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert np.isclose(score_no_pipe, score_pipe), \"I risultati dovrebbero essere identici!\"\n",
    "print(\"\\nMicro-checkpoint: Pipeline produce risultati identici con meno codice!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be62110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ESERCIZIO 5: PIPELINE COMPLETA PER PRODUZIONE ===\n",
    "# Perche: mostriamo il workflow completo: preprocessing, tuning, salvataggio\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ESERCIZIO 5: Pipeline Completa per Produzione\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Creiamo dataset con dati misti\n",
    "np.random.seed(42)\n",
    "n = 800\n",
    "\n",
    "df_prod = pd.DataFrame({\n",
    "    'feature_1': np.random.normal(0, 1, n),\n",
    "    'feature_2': np.random.exponential(2, n),\n",
    "    'feature_3': np.random.randint(0, 100, n).astype(float),\n",
    "    'category_1': np.random.choice(['A', 'B', 'C'], n),\n",
    "    'category_2': np.random.choice(['X', 'Y'], n)\n",
    "})\n",
    "\n",
    "df_prod['target'] = (\n",
    "    0.5 * df_prod['feature_1'] + \n",
    "    0.3 * df_prod['feature_2'] + \n",
    "    0.2 * (df_prod['category_1'] == 'A').astype(float)\n",
    ") > 0.5\n",
    "df_prod['target'] = df_prod['target'].astype(int)\n",
    "\n",
    "# Valori mancanti\n",
    "df_prod.loc[np.random.choice(n, 40, replace=False), 'feature_1'] = np.nan\n",
    "df_prod.loc[np.random.choice(n, 25, replace=False), 'category_1'] = np.nan\n",
    "\n",
    "print(f\"Dataset: {df_prod.shape}\")\n",
    "print(f\"Valori mancanti: {df_prod.isnull().sum().sum()}\")\n",
    "\n",
    "# Definiamo le colonne\n",
    "num_cols = ['feature_1', 'feature_2', 'feature_3']\n",
    "cat_cols = ['category_1', 'category_2']\n",
    "\n",
    "# Pipeline per numeriche\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline per categoriche\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipe, num_cols),\n",
    "    ('cat', cat_pipe, cat_cols)\n",
    "])\n",
    "\n",
    "# Pipeline completa\n",
    "production_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Split\n",
    "X_prod = df_prod.drop('target', axis=1)\n",
    "y_prod = df_prod['target']\n",
    "\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
    "    X_prod, y_prod, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid_prod = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [5, 10, None]\n",
    "}\n",
    "\n",
    "grid_prod = GridSearchCV(\n",
    "    production_pipeline,\n",
    "    param_grid_prod,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nEseguendo GridSearchCV...\")\n",
    "grid_prod.fit(X_train_p, y_train_p)\n",
    "\n",
    "print(f\"\\nMiglior CV Score: {grid_prod.best_score_:.4f}\")\n",
    "print(f\"Migliori Parametri: {grid_prod.best_params_}\")\n",
    "\n",
    "# Best model\n",
    "best_pipeline = grid_prod.best_estimator_\n",
    "test_score_prod = best_pipeline.score(X_test_p, y_test_p)\n",
    "print(f\"Test Accuracy: {test_score_prod:.4f}\")\n",
    "\n",
    "# Salvare il modello (demo)\n",
    "model_path = 'production_model.pkl'\n",
    "joblib.dump(best_pipeline, model_path)\n",
    "print(f\"\\nModello salvato: {model_path}\")\n",
    "print(f\"Dimensione file: {os.path.getsize(model_path) / 1024:.1f} KB\")\n",
    "\n",
    "# Simulare caricamento e predizione su nuovi dati\n",
    "loaded_pipeline = joblib.load(model_path)\n",
    "new_data = pd.DataFrame({\n",
    "    'feature_1': [0.5, -0.3],\n",
    "    'feature_2': [2.1, 1.5],\n",
    "    'feature_3': [50, 75],\n",
    "    'category_1': ['A', 'B'],\n",
    "    'category_2': ['X', 'Y']\n",
    "})\n",
    "\n",
    "predictions = loaded_pipeline.predict(new_data)\n",
    "probabilities = loaded_pipeline.predict_proba(new_data)\n",
    "\n",
    "print(f\"\\nPredizioni su nuovi dati:\")\n",
    "print(f\"  Predizioni: {predictions}\")\n",
    "print(f\"  Probabilita classe 1: {probabilities[:, 1]}\")\n",
    "\n",
    "# Pulizia\n",
    "os.remove(model_path)\n",
    "print(f\"\\nFile {model_path} rimosso (era solo una demo)\")\n",
    "\n",
    "# --- MICRO-CHECKPOINT ---\n",
    "assert test_score_prod > 0.70, \"Test score troppo basso\"\n",
    "assert len(predictions) == 2, \"Dovrebbero esserci 2 predizioni\"\n",
    "print(\"\\nMicro-checkpoint: Pipeline pronta per produzione!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb4028",
   "metadata": {},
   "source": [
    "# SEZIONE 4 - Metodi Principali\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "| Metodo | Sintassi | Descrizione |\n",
    "|--------|----------|-------------|\n",
    "| `Pipeline()` | `Pipeline([('name', transformer), ...])` | Crea pipeline con step nominati |\n",
    "| `make_pipeline()` | `make_pipeline(t1, t2, t3)` | Shortcut: genera nomi automatici |\n",
    "| `pipe.fit()` | `pipe.fit(X, y)` | Esegue fit_transform su tutti gli step |\n",
    "| `pipe.predict()` | `pipe.predict(X)` | Esegue transform + predict |\n",
    "| `pipe.score()` | `pipe.score(X, y)` | Esegue transform + score |\n",
    "| `pipe['name']` | `pipe['scaler']` | Accede a uno step per nome |\n",
    "| `pipe.named_steps` | `pipe.named_steps['scaler']` | Dizionario degli step |\n",
    "| `pipe.set_params()` | `pipe.set_params(scaler__with_mean=False)` | Modifica parametri |\n",
    "\n",
    "## ColumnTransformer\n",
    "\n",
    "| Metodo | Sintassi | Descrizione |\n",
    "|--------|----------|-------------|\n",
    "| `ColumnTransformer()` | `ColumnTransformer([('name', tr, cols)])` | Trasformazioni per colonne diverse |\n",
    "| `remainder` | `remainder='drop'\\|'passthrough'` | Gestione colonne non specificate |\n",
    "| `ct.fit_transform()` | `ct.fit_transform(X)` | Fit e trasforma le colonne |\n",
    "| `ct.get_feature_names_out()` | `ct.get_feature_names_out()` | Nomi feature dopo trasformazione |\n",
    "| `make_column_selector()` | `make_column_selector(dtype_include=np.number)` | Seleziona colonne per dtype |\n",
    "\n",
    "## GridSearchCV con Pipeline\n",
    "\n",
    "| Sintassi | Descrizione |\n",
    "|----------|-------------|\n",
    "| `'stepname__param'` | Accesso parametri: nome step + doppio underscore |\n",
    "| `grid.best_estimator_` | La pipeline con migliori parametri |\n",
    "| `grid.best_params_` | Dizionario dei migliori parametri |\n",
    "| `grid.cv_results_` | Risultati dettagliati di tutti i trial |\n",
    "\n",
    "## Persistenza\n",
    "\n",
    "| Metodo | Sintassi | Descrizione |\n",
    "|--------|----------|-------------|\n",
    "| `joblib.dump()` | `joblib.dump(pipe, 'model.pkl')` | Salva pipeline su disco |\n",
    "| `joblib.load()` | `joblib.load('model.pkl')` | Carica pipeline da disco |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115a9cb",
   "metadata": {},
   "source": [
    "# SEZIONE 5 - Glossario\n",
    "\n",
    "| Termine | Definizione |\n",
    "|---------|-------------|\n",
    "| **Pipeline** | Sequenza ordinata di trasformazioni + modello in un unico oggetto |\n",
    "| **ColumnTransformer** | Applica trasformazioni diverse a gruppi di colonne |\n",
    "| **Data Leakage** | Errore: usare informazioni dal test set durante il training |\n",
    "| **make_pipeline** | Shortcut per creare Pipeline con nomi automatici |\n",
    "| **fit_transform** | Combina fit e transform in una sola chiamata |\n",
    "| **transform** | Applica trasformazione senza ricalcolare i parametri |\n",
    "| **Transformer** | Oggetto che implementa fit e transform (es. StandardScaler) |\n",
    "| **Estimator** | Oggetto che implementa fit e predict (es. modello) |\n",
    "| **Step** | Un singolo componente della pipeline |\n",
    "| **remainder** | Parametro per gestire colonne non specificate in ColumnTransformer |\n",
    "| **passthrough** | Mantiene le colonne senza trasformazioni |\n",
    "| **handle_unknown** | Parametro OneHotEncoder per gestire categorie nuove |\n",
    "| **sparse_output** | Parametro per controllare se output e sparso o denso |\n",
    "| **named_steps** | Attributo per accedere agli step per nome |\n",
    "| **best_estimator_** | La pipeline fittata con i migliori parametri da GridSearchCV |\n",
    "| **stepname__param** | Sintassi per accedere ai parametri degli step |\n",
    "| **joblib** | Libreria per serializzare oggetti Python (pipeline) |\n",
    "| **make_column_selector** | Helper per selezionare colonne per dtype |\n",
    "| **n_jobs** | Parametro per parallelizzazione (-1 = tutti i core) |\n",
    "| **cv_results_** | Dizionario con risultati dettagliati di GridSearchCV |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c2394",
   "metadata": {},
   "source": [
    "# SEZIONE 6 - Errori Comuni\n",
    "\n",
    "| N. | Errore | Problema | Soluzione |\n",
    "|----|--------|----------|-----------|\n",
    "| 1 | Preprocessing prima dello split | Data leakage: scaler vede anche test | Usare Pipeline con CV |\n",
    "| 2 | fit_transform su test | Il test non deve influenzare i parametri | Solo transform su test (Pipeline lo fa automaticamente) |\n",
    "| 3 | Dimenticare handle_unknown | Crash con categorie nuove in produzione | `OneHotEncoder(handle_unknown='ignore')` |\n",
    "| 4 | Nome step sbagliato in GridSearchCV | Parametri non trovati | Controllare nomi con `pipe.named_steps.keys()` |\n",
    "| 5 | Doppio underscore dimenticato | `stepname_param` invece di `stepname__param` | Sempre doppio underscore per accedere ai parametri |\n",
    "| 6 | remainder='drop' non voluto | Colonne perse silenziosamente | Specificare `remainder='passthrough'` se servono |\n",
    "| 7 | Salvare modello e preprocessor separati | Rischio disallineamento | Salvare tutta la Pipeline con joblib |\n",
    "| 8 | Ordine step sbagliato | Scaler prima di Imputer fallisce con NaN | Prima Imputer, poi Scaler |\n",
    "| 9 | sparse_output=True con modelli incompatibili | Errori con alcuni classificatori | `sparse_output=False` per sicurezza |\n",
    "| 10 | Cross-validation senza Pipeline | Data leakage nel preprocessing | Sempre Pipeline quando fai CV |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6065cc",
   "metadata": {},
   "source": [
    "# SEZIONE 7 - Conclusione\n",
    "\n",
    "## Concetti Chiave Appresi\n",
    "\n",
    "1. **Pipeline** incapsula preprocessing + modello in un unico oggetto\n",
    "2. **Data Leakage** si previene automaticamente usando Pipeline con cross-validation\n",
    "3. **ColumnTransformer** permette trasformazioni diverse per colonne numeriche e categoriche\n",
    "4. **make_pipeline** e uno shortcut che genera nomi automatici\n",
    "5. **GridSearchCV** ottimizza iperparametri di qualsiasi step della pipeline\n",
    "6. **joblib** permette di salvare/caricare l'intera pipeline per produzione\n",
    "\n",
    "## Formula Mentale\n",
    "\n",
    "```\n",
    "PIPELINE = Sequenza(Transformer1, Transformer2, ..., Estimator)\n",
    "\n",
    "fit(X_train) = foreach step: fit_transform() fino all'ultimo che fa fit()\n",
    "predict(X_new) = foreach step: transform() fino all'ultimo che fa predict()\n",
    "```\n",
    "\n",
    "## Prossima Lezione\n",
    "\n",
    "**Lezione 14**: Alberi Decisionali e Random Forest\n",
    "- Come funziona un Decision Tree\n",
    "- Gini vs Entropy\n",
    "- Ensemble: Bagging e Random Forest\n",
    "- Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905aa86",
   "metadata": {},
   "source": [
    "# SEZIONE 8 - Checklist\n",
    "\n",
    "## Prima di usare Pipeline\n",
    "\n",
    "- [ ] Ho identificato tutti gli step di preprocessing necessari\n",
    "- [ ] Ho verificato l'ordine corretto degli step (Imputer prima di Scaler)\n",
    "- [ ] Ho separato colonne numeriche da categoriche (se presenti)\n",
    "\n",
    "## Durante la costruzione\n",
    "\n",
    "- [ ] Ho usato make_pipeline o Pipeline con nomi descrittivi\n",
    "- [ ] Ho impostato `handle_unknown='ignore'` per OneHotEncoder\n",
    "- [ ] Ho impostato `sparse_output=False` se necessario\n",
    "- [ ] Ho specificato `remainder` in ColumnTransformer\n",
    "\n",
    "## Per GridSearchCV\n",
    "\n",
    "- [ ] Ho usato la sintassi `stepname__param` per i parametri\n",
    "- [ ] Ho verificato i nomi degli step con `pipe.named_steps.keys()`\n",
    "- [ ] Ho impostato `n_jobs=-1` per parallelizzazione\n",
    "\n",
    "## Per produzione\n",
    "\n",
    "- [ ] Ho salvato l'intera pipeline con joblib (non solo il modello)\n",
    "- [ ] Ho testato load + predict su dati nuovi\n",
    "- [ ] Ho verificato che il file salvato sia ragionevole in dimensione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec586801",
   "metadata": {},
   "source": [
    "# SEZIONE 9 - Changelog\n",
    "\n",
    "| Versione | Data | Modifiche |\n",
    "|----------|------|-----------|\n",
    "| 1.0 | 2024-01-15 | Creazione lezione su Pipeline di Scikit-Learn |\n",
    "| 1.1 | 2024-01-15 | Ristrutturazione con template 9 sezioni |\n",
    "| 1.2 | 2024-01-15 | Aggiunta 5 esercizi dimostrativi con micro-checkpoint |\n",
    "| 1.3 | 2024-01-15 | Aggiunta sezione ColumnTransformer e GridSearchCV |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
