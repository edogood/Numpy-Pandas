{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57abc05c",
   "metadata": {},
   "source": [
    "# LEZIONE 13 â€” Pipeline di Scikit-Learn\n",
    "\n",
    "## Obiettivi della Lezione\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "1. **Comprendere** perchÃ© le Pipeline sono essenziali nei progetti reali\n",
    "2. **Costruire** Pipeline che combinano preprocessing e modello\n",
    "3. **Usare** ColumnTransformer per gestire feature diverse\n",
    "4. **Evitare** il data leakage con preprocessing corretto\n",
    "5. **Strutturare** workflow di ML riproducibili e manutenibili\n",
    "\n",
    "## PerchÃ© Ã¨ Importante\n",
    "\n",
    "Le Pipeline risolvono **problemi critici** che ogni data scientist affronta:\n",
    "\n",
    "- **Data Leakage**: preprocessing fatto prima dello split contamina i risultati\n",
    "- **RiproducibilitÃ **: senza pipeline, il codice diventa fragile e difficile da mantenere\n",
    "- **Produzione**: un modello senza pipeline non puÃ² essere deployato facilmente\n",
    "\n",
    "## Posizione nel Percorso\n",
    "\n",
    "```\n",
    "Lezione 12: Overfitting (cosa puÃ² andare storto)\n",
    "    â†“\n",
    "â†’ Lezione 13: Pipeline (come strutturare il workflow) â† SEI QUI\n",
    "    â†“\n",
    "Lezione 14: Modelli Tree-Based (Decision Tree, Random Forest)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Librerie Utilizzate\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f54f30",
   "metadata": {},
   "source": [
    "# Section 1 â€” Il Problema: PerchÃ© Servono le Pipeline?\n",
    "\n",
    "## Scenario Tipico (SBAGLIATO)\n",
    "\n",
    "```python\n",
    "# âŒ ERRORE COMUNE: preprocessing su TUTTI i dati prima dello split\n",
    "\n",
    "# 1. Carichiamo i dati\n",
    "X = load_data()\n",
    "\n",
    "# 2. Preprocessing su TUTTO il dataset\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # â† ERRORE! fit su tutti i dati\n",
    "\n",
    "# 3. Poi facciamo lo split\n",
    "X_train, X_test = train_test_split(X_scaled)\n",
    "\n",
    "# 4. Alleniamo il modello\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "## PerchÃ© Ã¨ Sbagliato? DATA LEAKAGE\n",
    "\n",
    "```\n",
    "            PRIMA DELLO SPLIT\n",
    "            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    Tutti i â”€â–º â”‚ Scaler  â”‚ â”€â–º fit() vede ANCHE i dati di test!\n",
    "    dati       â”‚ fit()   â”‚\n",
    "               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â–¼               â–¼\n",
    "         Train            Test\n",
    "         \n",
    "Il modello \"bara\": ha visto informazioni sul test set!\n",
    "```\n",
    "\n",
    "**Conseguenze:**\n",
    "- Le performance sul test set sono **ottimistiche**\n",
    "- In produzione il modello avrÃ  performance **peggiori**\n",
    "- Hai violato la regola fondamentale del ML\n",
    "\n",
    "## Scenario Corretto (MA SCOMODO)\n",
    "\n",
    "```python\n",
    "# âœ… CORRETTO ma verboso e error-prone\n",
    "\n",
    "# 1. Prima lo split\n",
    "X_train, X_test = train_test_split(X)\n",
    "\n",
    "# 2. Fit SOLO su train\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # â† fit solo su train\n",
    "\n",
    "# 3. Transform su entrambi\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Stessa cosa per ogni step di preprocessing...\n",
    "imputer = SimpleImputer()\n",
    "imputer.fit(X_train_scaled)\n",
    "X_train_imputed = imputer.transform(X_train_scaled)\n",
    "X_test_imputed = imputer.transform(X_test_scaled)\n",
    "\n",
    "# 5. Allena modello\n",
    "model.fit(X_train_imputed, y_train)\n",
    "\n",
    "# 6. Per predizioni su nuovi dati: ripeti tutti i transform!\n",
    "```\n",
    "\n",
    "**Problemi:**\n",
    "- Codice lungo e ripetitivo\n",
    "- Facile dimenticare un passaggio\n",
    "- Difficile da mantenere\n",
    "- Impossibile da usare in produzione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da3a2e2",
   "metadata": {},
   "source": [
    "# Section 2 â€” La Soluzione: sklearn.pipeline.Pipeline\n",
    "\n",
    "## Cos'Ã¨ una Pipeline?\n",
    "\n",
    "Una **Pipeline** Ã¨ una sequenza ordinata di trasformazioni + un modello finale, incapsulati in un unico oggetto.\n",
    "\n",
    "```\n",
    "               PIPELINE\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                                 â”‚\n",
    "    â”‚  Step 1      Step 2      Step 3 â”‚\n",
    "    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "X â”€â–ºâ”‚ â”‚Imputerâ”‚â”€â–º â”‚Scalerâ”‚ â”€â–ºâ”‚ Model â”‚ â”‚â”€â–º y_pred\n",
    "    â”‚ â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "    â”‚                                 â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \n",
    "    pipe.fit(X_train, y_train)  # allena TUTTO\n",
    "    pipe.predict(X_test)        # applica TUTTO\n",
    "```\n",
    "\n",
    "## Vantaggi\n",
    "\n",
    "| Senza Pipeline | Con Pipeline |\n",
    "|----------------|--------------|\n",
    "| Codice frammentato | Oggetto unico |\n",
    "| Facile dimenticare step | Tutti gli step garantiti |\n",
    "| Data leakage possibile | Data leakage impossibile |\n",
    "| Difficile salvare/caricare | Un solo file da salvare |\n",
    "| Cross-validation manuale | CV automatica e corretta |\n",
    "\n",
    "## Sintassi Base\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Metodo 1: esplicito con nomi\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Metodo 2: senza nomi (make_pipeline)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "```\n",
    "\n",
    "## Come Funziona\n",
    "\n",
    "Quando chiami `pipe.fit(X, y)`:\n",
    "1. Ogni transformer chiama `fit_transform()` sui dati\n",
    "2. L'output viene passato al passo successivo\n",
    "3. L'ultimo step (modello) chiama solo `fit()`\n",
    "\n",
    "Quando chiami `pipe.predict(X)`:\n",
    "1. Ogni transformer chiama solo `transform()` (NO fit!)\n",
    "2. L'ultimo step chiama `predict()`\n",
    "\n",
    "```\n",
    "fit(X_train, y_train):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_train â†’ [Imputer.fit_transform] â†’ [Scaler.fit_transform] â†’ [Model.fit]\n",
    "\n",
    "predict(X_test):\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_test â†’ [Imputer.transform] â†’ [Scaler.transform] â†’ [Model.predict] â†’ y_pred\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13acb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset con valori mancanti:\n",
      "Shape: (500, 10)\n",
      "Valori mancanti: 525 (10.5%)\n",
      "\n",
      "============================================================\n",
      "METODO 1: Senza Pipeline\n",
      "============================================================\n",
      "Test Accuracy: 0.7760\n",
      "Codice: ~15 righe, 6 variabili intermedie\n",
      "\n",
      "============================================================\n",
      "METODO 2: Con Pipeline\n",
      "============================================================\n",
      "Test Accuracy: 0.7760\n",
      "Codice: 2 righe, 0 variabili intermedie\n",
      "\n",
      "Risultati identici? True\n",
      "\n",
      "============================================================\n",
      "Cross-Validation con Pipeline\n",
      "============================================================\n",
      "CV Scores: ['0.760', '0.690', '0.730', '0.830', '0.770']\n",
      "CV Mean: 0.7560 Â± 0.0463\n",
      "\n",
      "âœ… Nessun data leakage: il preprocessing Ã¨ rifatto ad ogni fold!\n"
     ]
    }
   ],
   "source": [
    "# Dimostrazione: Prima Pipeline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Creiamo un dataset con valori mancanti\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=10,\n",
    "    n_informative=6,\n",
    "    flip_y=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Aggiungiamo valori mancanti (~10%)\n",
    "mask = np.random.random(X.shape) < 0.10\n",
    "X[mask] = np.nan\n",
    "\n",
    "print(\"Dataset con valori mancanti:\")\n",
    "print(f\"Shape: {X.shape}\")\n",
    "print(f\"Valori mancanti: {np.isnan(X).sum()} ({np.isnan(X).mean()*100:.1f}%)\")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# METODO 1: Senza Pipeline (per confronto)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METODO 1: Senza Pipeline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Imputer (fit solo su train)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(X_train)\n",
    "X_train_imputed = imputer.transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Step 2: Scaler (fit solo su train)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imputed)\n",
    "X_train_scaled = scaler.transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Step 3: Model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "score_no_pipe = model.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Test Accuracy: {score_no_pipe:.4f}\")\n",
    "print(\"Codice: ~15 righe, 6 variabili intermedie\")\n",
    "\n",
    "# ============================================================================\n",
    "# METODO 2: Con Pipeline\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METODO 2: Con Pipeline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Tutto in un oggetto!\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Fit e score in due righe\n",
    "pipe.fit(X_train, y_train)\n",
    "score_pipe = pipe.score(X_test, y_test)\n",
    "\n",
    "print(f\"Test Accuracy: {score_pipe:.4f}\")\n",
    "print(\"Codice: 2 righe, 0 variabili intermedie\")\n",
    "\n",
    "# Verifica che i risultati siano identici\n",
    "print(f\"\\nRisultati identici? {np.isclose(score_no_pipe, score_pipe)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Cross-Validation con Pipeline\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Cross-Validation con Pipeline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Con pipeline, la CV Ã¨ automaticamente corretta\n",
    "# Ogni fold fa fit del preprocessing solo sui dati di train di quel fold\n",
    "cv_scores = cross_val_score(pipe, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"CV Scores: {[f'{s:.3f}' for s in cv_scores]}\")\n",
    "print(f\"CV Mean: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "print(\"\\nâœ… Nessun data leakage: il preprocessing Ã¨ rifatto ad ogni fold!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede65a4b",
   "metadata": {},
   "source": [
    "# Section 3 â€” ColumnTransformer: Pipeline per Dati Eterogenei\n",
    "\n",
    "## Il Problema: Colonne Diverse, Trattamenti Diversi\n",
    "\n",
    "Nei dataset reali abbiamo spesso:\n",
    "- **Colonne numeriche** â†’ SimpleImputer(mean) + StandardScaler\n",
    "- **Colonne categoriche** â†’ SimpleImputer(most_frequent) + OneHotEncoder\n",
    "- **Colonne ordinali** â†’ OrdinalEncoder\n",
    "- **Colonne da ignorare** â†’ passthrough o drop\n",
    "\n",
    "## La Soluzione: `sklearn.compose.ColumnTransformer`\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('name_1', transformer_1, columns_1),\n",
    "    ('name_2', transformer_2, columns_2),\n",
    "    ...\n",
    "], remainder='drop')  # cosa fare con le altre colonne\n",
    "```\n",
    "\n",
    "### Parametri importanti:\n",
    "\n",
    "| Parametro | Opzioni | Descrizione |\n",
    "|-----------|---------|-------------|\n",
    "| `remainder` | `'drop'` (default) | Scarta le colonne non specificate |\n",
    "| | `'passthrough'` | Mantiene le colonne non trasformate |\n",
    "| | transformer | Applica un transformer alle colonne rimanenti |\n",
    "\n",
    "### Specificare le colonne:\n",
    "\n",
    "```python\n",
    "# Per nome (con DataFrame)\n",
    "columns = ['age', 'income', 'score']\n",
    "\n",
    "# Per indice (con array)\n",
    "columns = [0, 2, 5]\n",
    "\n",
    "# Con selector (sklearn >= 1.0)\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "num_cols = make_column_selector(dtype_include=np.number)\n",
    "cat_cols = make_column_selector(dtype_include=object)\n",
    "```\n",
    "\n",
    "## Pattern Comune: Pipeline Completa\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    ColumnTransformer                       â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n",
    "â”‚  â”‚ numeric_pipe    â”‚    â”‚ categorical_pipe â”‚               â”‚\n",
    "â”‚  â”‚  Imputer(mean)  â”‚    â”‚  Imputer(freq)   â”‚               â”‚\n",
    "â”‚  â”‚  StandardScaler â”‚    â”‚  OneHotEncoder   â”‚               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\n",
    "â”‚           â”‚                       â”‚                         â”‚\n",
    "â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚\n",
    "â”‚                       â”‚ concatena                           â”‚\n",
    "â”‚                       â–¼                                     â”‚\n",
    "â”‚            [features trasformate]                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "               â”‚     Modello     â”‚\n",
    "               â”‚ (es. XGBoost)   â”‚\n",
    "               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800df815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset con dati misti:\n",
      "    age        income  credit_score     employment    education home_owner  \\\n",
      "0  56.0  92443.123940         412.0     unemployed       master        yes   \n",
      "1  69.0  70649.305211         486.0            NaN     bachelor         no   \n",
      "2  46.0  19612.600681         526.0        retired     bachelor       rent   \n",
      "3  32.0  40315.318543         609.0     unemployed          phd         no   \n",
      "4  60.0  75338.222984         397.0     unemployed     bachelor         no   \n",
      "5  25.0  35846.610688         354.0  self-employed          phd         no   \n",
      "6  38.0  58876.388563         705.0     unemployed     bachelor         no   \n",
      "7  56.0  65492.681069         368.0        retired  high_school       rent   \n",
      "8  36.0  31461.390568         611.0  self-employed     bachelor       rent   \n",
      "9  40.0  48809.492879         588.0        retired  high_school       rent   \n",
      "\n",
      "   approved  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         1  \n",
      "4         0  \n",
      "5         0  \n",
      "6         1  \n",
      "7         0  \n",
      "8         0  \n",
      "9         0  \n",
      "\n",
      "Dtypes:\n",
      "age             float64\n",
      "income          float64\n",
      "credit_score    float64\n",
      "employment       object\n",
      "education        object\n",
      "home_owner       object\n",
      "approved          int64\n",
      "dtype: object\n",
      "\n",
      "Valori mancanti:\n",
      "age              0\n",
      "income          30\n",
      "credit_score    15\n",
      "employment      20\n",
      "education        0\n",
      "home_owner       0\n",
      "approved         0\n",
      "dtype: int64\n",
      "\n",
      "============================================================\n",
      "Struttura della Pipeline:\n",
      "============================================================\n",
      "Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='median')),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['age', 'income',\n",
      "                                                   'credit_score']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('encoder',\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                                 sparse_output=False))]),\n",
      "                                                  ['employment', 'education',\n",
      "                                                   'home_owner'])])),\n",
      "                ('classifier', RandomForestClassifier(random_state=42))])\n",
      "\n",
      "============================================================\n",
      "Risultati:\n",
      "============================================================\n",
      "Test Accuracy: 0.9760\n",
      "CV Accuracy: 0.9740 Â± 0.0174\n",
      "\n",
      "============================================================\n",
      "Ispezione della Pipeline:\n",
      "============================================================\n",
      "\n",
      "Accesso ai componenti:\n",
      "Preprocessor: ColumnTransformer(transformers=[('num',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='median')),\n",
      "                                                 ('scaler', StandardScaler())]),\n",
      "                                 ['age', 'income', 'credit_score']),\n",
      "                                ('cat',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='most_frequent')),\n",
      "                                                 ('encoder',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                sparse_output=False))]),\n",
      "                                 ['employment', 'education', 'home_owner'])])\n",
      "Classifier: RandomForestClassifier(random_state=42)\n",
      "\n",
      "Features trasformate (14 totali):\n",
      "['num__age' 'num__income' 'num__credit_score' 'cat__employment_employed'\n",
      " 'cat__employment_retired'] ...\n"
     ]
    }
   ],
   "source": [
    "# Dimostrazione: ColumnTransformer con dati misti\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Creiamo un dataset realistico con dati misti\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 70, n_samples).astype(float),\n",
    "    'income': np.random.normal(50000, 20000, n_samples),\n",
    "    'credit_score': np.random.randint(300, 850, n_samples).astype(float),\n",
    "    'employment': np.random.choice(['employed', 'self-employed', 'unemployed', 'retired'], n_samples),\n",
    "    'education': np.random.choice(['high_school', 'bachelor', 'master', 'phd'], n_samples),\n",
    "    'home_owner': np.random.choice(['yes', 'no', 'rent'], n_samples)\n",
    "})\n",
    "\n",
    "# Target binario\n",
    "df['approved'] = ((df['income'] > 40000) & \n",
    "                   (df['credit_score'] > 600)).astype(int)\n",
    "\n",
    "# Aggiungiamo valori mancanti realistici\n",
    "df.loc[np.random.choice(n_samples, 30, replace=False), 'income'] = np.nan\n",
    "df.loc[np.random.choice(n_samples, 20, replace=False), 'employment'] = np.nan\n",
    "df.loc[np.random.choice(n_samples, 15, replace=False), 'credit_score'] = np.nan\n",
    "\n",
    "print(\"Dataset con dati misti:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nDtypes:\\n{df.dtypes}\")\n",
    "print(f\"\\nValori mancanti:\\n{df.isnull().sum()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Definiamo le colonne\n",
    "# ============================================================================\n",
    "numeric_features = ['age', 'income', 'credit_score']\n",
    "categorical_features = ['employment', 'education', 'home_owner']\n",
    "\n",
    "# Pipeline per features numeriche\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # median per outlier\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline per features categoriche\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# ColumnTransformer che combina tutto\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Pipeline finale con modello\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Struttura della Pipeline:\")\n",
    "print(\"=\"*60)\n",
    "print(full_pipeline)\n",
    "\n",
    "# ============================================================================\n",
    "# Training e Valutazione\n",
    "# ============================================================================\n",
    "X = df.drop('approved', axis=1)\n",
    "y = df['approved']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Fit e predict in una riga ciascuno!\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "score = full_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Risultati:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test Accuracy: {score:.4f}\")\n",
    "\n",
    "# Cross-validation con la pipeline completa\n",
    "cv_scores = cross_val_score(full_pipeline, X, y, cv=5)\n",
    "print(f\"CV Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Ispezionare la Pipeline\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Ispezione della Pipeline:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Accedere ai componenti\n",
    "print(\"\\nAccesso ai componenti:\")\n",
    "print(f\"Preprocessor: {full_pipeline['preprocessor']}\")\n",
    "print(f\"Classifier: {full_pipeline['classifier']}\")\n",
    "\n",
    "# Feature names dopo one-hot encoding\n",
    "feature_names = full_pipeline['preprocessor'].get_feature_names_out()\n",
    "print(f\"\\nFeatures trasformate ({len(feature_names)} totali):\")\n",
    "print(feature_names[:5], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8997a",
   "metadata": {},
   "source": [
    "# Section 4 â€” Schema Mentale: Quando Usare le Pipeline\n",
    "\n",
    "## Checklist Decisionale\n",
    "\n",
    "### âœ… USA Pipeline quando:\n",
    "\n",
    "| Situazione | Motivo |\n",
    "|------------|--------|\n",
    "| Hai **piÃ¹ step** di preprocessing | Eviti variabili intermedie e errori |\n",
    "| Fai **cross-validation** | Previeni data leakage automaticamente |\n",
    "| Vuoi fare **hyperparameter tuning** | GridSearchCV lavora con pipeline |\n",
    "| Il modello va in **produzione** | Un solo oggetto da serializzare |\n",
    "| Lavori in **team** | Codice piÃ¹ leggibile e manutenibile |\n",
    "\n",
    "### âš ï¸ Pipeline NON necessaria quando:\n",
    "\n",
    "| Situazione | Alternativa |\n",
    "|------------|-------------|\n",
    "| Un solo step di preprocessing | PuÃ² bastare il transformer singolo |\n",
    "| Esplorazione dati (EDA) | Lavori step-by-step per capire |\n",
    "| Debug di preprocessing | Isoli il problema senza pipeline |\n",
    "\n",
    "## Regola d'Oro\n",
    "\n",
    "> **\"Se il tuo preprocessing ha piÃ¹ di uno step E farai training/prediction, usa Pipeline\"**\n",
    "\n",
    "## Vantaggi Riassunti\n",
    "\n",
    "```\n",
    "SENZA PIPELINE                    CON PIPELINE\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â€¢ 6 variabili intermedie          â€¢ 0 variabili intermedie\n",
    "â€¢ Rischio errori fit/transform    â€¢ Impossibile sbagliare\n",
    "â€¢ Data leakage in CV             â€¢ CV automaticamente corretta\n",
    "â€¢ 3 oggetti da salvare           â€¢ 1 oggetto da salvare\n",
    "â€¢ Codice lungo                    â€¢ Codice compatto\n",
    "```\n",
    "\n",
    "## Pattern Tipici da Ricordare\n",
    "\n",
    "### 1. Pipeline Semplice (solo numeriche)\n",
    "```python\n",
    "pipe = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. Pipeline con Dati Misti\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, num_cols),\n",
    "    ('cat', categorical_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "full_pipe = Pipeline([\n",
    "    ('prep', preprocessor),\n",
    "    ('model', XGBClassifier())\n",
    "])\n",
    "```\n",
    "\n",
    "### 3. Pipeline per Produzione\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Salva tutto in un file\n",
    "joblib.dump(full_pipe, 'model.pkl')\n",
    "\n",
    "# Carica e predici\n",
    "loaded_pipe = joblib.load('model.pkl')\n",
    "predictions = loaded_pipe.predict(new_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a27376",
   "metadata": {},
   "source": [
    "# Section 5 â€” Esercizi Svolti\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Esercizio 13.1 â€” Pipeline Base per Dataset Numerico\n",
    "\n",
    "**Obiettivo**: Creare una pipeline per classificazione con preprocessing numerico.\n",
    "\n",
    "**Consegna**:\n",
    "1. Genera un dataset di classificazione con `make_classification` (500 campioni, 8 features)\n",
    "2. Aggiungi 10% di valori mancanti\n",
    "3. Crea una pipeline con: SimpleImputer â†’ StandardScaler â†’ SVC\n",
    "4. Valida con cross-validation a 5 fold\n",
    "5. Mostra i risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2def2f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ESERCIZIO 13.1 â€” Pipeline Base per Dataset Numerico\n",
      "============================================================\n",
      "\n",
      "1. Dataset generato: (500, 8)\n",
      "2. Valori mancanti aggiunti: 426 (10.7%)\n",
      "\n",
      "3. Pipeline creata:\n",
      "Pipeline(steps=[('simpleimputer', SimpleImputer()),\n",
      "                ('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(random_state=42))])\n",
      "\n",
      "4-5. Risultati Cross-Validation:\n",
      "     Scores per fold: ['0.930', '0.930', '0.940', '0.890', '0.910']\n",
      "     Mean Accuracy: 0.9200 Â± 0.0179\n",
      "\n",
      "âœ… Pipeline funzionante con preprocessing automatico!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SOLUZIONE ESERCIZIO 13.1 â€” Pipeline Base per Dataset Numerico\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ESERCIZIO 13.1 â€” Pipeline Base per Dataset Numerico\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Genera dataset\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=8,\n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n1. Dataset generato: {X.shape}\")\n",
    "\n",
    "# 2. Aggiungi 10% valori mancanti\n",
    "mask = np.random.random(X.shape) < 0.10\n",
    "X[mask] = np.nan\n",
    "\n",
    "n_missing = np.isnan(X).sum()\n",
    "print(f\"2. Valori mancanti aggiunti: {n_missing} ({n_missing/(X.shape[0]*X.shape[1])*100:.1f}%)\")\n",
    "\n",
    "# 3. Crea pipeline\n",
    "pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    SVC(kernel='rbf', random_state=42)\n",
    ")\n",
    "\n",
    "print(f\"\\n3. Pipeline creata:\")\n",
    "print(pipe)\n",
    "\n",
    "# 4. Cross-validation\n",
    "cv_scores = cross_val_score(pipe, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# 5. Risultati\n",
    "print(f\"\\n4-5. Risultati Cross-Validation:\")\n",
    "print(f\"     Scores per fold: {[f'{s:.3f}' for s in cv_scores]}\")\n",
    "print(f\"     Mean Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Pipeline funzionante con preprocessing automatico!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72a4e10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Esercizio 13.2 â€” Pipeline con ColumnTransformer\n",
    "\n",
    "**Obiettivo**: Costruire una pipeline completa per dati misti (numerici e categorici).\n",
    "\n",
    "**Consegna**:\n",
    "1. Usa il Titanic dataset (o creane uno simile)\n",
    "2. Identifica colonne numeriche e categoriche\n",
    "3. Crea un ColumnTransformer con:\n",
    "   - Numeriche: imputer(median) â†’ scaler\n",
    "   - Categoriche: imputer(most_frequent) â†’ one-hot\n",
    "4. Aggiungi un classificatore (RandomForest)\n",
    "5. Valuta con train/test split e mostra accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "401caa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ESERCIZIO 13.2 â€” Pipeline con ColumnTransformer\n",
      "============================================================\n",
      "\n",
      "1. Dataset creato: (600, 7)\n",
      "   Pclass     Sex        Age  SibSp        Fare Embarked  Survived\n",
      "0       2    male  33.913596      0   42.618125        C         0\n",
      "1       3  female  14.986637      2   24.702602        S         0\n",
      "2       3  female  41.088324      2  115.128282        S         0\n",
      "3       3  female  27.781174      1   44.723426        C         1\n",
      "4       1    male  23.727324      1   53.696144        S         0\n",
      "\n",
      "Valori mancanti:\n",
      "Pclass       0\n",
      "Sex          0\n",
      "Age         80\n",
      "SibSp        0\n",
      "Fare         0\n",
      "Embarked    15\n",
      "Survived     0\n",
      "dtype: int64\n",
      "\n",
      "2. Colonne identificate:\n",
      "   Numeriche: ['Age', 'Fare', 'SibSp', 'Pclass']\n",
      "   Categoriche: ['Sex', 'Embarked']\n",
      "\n",
      "3. ColumnTransformer creato\n",
      "4. Pipeline completa con RandomForest\n",
      "\n",
      "5. Risultati:\n",
      "   Training samples: 450\n",
      "   Test samples: 150\n",
      "   Test Accuracy: 0.8200\n",
      "\n",
      "   Feature trasformate (9):\n",
      "   1. num__Age\n",
      "   2. num__Fare\n",
      "   3. num__SibSp\n",
      "   4. num__Pclass\n",
      "   5. cat__Sex_female\n",
      "   6. cat__Sex_male\n",
      "   7. cat__Embarked_C\n",
      "   8. cat__Embarked_Q\n",
      "   9. cat__Embarked_S\n",
      "\n",
      "âœ… Pipeline con dati misti completata!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SOLUZIONE ESERCIZIO 13.2 â€” Pipeline con ColumnTransformer\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ESERCIZIO 13.2 â€” Pipeline con ColumnTransformer\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Creiamo un dataset simil-Titanic\n",
    "np.random.seed(42)\n",
    "n = 600\n",
    "\n",
    "titanic_like = pd.DataFrame({\n",
    "    'Pclass': np.random.choice([1, 2, 3], n, p=[0.2, 0.3, 0.5]),\n",
    "    'Sex': np.random.choice(['male', 'female'], n),\n",
    "    'Age': np.random.normal(30, 12, n).clip(1, 80),\n",
    "    'SibSp': np.random.choice([0, 1, 2, 3], n, p=[0.6, 0.25, 0.10, 0.05]),\n",
    "    'Fare': np.abs(np.random.normal(50, 30, n)),\n",
    "    'Embarked': np.random.choice(['S', 'C', 'Q'], n, p=[0.7, 0.2, 0.1])\n",
    "})\n",
    "\n",
    "# Target: sopravvivenza (correlata a classe, sesso, etÃ )\n",
    "survival_prob = (\n",
    "    0.3 \n",
    "    + 0.2 * (titanic_like['Sex'] == 'female')\n",
    "    + 0.15 * (titanic_like['Pclass'] == 1)\n",
    "    - 0.01 * titanic_like['Age']\n",
    ")\n",
    "titanic_like['Survived'] = (np.random.random(n) < survival_prob.clip(0, 1)).astype(int)\n",
    "\n",
    "# Aggiungi valori mancanti realistici\n",
    "titanic_like.loc[np.random.choice(n, 80, replace=False), 'Age'] = np.nan\n",
    "titanic_like.loc[np.random.choice(n, 15, replace=False), 'Embarked'] = np.nan\n",
    "\n",
    "print(f\"\\n1. Dataset creato: {titanic_like.shape}\")\n",
    "print(titanic_like.head())\n",
    "print(f\"\\nValori mancanti:\\n{titanic_like.isnull().sum()}\")\n",
    "\n",
    "# 2. Identificare colonne\n",
    "numeric_cols = ['Age', 'Fare', 'SibSp', 'Pclass']\n",
    "categorical_cols = ['Sex', 'Embarked']\n",
    "\n",
    "print(f\"\\n2. Colonne identificate:\")\n",
    "print(f\"   Numeriche: {numeric_cols}\")\n",
    "print(f\"   Categoriche: {categorical_cols}\")\n",
    "\n",
    "# 3. ColumnTransformer\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "print(f\"\\n3. ColumnTransformer creato\")\n",
    "\n",
    "# 4. Pipeline completa con RandomForest\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "print(f\"4. Pipeline completa con RandomForest\")\n",
    "\n",
    "# 5. Train/test split e valutazione\n",
    "X = titanic_like.drop('Survived', axis=1)\n",
    "y = titanic_like['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "accuracy = full_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\n5. Risultati:\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "print(f\"   Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Bonus: feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "print(f\"\\n   Feature trasformate ({len(feature_names)}):\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"   {i+1}. {name}\")\n",
    "\n",
    "print(\"\\nâœ… Pipeline con dati misti completata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb5c09",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Esercizio 13.3 â€” Pipeline con GridSearchCV\n",
    "\n",
    "**Obiettivo**: Usare GridSearchCV per ottimizzare una pipeline completa.\n",
    "\n",
    "**Consegna**:\n",
    "1. Crea una pipeline: StandardScaler â†’ SVC\n",
    "2. Definisci una griglia di iperparametri per SVC (C, kernel)\n",
    "3. Usa GridSearchCV con 5-fold CV\n",
    "4. Mostra i migliori parametri e score\n",
    "5. Valuta sul test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d058c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ESERCIZIO 13.3 â€” Pipeline con GridSearchCV\n",
      "============================================================\n",
      "Dataset: (400, 10)\n",
      "Train: (300, 10), Test: (100, 10)\n",
      "\n",
      "1. Pipeline creata:\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(random_state=42))])\n",
      "\n",
      "2. Griglia iperparametri:\n",
      "   svc__C: [0.1, 1, 10, 100]\n",
      "   svc__kernel: ['linear', 'rbf', 'poly']\n",
      "   svc__gamma: ['scale', 'auto']\n",
      "\n",
      "3. GridSearchCV con 5-fold CV...\n",
      "\n",
      "4. Risultati GridSearchCV:\n",
      "   Miglior CV score: 0.9200\n",
      "   Migliori parametri:\n",
      "      svc__C: 10\n",
      "      svc__gamma: scale\n",
      "      svc__kernel: rbf\n",
      "\n",
      "5. Test Accuracy (con migliori parametri): 0.9000\n",
      "\n",
      "   Top 5 combinazioni:\n",
      "    rank_test_score  mean_test_score  std_test_score  \\\n",
      "13                1         0.920000        0.022111   \n",
      "16                1         0.920000        0.022111   \n",
      "10                3         0.913333        0.033993   \n",
      "7                 3         0.913333        0.033993   \n",
      "19                5         0.910000        0.029059   \n",
      "\n",
      "                                               params  \n",
      "13  {'svc__C': 10, 'svc__gamma': 'scale', 'svc__ke...  \n",
      "16  {'svc__C': 10, 'svc__gamma': 'auto', 'svc__ker...  \n",
      "10  {'svc__C': 1, 'svc__gamma': 'auto', 'svc__kern...  \n",
      "7   {'svc__C': 1, 'svc__gamma': 'scale', 'svc__ker...  \n",
      "19  {'svc__C': 100, 'svc__gamma': 'scale', 'svc__k...  \n",
      "\n",
      "âœ… GridSearchCV ottimizza TUTTA la pipeline automaticamente!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SOLUZIONE ESERCIZIO 13.3 â€” Pipeline con GridSearchCV\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ESERCIZIO 13.3 â€” Pipeline con GridSearchCV\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dataset\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(\n",
    "    n_samples=400,\n",
    "    n_features=10,\n",
    "    n_informative=6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {X.shape}\")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# 1. Crea pipeline\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(random_state=42)\n",
    ")\n",
    "\n",
    "print(f\"\\n1. Pipeline creata:\")\n",
    "print(pipe)\n",
    "\n",
    "# 2. Griglia iperparametri\n",
    "# NOTA: per accedere ai parametri usiamo stepname__param\n",
    "# make_pipeline usa nomi automatici: 'standardscaler', 'svc'\n",
    "param_grid = {\n",
    "    'svc__C': [0.1, 1, 10, 100],\n",
    "    'svc__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'svc__gamma': ['scale', 'auto']  # solo per rbf/poly\n",
    "}\n",
    "\n",
    "print(f\"\\n2. Griglia iperparametri:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"   {param}: {values}\")\n",
    "\n",
    "# 3. GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # usa tutti i core\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\n3. GridSearchCV con 5-fold CV...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 4. Migliori parametri e score\n",
    "print(f\"\\n4. Risultati GridSearchCV:\")\n",
    "print(f\"   Miglior CV score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"   Migliori parametri:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"      {param}: {value}\")\n",
    "\n",
    "# 5. Valutazione su test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f\"\\n5. Test Accuracy (con migliori parametri): {test_score:.4f}\")\n",
    "\n",
    "# Mostra top 5 combinazioni\n",
    "print(f\"\\n   Top 5 combinazioni:\")\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results = results.sort_values('rank_test_score')\n",
    "cols = ['rank_test_score', 'mean_test_score', 'std_test_score', 'params']\n",
    "print(results[cols].head())\n",
    "\n",
    "print(\"\\nâœ… GridSearchCV ottimizza TUTTA la pipeline automaticamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115a9cb",
   "metadata": {},
   "source": [
    "# Conclusione\n",
    "\n",
    "## ğŸ§  Bignami â€” Pipeline di Scikit-Learn\n",
    "\n",
    "### Concetti Chiave\n",
    "\n",
    "| Concetto | Descrizione |\n",
    "|----------|-------------|\n",
    "| **Data Leakage** | Usare info dal test set durante il training (errore grave!) |\n",
    "| **Pipeline** | Catena di trasformazioni + modello in un oggetto |\n",
    "| **ColumnTransformer** | Applica trasformazioni diverse a colonne diverse |\n",
    "| **make_pipeline** | Shortcut: genera nomi automatici |\n",
    "| **GridSearchCV** | Cerca parametri ottimali nella pipeline completa |\n",
    "\n",
    "### Sintassi Essenziale\n",
    "\n",
    "```python\n",
    "# Pipeline base\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('step1', Transformer1()),\n",
    "    ('step2', Transformer2()),\n",
    "    ('model', Classifier())\n",
    "])\n",
    "\n",
    "# Equivalente con make_pipeline\n",
    "pipe = make_pipeline(Transformer1(), Transformer2(), Classifier())\n",
    "\n",
    "# ColumnTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('num', num_pipe, num_cols),\n",
    "    ('cat', cat_pipe, cat_cols)\n",
    "])\n",
    "\n",
    "# Accesso parametri per GridSearchCV\n",
    "param_grid = {'stepname__parameter': [val1, val2]}\n",
    "```\n",
    "\n",
    "### Vantaggi delle Pipeline\n",
    "\n",
    "1. âœ… **Prevengono data leakage** - fit solo su training data\n",
    "2. âœ… **Codice pulito** - niente variabili intermedie\n",
    "3. âœ… **Riproducibile** - un oggetto contiene tutto\n",
    "4. âœ… **Produzione-ready** - save/load con joblib\n",
    "5. âœ… **Hyperparameter tuning** - GridSearchCV nativo\n",
    "\n",
    "### Errori da Evitare\n",
    "\n",
    "| âŒ Errore | âœ… Soluzione |\n",
    "|-----------|-------------|\n",
    "| Fit scaler su tutto X | Usare Pipeline con CV |\n",
    "| Dimenticare handle_unknown | `OneHotEncoder(handle_unknown='ignore')` |\n",
    "| Variabili intermedie | Tutto in Pipeline |\n",
    "| Salvare modello separato | `joblib.dump(pipeline, 'model.pkl')` |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Prossima Lezione\n",
    "\n",
    "**Lezione 14**: Alberi Decisionali e Random Forest\n",
    "- Come funziona un Decision Tree\n",
    "- Gini vs Entropy\n",
    "- Ensemble: Bagging e Random Forest\n",
    "- Feature Importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
