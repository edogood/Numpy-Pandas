{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e086e21",
   "metadata": {},
   "source": [
    "# 1) Titolo e obiettivi\n",
    "\n",
    "Lezione 26: Anomaly Detection - Trovare l'ago nel pagliaio con metodi non supervisionati\n",
    "\n",
    "---\n",
    "\n",
    "## Mappa della lezione\n",
    "\n",
    "| Sezione | Contenuto | Tempo stimato |\n",
    "|---------|-----------|---------------|\n",
    "| 1 | Titolo, obiettivi, tassonomia anomalie | 5 min |\n",
    "| 2 | Teoria profonda: IF, LOF, soglie, contamination | 20 min |\n",
    "| 3 | Schema mentale: workflow detection | 5 min |\n",
    "| 4 | Demo: IF base, LOF locale, tuning, frodi, ensemble | 30 min |\n",
    "| 5 | Esercizi guidati + debug | 15 min |\n",
    "| 6 | Conclusione operativa | 10 min |\n",
    "| 7 | Checklist di fine lezione + glossario | 5 min |\n",
    "| 8 | Changelog didattico | 2 min |\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivi della lezione\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "| # | Obiettivo | Verifica |\n",
    "|---|-----------|----------|\n",
    "| 1 | Distinguere **anomalie globali, locali, contestuali** | Sai descrivere un esempio per ciascuna? |\n",
    "| 2 | Applicare **Isolation Forest** | Sai usare contamination e decision_function? |\n",
    "| 3 | Applicare **Local Outlier Factor** | Sai scegliere n_neighbors? |\n",
    "| 4 | Scegliere **soglie e contamination** con criterio | Sai usare percentili e F1 per tuning? |\n",
    "| 5 | Valutare con **precision/recall/F1** | Sai interpretare falsi positivi vs recall? |\n",
    "| 6 | Combinare **ensemble IF + LOF** | Sai fare AND/OR tra predizioni? |\n",
    "\n",
    "---\n",
    "\n",
    "## L'idea centrale: cosa sono le anomalie\n",
    "\n",
    "```\n",
    "DISTRIBUZIONE NORMALE:                 ANOMALIE:\n",
    "\n",
    "    ●●●●●●●●●●●                       Globale: ★ (lontano da tutto)\n",
    "   ●●●●●●●●●●●●●                      \n",
    "  ●●●●●●●●●●●●●●●                     Locale: ○ (in zona meno densa)\n",
    "   ●●●●●●●●●●●●●                      \n",
    "    ●●●●●●●●●●●          ★            Contestuale: punto normale, \n",
    "       cluster                          ma anomalo a Natale\n",
    "```\n",
    "\n",
    "**Anomaly detection:** trovare i punti che \"non appartengono\" al pattern normale.\n",
    "\n",
    "---\n",
    "\n",
    "## Perché è difficile\n",
    "\n",
    "| Sfida | Descrizione | Conseguenza |\n",
    "|-------|-------------|-------------|\n",
    "| **Classi sbilanciate** | 0.1%-5% di anomalie | Accuracy inutile |\n",
    "| **Assenza di etichette** | Spesso non sai cosa è anomalo | Non puoi fare supervised |\n",
    "| **Soglia da scegliere** | Dove metti il cutoff? | Trade-off precision/recall |\n",
    "| **Dati eterogenei** | Scale diverse, feature miste | Scaling obbligatorio |\n",
    "\n",
    "---\n",
    "\n",
    "## I 2 algoritmi chiave: Isolation Forest vs LOF\n",
    "\n",
    "```\n",
    "ISOLATION FOREST:                      LOCAL OUTLIER FACTOR:\n",
    "\n",
    "Domanda: \"Quanti tagli                Domanda: \"La mia zona è\n",
    "servono per isolare                   meno densa di quella\n",
    "questo punto?\"                        dei miei vicini?\"\n",
    "\n",
    "    ┌───────────────┐                    ●●●●  densità alta\n",
    "    │   ● ●   ★     │                    ● ○   densità bassa\n",
    "    │ ●   ●         │                    ↑\n",
    "    └───────────────┘                    outlier locale!\n",
    "         ↑                               \n",
    "   Anomalia isolata                   LOF > 1 → anomalo\n",
    "   con 2 split!                       LOF ≈ 1 → normale\n",
    "```\n",
    "\n",
    "| Aspetto | Isolation Forest | LOF |\n",
    "|---------|------------------|-----|\n",
    "| **Tipo anomalie** | Globali, sparse | Locali, in cluster densi |\n",
    "| **Scalabilità** | Ottima (O(n log n)) | Peggiore (O(n² k)) |\n",
    "| **Output** | decision_function (score) | negative_outlier_factor_ |\n",
    "| **Quando preferire** | Tante feature, anomalie lontane | Poche feature, anomalie in mezzo |\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisiti\n",
    "\n",
    "| Concetto | Dove lo trovi | Verifica |\n",
    "|----------|---------------|----------|\n",
    "| StandardScaler | Lezione 13, 20 | Sai perché scalare prima di IF/LOF? |\n",
    "| Alberi decisionali | Lezione 14 | Capisci come IF fa split casuali? |\n",
    "| k-NN concetto | Lezione base | Sai cosa sono i k vicini? |\n",
    "| Precision/Recall/F1 | Lezione 17 | Sai interpretare trade-off? |\n",
    "\n",
    "**Cosa useremo:** make_blobs, StandardScaler, IsolationForest, LocalOutlierFactor, classification_report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3816d89",
   "metadata": {},
   "source": [
    "# 2) Teoria concettuale\n",
    "## 2.1 Cosa sono le anomalie e perche' sono difficili\n",
    "- Un'anomalia e' un'osservazione che si discosta nettamente dal comportamento atteso.\n",
    "- Tipi: globali (lontane da tutto), locali (in zone meno dense), contestuali (anomale solo in certi contesti/tempi).\n",
    "- Sfide: classi sbilanciate, assenza di etichette, soglie da impostare, dati spesso non scalati e con feature eterogenee.\n",
    "## 2.2 Algoritmi chiave\n",
    "- Isolation Forest: isola i punti con tagli casuali; anomalie richiedono meno tagli. Input matrice (n_samples, n_features) scalata; output etichette (+1 regolare, -1 anomalo) e decision_function (score). Errori tipici: contamination incoerente, dati non scalati, troppi estimators lenti.\n",
    "- Local Outlier Factor (LOF): confronta densita' locale con quella dei vicini. Input matrice (n_samples, n_features), output etichette (-1 anomalo) e negative_outlier_factor_. Errori tipici: n_neighbors troppo alto/basso, dati non scalati.\n",
    "## 2.3 Metriche e soglie\n",
    "- Con etichette: precision, recall, F1, confusion matrix. Con punteggi continui: PR curve, ROC, percentili come soglia.\n",
    "- Contamination: stima della % di anomalie. Se troppo alta genera falsi positivi; se troppo bassa perde anomalie.\n",
    "- Regola pratica: partire da 1-5% e fare tuning empirico con F1/recall in base alle priorita' business.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0a22c",
   "metadata": {},
   "source": [
    "## 2.2 Quando usare IF o LOF\n",
    "- Preferisci Isolation Forest se le anomalie sono sparse e globali, e vuoi un modello che scala bene con molte feature.\n",
    "- Preferisci LOF se le anomalie sono locali (zone meno dense dentro cluster) e hai dati 2D/3D o pochi attributi dopo PCA.\n",
    "- Se non conosci il tipo di anomalia, prova entrambi e confronta F1/recall in base al costo degli errori.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70b4d27",
   "metadata": {},
   "source": [
    "## 2.3 Soglie, percentili e interpretazione degli score\n",
    "- Isolation Forest: `decision_function` restituisce score (alto = normale). Scegli soglie con percentili (es. 95-esimo) o usa `contamination` per fissare la percentuale di anomalie.\n",
    "- LOF: `negative_outlier_factor_` (piu' negativo = piu' anomalo). Puoi ordinare gli score e scegliere un cutoff.\n",
    "- Interpretazione: confronta sempre score/soglie con esempi noti di anomalie per validare che il modello non stia marcando punti plausibili come outlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d0b08",
   "metadata": {},
   "source": [
    "# 3) Schema mentale / mappa decisionale\n",
    "Workflow: load -> check/clean -> scale -> scegli modello (IF/LOF) -> stima contamination/soglia -> valuta -> interpreta -> iterazioni.\n",
    "Decision map sintetica:\n",
    "1. Scala le feature numeriche (StandardScaler/RobustScaler).\n",
    "2. Se hai label di anomalie, usa PR/F1 per scegliere soglia o contamination.\n",
    "3. Prova sia Isolation Forest (globali) sia LOF (locali); confronta.\n",
    "4. Se anomalie sono poche e vicine a cluster densi, LOF e' spesso migliore; se sono sparse globali, Isolation Forest.\n",
    "5. Documenta la soglia scelta e verifica stabilita' su run multipli.\n",
    "Micro-checklist: nessun NaN, forme coerenti, contamination compatibile con la % attesa, almeno alcuni punti marcati come anomali.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de206ae",
   "metadata": {},
   "source": [
    "# 4) Sezione dimostrativa\n",
    "Panoramica demo:\n",
    "- Demo 1: Isolation Forest su dataset sintetico, metriche e grafico.\n",
    "- Demo 2: LOF su anomalie locali vs globali.\n",
    "- Demo 3: Tuning della contamination con F1.\n",
    "- Demo 4: Simulazione frodi con Isolation Forest.\n",
    "- Demo 5: Ensemble IF + LOF e confronto strategie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2524b8",
   "metadata": {},
   "source": [
    "## Demo 1 - Isolation Forest base\n",
    "Perche': mostrare isolamento rapido di anomalie globali su dati scalati.\n",
    "Metodi: `StandardScaler` (input n_samples x n_features, output scalato), `IsolationForest` (etichette +1/-1, decision_function). Checkpoint: nessun NaN, contamination coerente, almeno alcune anomalie rilevate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83d7baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: Isolation Forest su dataset sintetico\n",
    "# Scopo: generare dati con poche anomalie globali, scalare le feature, applicare Isolation Forest e valutare precision/recall/F1.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.close('all')\n",
    "\n",
    "# 1) Crea dataset con anomalie\n",
    "X_normal, _ = make_blobs(n_samples=800, centers=3, cluster_std=0.6, random_state=42)\n",
    "anomalies = np.random.uniform(low=-6, high=6, size=(40, 2))\n",
    "X = np.vstack([X_normal, anomalies])\n",
    "y_true = np.hstack([np.zeros(len(X_normal)), np.ones(len(anomalies))]).astype(int)\n",
    "print(f\"Forma dati: {X.shape}, anomalie attese: {y_true.sum()}\")\n",
    "assert X.shape[0] == y_true.shape[0], \"Shape incoerente\"\n",
    "assert not np.isnan(X).any(), \"NaN nei dati\"\n",
    "\n",
    "# 2) Scala le feature\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(f\"Forma dopo scaling: {X_scaled.shape}\")\n",
    "\n",
    "# 3) Isolation Forest con contamination ipotizzata al 5%\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "y_pred = iso.fit_predict(X_scaled)\n",
    "y_pred_bin = (y_pred == -1).astype(int)\n",
    "\n",
    "# 4) Metriche\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred_bin, average='binary', zero_division=0)\n",
    "print(classification_report(y_true, y_pred_bin, digits=3))\n",
    "print(f\"Confusion matrix:{confusion_matrix(y_true, y_pred_bin)}\")\n",
    "print(f\"Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}\")\n",
    "assert y_pred_bin.sum() > 0, \"Nessuna anomalia rilevata\"\n",
    "\n",
    "# 5) Visualizzazione\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_pred_bin, cmap='coolwarm', s=15, alpha=0.8)\n",
    "ax.set_title('Isolation Forest: 0=normale, 1=anomalia (scalato)')\n",
    "ax.set_xlabel('Feature 1 (scaled)')\n",
    "ax.set_ylabel('Feature 2 (scaled)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f0123",
   "metadata": {},
   "source": [
    "## Demo 2 - Local Outlier Factor\n",
    "Perche': gestire anomalie locali che un modello globale potrebbe ignorare. Metodi: `LocalOutlierFactor` con n_neighbors e contamination. Checkpoint: almeno un'anomalia trovata, differenza tra anomalie globali e locali.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a55de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Local Outlier Factor (LOF)\n",
    "# Scopo: evidenziare anomalie locali e globali con LOF su dati 2D scalati.\n",
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEMO 2 - Local Outlier Factor\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dataset con cluster denso e sparso + anomalie\n",
    "cluster_dense = np.random.randn(200, 2) * 0.3\n",
    "cluster_sparse = np.random.randn(120, 2) * 2 + [5, 5]\n",
    "local_anomalies = np.array([[2.5, 2.5], [2.0, 3.0], [3.0, 2.0], [-2, -2], [-2.5, -1.5]])\n",
    "global_anomalies = np.array([[10, 10], [-8, 8]])\n",
    "X_lof = np.vstack([cluster_dense, cluster_sparse, local_anomalies, global_anomalies])\n",
    "y_true_lof = np.hstack([\n",
    "    np.zeros(len(cluster_dense) + len(cluster_sparse)),\n",
    "    np.ones(len(local_anomalies) + len(global_anomalies))\n",
    "]).astype(int)\n",
    "\n",
    "scaler_lof = StandardScaler()\n",
    "X_lof_scaled = scaler_lof.fit_transform(X_lof)\n",
    "print(f\"Forma dataset LOF: {X_lof_scaled.shape}\")\n",
    "assert not np.isnan(X_lof_scaled).any(), \"NaN nei dati LOF\"\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "y_pred_lof = lof.fit_predict(X_lof_scaled)\n",
    "y_pred_lof_bin = (y_pred_lof == -1).astype(int)\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true_lof, y_pred_lof_bin, average='binary', zero_division=0)\n",
    "print(f\"Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}\")\n",
    "assert y_pred_lof_bin.sum() > 0, \"LOF non ha rilevato anomalie\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.scatter(X_lof_scaled[:, 0], X_lof_scaled[:, 1], c=y_pred_lof_bin, cmap='coolwarm', s=15, alpha=0.8)\n",
    "ax.set_title('LOF: 0=normale, 1=anomalia (scalato)')\n",
    "ax.set_xlabel('Feature 1 (scaled)')\n",
    "ax.set_ylabel('Feature 2 (scaled)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ffa86",
   "metadata": {},
   "source": [
    "## Demo 3 - Tuning del parametro contamination\n",
    "Perche': la % di anomalie attese impatta fortemente precision/recall. Strategia: grid manuale su contamination e confronto F1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ee823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 3: tuning del parametro contamination\n",
    "# Scopo: mostrare l'impatto di contamination su precision/recall/F1 con Isolation Forest (dataset Demo 1).\n",
    "contamination_values = [0.01, 0.02, 0.05, 0.10]\n",
    "\n",
    "results_cont = []\n",
    "for cont in contamination_values:\n",
    "    iso_tune = IsolationForest(contamination=cont, random_state=42)\n",
    "    pred = iso_tune.fit_predict(X_scaled)\n",
    "    pred_bin = (pred == -1).astype(int)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, pred_bin, average='binary', zero_division=0)\n",
    "    results_cont.append({\"contamination\": cont, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"anomalie_predette\": pred_bin.sum()})\n",
    "\n",
    "res_df = pd.DataFrame(results_cont).sort_values(by='f1', ascending=False)\n",
    "print(res_df)\n",
    "\n",
    "best_row = res_df.iloc[0]\n",
    "print(f\"Miglior contamination per F1: {best_row['contamination']}, F1={best_row['f1']:.3f}\")\n",
    "assert best_row['anomalie_predette'] > 0, \"Nessuna anomalia individuata nel tuning\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c2400",
   "metadata": {},
   "source": [
    "## Demo 4 - Anomaly detection simulazione frodi\n",
    "Perche': applicare Isolation Forest a un dataset sbilanciato con feature eterogenee (importo, orario, distanza). Checkpoint: recall accettabile sulle frodi e controllo falsi positivi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b4181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 4: anomaly detection su simulazione frodi\n",
    "# Scopo: simulare transazioni (2% frodi), scalare feature eterogenee e valutare Isolation Forest.\n",
    "np.random.seed(42)\n",
    "\n",
    "n_normal = 5000\n",
    "n_fraud = 100  # 2% frodi\n",
    "\n",
    "# Transazioni normali\n",
    "normal_amount = np.abs(np.random.exponential(50, n_normal))\n",
    "normal_hour = np.random.normal(14, 4, n_normal) % 24\n",
    "normal_freq = np.random.poisson(5, n_normal)\n",
    "normal_distance = np.abs(np.random.normal(20, 10, n_normal))\n",
    "\n",
    "# Transazioni fraudolente\n",
    "fraud_amount = np.abs(np.random.exponential(200, n_fraud))\n",
    "fraud_hour = (np.random.normal(2, 3, n_fraud) % 24)\n",
    "fraud_freq = np.random.poisson(1, n_fraud)\n",
    "fraud_distance = np.abs(np.random.normal(200, 50, n_fraud))\n",
    "\n",
    "X_fraud = np.vstack([\n",
    "    np.column_stack([normal_amount, normal_hour, normal_freq, normal_distance]),\n",
    "    np.column_stack([fraud_amount, fraud_hour, fraud_freq, fraud_distance])\n",
    "])\n",
    "y_fraud = np.hstack([np.zeros(n_normal), np.ones(n_fraud)]).astype(int)\n",
    "print(f\"Forma X_fraud: {X_fraud.shape}, frodi attese: {y_fraud.sum()}\")\n",
    "assert not np.isnan(X_fraud).any(), \"NaN nel dataset frodi\"\n",
    "\n",
    "scaler_fraud = StandardScaler()\n",
    "X_fraud_scaled = scaler_fraud.fit_transform(X_fraud)\n",
    "\n",
    "iso_fraud = IsolationForest(contamination=0.02, random_state=42)\n",
    "pred_fraud = iso_fraud.fit_predict(X_fraud_scaled)\n",
    "pred_fraud_bin = (pred_fraud == -1).astype(int)\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_fraud, pred_fraud_bin, average='binary', zero_division=0)\n",
    "print(classification_report(y_fraud, pred_fraud_bin, digits=3))\n",
    "print(f\"Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}\")\n",
    "assert pred_fraud_bin.sum() > 0, \"Nessuna frode rilevata\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b61a72",
   "metadata": {},
   "source": [
    "## Demo 5 - Ensemble IF + LOF\n",
    "Perche': combinare punti di forza di modelli globali e locali. Strategie: AND (entrambi dicono anomalia) e OR (almeno uno). Valutiamo precision/recall/F1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c174975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 5: ensemble Isolation Forest + LOF\n",
    "# Scopo: confrontare strategie AND/OR combinando IF e LOF sul dataset frodi.\n",
    "lof_fraud = LocalOutlierFactor(n_neighbors=25, contamination=0.02)\n",
    "# fit_predict restituisce -1 per anomalie\n",
    "pred_lof = lof_fraud.fit_predict(X_fraud_scaled)\n",
    "pred_lof_bin = (pred_lof == -1).astype(int)\n",
    "\n",
    "# Strategie di combinazione\n",
    "pred_and = ((pred_fraud_bin == 1) & (pred_lof_bin == 1)).astype(int)\n",
    "pred_or = ((pred_fraud_bin == 1) | (pred_lof_bin == 1)).astype(int)\n",
    "\n",
    "rows = []\n",
    "for name, pred_vec in [(\"IsolationForest\", pred_fraud_bin), (\"LOF\", pred_lof_bin), (\"AND\", pred_and), (\"OR\", pred_or)]:\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_fraud, pred_vec, average='binary', zero_division=0)\n",
    "    rows.append({\"metodo\": name, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"anomalie_predette\": pred_vec.sum()})\n",
    "\n",
    "res_ensemble = pd.DataFrame(rows).sort_values(by='f1', ascending=False)\n",
    "print(res_ensemble)\n",
    "assert len(res_ensemble) == 4, \"Risultati ensemble mancanti\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3220b6c0",
   "metadata": {},
   "source": [
    "# 5) Esercizi svolti (passo-passo)\n",
    "## Esercizio 26.1 - Monitoraggio sensori industriali\n",
    "Obiettivo: generare letture sensori, inserire anomalie e rilevarle con Isolation Forest, controllando recall e falsi positivi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a34f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 26.1: monitoraggio sensori industriali\n",
    "# Passi: crea dati sensori, aggiungi anomalie, scala, applica Isolation Forest, valuta recall e falsi positivi.\n",
    "np.random.seed(42)\n",
    "\n",
    "n_normal_sensors = 10000\n",
    "n_anomaly_sensors = 100\n",
    "\n",
    "# Letture normali (macchinario funzionante)\n",
    "temp_normal = np.random.normal(70, 5, n_normal_sensors)\n",
    "pressure_normal = np.random.normal(100, 10, n_normal_sensors)\n",
    "vibration_normal = np.random.normal(0.5, 0.1, n_normal_sensors)\n",
    "rpm_normal = np.random.normal(3000, 100, n_normal_sensors)\n",
    "energy_normal = np.random.normal(50, 5, n_normal_sensors)\n",
    "\n",
    "# Anomalie (surriscaldamento, pressione anomala)\n",
    "temp_anom = np.random.normal(95, 3, n_anomaly_sensors)\n",
    "pressure_anom = np.random.normal(140, 5, n_anomaly_sensors)\n",
    "vibration_anom = np.random.normal(1.0, 0.2, n_anomaly_sensors)\n",
    "rpm_anom = np.random.normal(3200, 80, n_anomaly_sensors)\n",
    "energy_anom = np.random.normal(70, 6, n_anomaly_sensors)\n",
    "\n",
    "X_sensors = np.vstack([\n",
    "    np.column_stack([temp_normal, pressure_normal, vibration_normal, rpm_normal, energy_normal]),\n",
    "    np.column_stack([temp_anom, pressure_anom, vibration_anom, rpm_anom, energy_anom])\n",
    "])\n",
    "y_sensors = np.hstack([np.zeros(n_normal_sensors), np.ones(n_anomaly_sensors)]).astype(int)\n",
    "print(f\"Shape dati sensori: {X_sensors.shape}, anomalie attese: {y_sensors.sum()}\")\n",
    "assert not np.isnan(X_sensors).any(), \"NaN nei dati sensori\"\n",
    "\n",
    "scaler_sensors = StandardScaler()\n",
    "X_sensors_scaled = scaler_sensors.fit_transform(X_sensors)\n",
    "\n",
    "iso_sensors = IsolationForest(contamination=0.01, random_state=42)\n",
    "pred_sensors = iso_sensors.fit_predict(X_sensors_scaled)\n",
    "pred_sensors_bin = (pred_sensors == -1).astype(int)\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_sensors, pred_sensors_bin, average='binary', zero_division=0)\n",
    "print(f\"Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}\")\n",
    "print(f\"Anomalie rilevate: {pred_sensors_bin.sum()} su {y_sensors.sum()}\")\n",
    "assert rec > 0, \"Recall nulla: alzare contamination\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4b6f2",
   "metadata": {},
   "source": [
    "## Esercizio 26.2 - Confronto LOF vs Isolation Forest\n",
    "Obiettivo: su dati con anomalie globali e locali, confrontare F1 e scegliere l'algoritmo piu' adatto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 26.2: confronto LOF vs Isolation Forest\n",
    "# Passi: dati con anomalie globali e locali, applica entrambi e confronta F1.\n",
    "np.random.seed(42)\n",
    "\n",
    "cluster_dense = np.random.randn(500, 2) * 0.5\n",
    "cluster_sparse = np.random.randn(500, 2) * 1.5 + [6, 6]\n",
    "global_anom = np.array([[15, 15], [-10, 10], [10, -10], [-10, -10], [0, 15]])\n",
    "local_anom = np.array([[3, 3], [2.5, 3.5], [3.5, 2.5], [4, 4], [2, 4]])\n",
    "\n",
    "X_comp = np.vstack([cluster_dense, cluster_sparse, global_anom, local_anom])\n",
    "y_comp = np.hstack([np.zeros(len(cluster_dense) + len(cluster_sparse)), np.ones(len(global_anom) + len(local_anom))]).astype(int)\n",
    "print(f\"Shape dataset confronto: {X_comp.shape}\")\n",
    "assert not np.isnan(X_comp).any(), \"NaN nei dati\"\n",
    "\n",
    "scaler_comp = StandardScaler()\n",
    "X_comp_scaled = scaler_comp.fit_transform(X_comp)\n",
    "\n",
    "iso_comp = IsolationForest(contamination=0.02, random_state=42)\n",
    "pred_iso = (iso_comp.fit_predict(X_comp_scaled) == -1).astype(int)\n",
    "lof_comp = LocalOutlierFactor(n_neighbors=30, contamination=0.02)\n",
    "pred_lof = (lof_comp.fit_predict(X_comp_scaled) == -1).astype(int)\n",
    "\n",
    "rows = []\n",
    "for name, preds in [(\"IsolationForest\", pred_iso), (\"LOF\", pred_lof)]:\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_comp, preds, average='binary', zero_division=0)\n",
    "    rows.append({\"modello\": name, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"anomalie_predette\": preds.sum()})\n",
    "\n",
    "res_comp = pd.DataFrame(rows)\n",
    "print(res_comp)\n",
    "assert len(res_comp) == 2, \"Risultati mancanti\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7bf020",
   "metadata": {},
   "source": [
    "## Esercizio 26.3 - Ottimizzazione della soglia\n",
    "Obiettivo: usare gli anomaly score di Isolation Forest sulle frodi e scegliere la soglia migliore (percentile) per massimizzare F1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esercizio 26.3: ottimizzazione soglia su anomaly score\n",
    "# Passi: usa gli score di Isolation Forest sul dataset frodi e seleziona il percentile che massimizza F1.\n",
    "from numpy import percentile\n",
    "\n",
    "scores = -iso_fraud.decision_function(X_fraud_scaled)  # piu' alto = piu' anomalo\n",
    "print(f\"Score: min {scores.min():.3f}, max {scores.max():.3f}, mean {scores.mean():.3f}\")\n",
    "assert scores.shape[0] == y_fraud.shape[0], \"Shape inconsistente\"\n",
    "\n",
    "percentiles = [90, 92, 94, 95, 96, 97, 98]\n",
    "rows = []\n",
    "for p in percentiles:\n",
    "    thr = percentile(scores, p)\n",
    "    preds = (scores >= thr).astype(int)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_fraud, preds, average='binary', zero_division=0)\n",
    "    rows.append({\"percentile\": p, \"threshold\": thr, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"anomalie_predette\": preds.sum()})\n",
    "\n",
    "res_thr = pd.DataFrame(rows).sort_values(by='f1', ascending=False)\n",
    "print(res_thr)\n",
    "\n",
    "best = res_thr.iloc[0]\n",
    "print(f\"Miglior soglia: percentile {best['percentile']} con F1={best['f1']:.3f}\")\n",
    "assert best['anomalie_predette'] > 0, \"Soglia troppo alta, nessuna anomalia trovata\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b534fb",
   "metadata": {},
   "source": [
    "# 6) Conclusione operativa\n",
    "\n",
    "## 5 take-home messages\n",
    "\n",
    "| # | Messaggio | Perché importante |\n",
    "|---|-----------|-------------------|\n",
    "| 1 | **IF per globali, LOF per locali** | Algoritmi diversi per pattern diversi |\n",
    "| 2 | **Contamination = stima % anomalie** | Troppo alta → falsi positivi, troppo bassa → miss |\n",
    "| 3 | **Sempre scalare prima** | IF e LOF sono sensibili alla scala |\n",
    "| 4 | **Valuta con F1, non accuracy** | Classi sbilanciate rendono accuracy inutile |\n",
    "| 5 | **Ensemble IF + LOF** | AND (conservativo) o OR (aggressivo) |\n",
    "\n",
    "---\n",
    "\n",
    "## Confronto sintetico: quando usare quale metodo\n",
    "\n",
    "| Situazione | Metodo consigliato | Perché |\n",
    "|------------|-------------------|--------|\n",
    "| Anomalie sparse globali | Isolation Forest | Isola velocemente punti lontani |\n",
    "| Anomalie locali in cluster | LOF | Confronta densità locale |\n",
    "| Non sai il tipo | Entrambi + confronto F1 | Sicurezza empirica |\n",
    "| Molte feature (>50) | IF (o IF su PCA) | LOF scala male |\n",
    "| Serve interpretabilità | IF con decision_function | Score continuo intuitivo |\n",
    "\n",
    "---\n",
    "\n",
    "## Perché questi metodi funzionano\n",
    "\n",
    "### 1) Isolation Forest - intuizione\n",
    "\n",
    "```\n",
    "Punto normale:                   Punto anomalo:\n",
    "\n",
    "    ┌─────────────────┐             ┌─────────────────┐\n",
    "    │ ┌───────────┐   │             │ ★               │\n",
    "    │ │ ┌───────┐ │   │             └─────────────────┘\n",
    "    │ │ │ ●     │ │   │                  1 split!\n",
    "    │ │ └───────┘ │   │\n",
    "    │ └───────────┘   │\n",
    "    └─────────────────┘\n",
    "         5 split...\n",
    "\n",
    "Anomalia = pochi split per isolare → path corto nell'albero\n",
    "```\n",
    "\n",
    "### 2) LOF - intuizione\n",
    "\n",
    "```\n",
    "Densità punti A: 10 vicini in raggio 0.5 → alta\n",
    "Densità punto B: 2 vicini in raggio 0.5 → bassa\n",
    "\n",
    "LOF(B) = densità_media_vicini(B) / densità(B) > 1 → outlier!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Metodi spiegati: reference card\n",
    "\n",
    "| Metodo | Input | Output | Quando usarlo |\n",
    "|--------|-------|--------|---------------|\n",
    "| `IsolationForest(contamination)` | X scalato | labels (+1/-1), decision_function | Anomalie globali |\n",
    "| `LocalOutlierFactor(n_neighbors)` | X scalato | labels (-1 anomalo), negative_outlier_factor_ | Anomalie locali |\n",
    "| `decision_function(X)` | X nuovo | score (alto = normale) | Soglie custom |\n",
    "| `fit_predict(X)` | X train | labels | Stesso set |\n",
    "| `StandardScaler` | X raw | X scalato | Prima di IF/LOF |\n",
    "\n",
    "---\n",
    "\n",
    "## Errori comuni e debug rapido\n",
    "\n",
    "| Errore | Perché sbagliato | Fix |\n",
    "|--------|-----------------|-----|\n",
    "| Non scalare | IF/LOF sensibili alla scala | StandardScaler prima |\n",
    "| contamination = 0.5 | Metà dataset come anomalie? | Usa 0.01-0.05 realistici |\n",
    "| n_neighbors = 1 in LOF | Troppo locale, instabile | Usa 20-50 |\n",
    "| Valutare con accuracy | 99% accuracy con 1% anomalie è inutile | Usa F1, precision, recall |\n",
    "| Ignorare false positive | Costo business! | Bilancia con recall |\n",
    "\n",
    "---\n",
    "\n",
    "## Template completo Anomaly Detection\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 1) Carica e scala\n",
    "X = ...  # tua matrice\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2) Stima contamination (se hai label)\n",
    "contamination_stima = y_true.mean() if 'y_true' in dir() else 0.05\n",
    "\n",
    "# 3) Isolation Forest\n",
    "iso = IsolationForest(contamination=contamination_stima, random_state=42)\n",
    "labels_iso = iso.fit_predict(X_scaled)\n",
    "labels_iso = (labels_iso == -1).astype(int)  # 1 = anomalo\n",
    "\n",
    "# 4) LOF\n",
    "lof = LocalOutlierFactor(n_neighbors=30, contamination=contamination_stima)\n",
    "labels_lof = lof.fit_predict(X_scaled)\n",
    "labels_lof = (labels_lof == -1).astype(int)  # 1 = anomalo\n",
    "\n",
    "# 5) Ensemble (AND = conservativo, OR = aggressivo)\n",
    "labels_and = labels_iso & labels_lof  # entrambi dicono anomalo\n",
    "labels_or = labels_iso | labels_lof   # almeno uno dice anomalo\n",
    "\n",
    "# 6) Valuta (se hai y_true)\n",
    "if 'y_true' in dir():\n",
    "    print(\"Isolation Forest:\")\n",
    "    print(classification_report(y_true, labels_iso))\n",
    "    print(\"LOF:\")\n",
    "    print(classification_report(y_true, labels_lof))\n",
    "    print(\"Ensemble AND:\")\n",
    "    print(f\"F1: {f1_score(y_true, labels_and):.3f}\")\n",
    "\n",
    "# 7) Soglia custom con decision_function\n",
    "scores = iso.decision_function(X_scaled)\n",
    "threshold = np.percentile(scores, 5)  # 5% come anomalie\n",
    "labels_custom = (scores < threshold).astype(int)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Prossimi passi\n",
    "\n",
    "| Argomento | Collegamento |\n",
    "|-----------|--------------|\n",
    "| One-Class SVM | Alternativa a IF per confini non-lineari |\n",
    "| Autoencoder | Anomaly detection con reconstruction error |\n",
    "| Time series | Anomalie temporali con rolling statistics |\n",
    "| Domain-specific | Frodi, intrusioni, difetti industriali |\n",
    "\n",
    "**Fine del corso unsupervised!** Hai ora tutti gli strumenti per clustering, riduzione e anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f77a58",
   "metadata": {},
   "source": [
    "# 7) Checklist di fine lezione\n",
    "- [ ] Ho scalato le feature prima di applicare IF/LOF.\n",
    "- [ ] Ho scelto contamination o soglie in linea con la % attesa di anomalie.\n",
    "- [ ] Ho verificato che almeno alcune anomalie vengano identificate.\n",
    "- [ ] Ho confrontato precision/recall/F1 e documentato il trade-off.\n",
    "- [ ] Ho testato almeno un modello globale (IF) e uno locale (LOF) se il contesto lo richiede.\n",
    "- [ ] Ho salvato la soglia/scaler per riprodurre i risultati.\n",
    "\n",
    "Glossario (termini usati):\n",
    "- Anomalia/outlier: osservazione lontana dal comportamento atteso.\n",
    "- Contamination: stima percentuale di anomalie usata dal modello.\n",
    "- Isolation Forest: algoritmo basato su alberi casuali per isolare punti rari.\n",
    "- Local Outlier Factor: algoritmo basato su densita' locale.\n",
    "- n_neighbors: numero di vicini considerati da LOF.\n",
    "- decision_function: score continuo di anomalia/normalita'.\n",
    "- Precision: quota di predetti anomali che sono davvero anomali.\n",
    "- Recall: quota di anomalie reali identificate.\n",
    "- F1-score: media armonica precision/recall.\n",
    "- Threshold: soglia sullo score per decidere anomalie.\n",
    "- False positive: punto normale marcato come anomalo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Changelog didattico\n",
    "\n",
    "| Versione | Data | Modifiche |\n",
    "|----------|------|-----------|\n",
    "| 1.0 | 2024-01-20 | Creazione: IF e LOF base, metriche |\n",
    "| 1.1 | 2024-01-28 | Aggiunto tuning contamination, demo frodi |\n",
    "| 2.0 | 2024-02-05 | Integrata sezione ensemble IF + LOF |\n",
    "| 2.1 | 2024-02-12 | Refactor con assert e checklist |\n",
    "| **2.3** | **2024-12-19** | **ESPANSIONE COMPLETA:** mappa lezione 8 sezioni, tabella obiettivi, ASCII IF vs LOF, tassonomia anomalie (globali/locali/contestuali), confronto scalabilità, 5 take-home messages, template completo con ensemble AND/OR, soglia custom con percentili, reference card metodi, errori comuni |\n",
    "\n",
    "---\n",
    "\n",
    "## Note per lo studente\n",
    "\n",
    "Questa lezione chiude il modulo **Unsupervised Learning**:\n",
    "\n",
    "| Lezione | Argomento | Cosa hai imparato |\n",
    "|---------|-----------|-------------------|\n",
    "| 19 | Intro Unsupervised | Paradigma senza label |\n",
    "| 20 | K-Means | Clustering a centroidi |\n",
    "| 21 | Scelta K | Elbow, Silhouette, Gap |\n",
    "| 22 | Gerarchico | Dendrogrammi e linkage |\n",
    "| 23 | DBSCAN | Clustering a densità |\n",
    "| 24 | PCA | Riduzione dimensionale |\n",
    "| 25 | PCA + Clustering | Combinazione workflow |\n",
    "| **26** | **Anomaly Detection** | **IF, LOF, soglie, ensemble** |\n",
    "\n",
    "**Congratulazioni!** Hai completato il percorso dal clustering base fino all'anomaly detection.\n",
    "\n",
    "**Prossimi passi consigliati:**\n",
    "- Esercizi di consolidamento\n",
    "- Progetti reali con dati tabulari\n",
    "- Approfondimenti: UMAP, autoencoders, time series"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
