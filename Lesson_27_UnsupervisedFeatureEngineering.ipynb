{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ae6d78",
   "metadata": {},
   "source": [
    "# 1) Titolo e obiettivi\n",
    "\n",
    "Lezione 27: Unsupervised Feature Engineering - Creare feature potenti senza etichette\n",
    "\n",
    "---\n",
    "\n",
    "## Mappa della lezione\n",
    "\n",
    "| Sezione | Contenuto | Tempo stimato |\n",
    "|---------|-----------|---------------|\n",
    "| 1 | Titolo, obiettivi, perché feature engineering unsupervised | 5 min |\n",
    "| 2 | Teoria profonda: 3 famiglie di feature | 15 min |\n",
    "| 3 | Schema mentale: workflow creazione e selezione | 5 min |\n",
    "| 4 | Demo: PCA features, cluster features, anomaly scores | 30 min |\n",
    "| 5 | Esercizi guidati + pipeline completa | 15 min |\n",
    "| 6 | Conclusione operativa | 10 min |\n",
    "| 7 | Checklist di fine lezione + glossario | 5 min |\n",
    "| 8 | Changelog didattico | 2 min |\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivi della lezione\n",
    "\n",
    "Al termine di questa lezione sarai in grado di:\n",
    "\n",
    "| # | Obiettivo | Verifica |\n",
    "|---|-----------|----------|\n",
    "| 1 | Creare **feature da PCA** | Sai estrarre PC, loadings, reconstruction error? |\n",
    "| 2 | Creare **feature da clustering** | Sai usare label, distanze, probabilità GMM? |\n",
    "| 3 | Creare **feature da anomaly detection** | Sai usare IF/LOF scores come colonne? |\n",
    "| 4 | **Selezionare feature** senza label | Sai applicare VarianceThreshold e filtro correlazione? |\n",
    "| 5 | Costruire **pipeline end-to-end** | Sai concatenare tutto in un workflow riproducibile? |\n",
    "\n",
    "---\n",
    "\n",
    "## L'idea centrale: arricchire il dataset senza label\n",
    "\n",
    "```\n",
    "DATI ORIGINALI:                  DATI ARRICCHITI:\n",
    "\n",
    "X1   X2   X3   X4   X5          X1  X2  X3  X4  X5  PC1  PC2  cluster  dist_centroid  anomaly_score\n",
    "─────────────────────   +   ──────────────────────────────────────────────────────────────────────\n",
    "10   20   30   40   50          10  20  30  40  50  2.1  -0.5    0         0.12           0.03\n",
    "...                             ...\n",
    "\n",
    "Nuove feature:                   Vecchie feature filtrate:\n",
    "├── PC1, PC2 (compressione)     └── Rimosso X3 (correlato 0.98 con X2)\n",
    "├── cluster_label               └── Rimosso X5 (varianza < 0.01)\n",
    "├── dist_centroid\n",
    "└── anomaly_score_IF\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Le 3 famiglie di feature unsupervised\n",
    "\n",
    "| Famiglia | Feature prodotte | Quando utili |\n",
    "|----------|------------------|--------------|\n",
    "| **PCA** | Componenti (PC), loadings, reconstruction_error | Ridurre rumore, decorrelation |\n",
    "| **Clustering** | label, dist_centroid, cluster_prob (GMM) | Segmentazione, segnali di gruppo |\n",
    "| **Anomaly** | IF_score, LOF_score, anomaly_flag | Risk features, segnali di qualità |\n",
    "\n",
    "---\n",
    "\n",
    "## Perché è potente\n",
    "\n",
    "| Vantaggio | Spiegazione |\n",
    "|-----------|-------------|\n",
    "| **Non serve target** | Puoi arricchire PRIMA di avere label |\n",
    "| **Cattura struttura** | Cluster, densità, varianza sono informativi |\n",
    "| **Migliora modelli downstream** | XGBoost con cluster_label performa meglio |\n",
    "| **Riduce rumore** | VarianceThreshold e correlazione eliminano feature inutili |\n",
    "\n",
    "---\n",
    "\n",
    "## Schema workflow\n",
    "\n",
    "```\n",
    "┌──────────────┐     ┌──────────────┐     ┌──────────────┐\n",
    "│   Raw Data   │ →   │    Scale     │ →   │   Crea FE    │\n",
    "└──────────────┘     └──────────────┘     └──────────────┘\n",
    "                                                  │\n",
    "                           ┌──────────────────────┼──────────────────────┐\n",
    "                           │                      │                      │\n",
    "                    ┌──────▼──────┐       ┌───────▼───────┐      ┌───────▼───────┐\n",
    "                    │  PCA feat   │       │ Cluster feat  │      │ Anomaly feat  │\n",
    "                    └──────┬──────┘       └───────┬───────┘      └───────┬───────┘\n",
    "                           │                      │                      │\n",
    "                           └──────────────────────┼──────────────────────┘\n",
    "                                                  ▼\n",
    "                                     ┌──────────────────────┐\n",
    "                                     │   Concatena tutto    │\n",
    "                                     └──────────────────────┘\n",
    "                                                  │\n",
    "                           ┌──────────────────────┼──────────────────────┐\n",
    "                           │                      │                      │\n",
    "                    ┌──────▼──────┐       ┌───────▼───────┐      ┌───────▼───────┐\n",
    "                    │ Var filter  │       │ Corr filter   │      │  Final DF     │\n",
    "                    └─────────────┘       └───────────────┘      └───────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisiti\n",
    "\n",
    "| Concetto | Dove lo trovi | Verifica |\n",
    "|----------|---------------|----------|\n",
    "| PCA | Lezione 24 | Sai estrarre componenti e varianza? |\n",
    "| K-Means, GMM | Lezioni 20, clustering | Sai usare fit_predict e transform? |\n",
    "| IF, LOF | Lezione 26 | Sai estrarre decision_function? |\n",
    "| Correlazione | Lezione base | Sai calcolare corr() e filtrarla? |\n",
    "\n",
    "**Cosa useremo:** StandardScaler, PCA, KMeans, GaussianMixture, IsolationForest, LocalOutlierFactor, VarianceThreshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462abee",
   "metadata": {},
   "source": [
    "# 2) Teoria concettuale\n",
    "## 2.1 Perche' fare feature engineering unsupervised\n",
    "- Nuove feature migliorano separabilita' e stabilita' dei modelli, anche senza label.\n",
    "- Decorrelare (PCA) e sintetizzare distanze/score rende piu' semplice la fase di clustering/anomaly detection successiva.\n",
    "- Feature selection riduce rumore e tempi di calcolo, evitando overfitting su pattern casuali.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d2fc9",
   "metadata": {},
   "source": [
    "## 2.2 Tipi di feature da creare\n",
    "- Da PCA: componenti principali, varianza spiegata, reconstruction error per rilevare punti mal ricostruiti.\n",
    "- Da clustering: label di appartenenza, distanza dal centroide, probabilita' (con GMM) per catturare appartenenze soft.\n",
    "- Da anomaly detection: score IF/LOF e flag binari come segnali di rischio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf62c9",
   "metadata": {},
   "source": [
    "## 2.3 Selezione e valutazione\n",
    "- Variance threshold: elimina feature quasi costanti (input matrice scalata, output feature ridotte); errore tipico: applicarla prima dello scaling.\n",
    "- Filtro di correlazione: rimuove colonne altamente correlate per ridurre ridondanza; attenzione a mantenere le piu' interpretabili.\n",
    "- Valutazione: controlla distribuzioni, varianza spiegata e stabilita' dei cluster prima/ dopo le nuove feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8fc5ed",
   "metadata": {},
   "source": [
    "# 3) Schema mentale / mappa decisionale\n",
    "Workflow: carica -> scala -> crea nuove feature (PCA/cluster/score) -> concatena -> seleziona -> valida con metriche (varianza, correlazioni) -> usa nei modelli.\n",
    "Decision map sintetica:\n",
    "1. Scala le feature numeriche.\n",
    "2. Se servono feature dense e compatte: PCA.\n",
    "3. Se vuoi segnali di appartenenza: clustering per label/distanze.\n",
    "4. Se vuoi segnali di rischio: anomaly scores.\n",
    "5. Filtra feature a bassa varianza e ad alta correlazione.\n",
    "6. Documenta le trasformazioni (necessario per riproducibilita').\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73be78",
   "metadata": {},
   "source": [
    "# 4) Sezione dimostrativa\n",
    "- Demo 1: Features da PCA (componenti, loadings, reconstruction error).\n",
    "- Demo 2: Features da clustering (label e distanze ai centroidi).\n",
    "- Demo 3: Features da anomaly detection (score e flag).\n",
    "- Demo 4: Feature selection unsupervised (varianza e correlazione).\n",
    "- Demo 5: Pipeline completa di feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6cc58",
   "metadata": {},
   "source": [
    "## Demo 1 - Features da PCA\n",
    "Perche': ottenere feature decorrelate e misurare la varianza spiegata. Checkpoint: nessun NaN, varianza cumulata dichiarata, reconstruction error con media > 0 ma non enorme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf8f95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape iniziale: (500, 6)\n",
      "Varianza spiegata cumulata: 0.623\n",
      "  feature       PC1       PC2       PC3\n",
      "0  feat_0 -0.268777  0.689764 -0.185658\n",
      "1  feat_1  0.076210  0.347309  0.921509\n",
      "2  feat_2  0.030249  0.531830 -0.296600\n",
      "3  feat_3  0.557793  0.221670 -0.014882\n",
      "4  feat_4  0.645974 -0.129437 -0.050501\n",
      "Reconstruction error medio: 0.3769\n",
      "        pc1       pc2       pc3  recon_error\n",
      "0 -2.340224 -0.859562  0.832477     1.538057\n",
      "1  0.829370  1.355790 -2.610694     0.510795\n",
      "2  2.131989  3.079159  1.147079     0.282756\n",
      "3 -0.109150  0.187645 -1.400600     0.210626\n",
      "4  2.322739 -0.127321  1.362790     0.191162\n"
     ]
    }
   ],
   "source": [
    "# Demo 1: features da PCA\n",
    "# Scopo: creare componenti principali, analizzare loadings e calcolare reconstruction error come feature aggiuntiva.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.close('all')\n",
    "\n",
    "# Dataset simulato\n",
    "X, _ = make_classification(n_samples=500, n_features=6, n_informative=4, n_redundant=0, random_state=42)\n",
    "print(f\"Shape iniziale: {X.shape}\")\n",
    "assert not np.isnan(X).any(), \"NaN nei dati\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "var_cum = pca.explained_variance_ratio_.sum()\n",
    "print(f\"Varianza spiegata cumulata: {var_cum:.3f}\")\n",
    "assert X_pca.shape == (500, 3), \"Shape PCA inattesa\"\n",
    "\n",
    "# Loadings\n",
    "loadings = pd.DataFrame(pca.components_.T, columns=['PC1','PC2','PC3'])\n",
    "loadings['feature'] = [f'feat_{i}' for i in range(X.shape[1])]\n",
    "print(loadings[['feature','PC1','PC2','PC3']].head())\n",
    "\n",
    "# Reconstruction error\n",
    "X_reconstructed = pca.inverse_transform(X_pca)\n",
    "recon_error = np.mean((X_scaled - X_reconstructed)**2, axis=1)\n",
    "print(f\"Reconstruction error medio: {recon_error.mean():.4f}\")\n",
    "\n",
    "# Dataset arricchito\n",
    "pc_df = pd.DataFrame(X_pca, columns=['pc1','pc2','pc3'])\n",
    "pc_df['recon_error'] = recon_error\n",
    "print(pc_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a1703",
   "metadata": {},
   "source": [
    "## Demo 2 - Features da clustering\n",
    "Perche': usare appartenenza e distanza per arricchire il dataset. Checkpoint: cluster presenti, distanze finite, label in range previsto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75da6b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape blobs: (400, 4)\n",
      "     feat_0    feat_1    feat_2    feat_3  cluster_label  min_dist_centroid\n",
      "0  1.080266 -0.828261 -0.126870 -1.462834              2           0.264878\n",
      "1  1.306175 -0.788549 -0.215939 -1.687295              2           0.175003\n",
      "2 -0.486784  1.361946  1.996068 -0.157401              1           0.332965\n",
      "3  0.997877 -0.702270 -0.363898 -1.660943              2           0.399115\n",
      "4 -0.272549  1.217380  1.513722 -0.106040              1           0.271966\n",
      "Cluster unici: [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Demo 2: features da clustering\n",
    "# Scopo: aggiungere label e distanze ai centroidi come feature.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X_blobs, _ = make_blobs(n_samples=400, centers=4, n_features=4, cluster_std=1.0, random_state=42)\n",
    "print(f\"Shape blobs: {X_blobs.shape}\")\n",
    "assert not np.isnan(X_blobs).any(), \"NaN nei dati\"\n",
    "\n",
    "scaler_blobs = StandardScaler()\n",
    "X_blobs_scaled = scaler_blobs.fit_transform(X_blobs)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_blobs_scaled)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Distanze ai centroidi\n",
    "all_dists = cdist(X_blobs_scaled, centroids)\n",
    "min_dist = all_dists.min(axis=1)\n",
    "\n",
    "clust_df = pd.DataFrame(X_blobs_scaled, columns=[f'feat_{i}' for i in range(X_blobs.shape[1])])\n",
    "clust_df['cluster_label'] = labels\n",
    "clust_df['min_dist_centroid'] = min_dist\n",
    "\n",
    "print(clust_df.head())\n",
    "print(f\"Cluster unici: {np.unique(labels)}\")\n",
    "assert clust_df['cluster_label'].nunique() == 4, \"Numero cluster inatteso\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ea4c8",
   "metadata": {},
   "source": [
    "## Demo 3 - Features da anomaly detection\n",
    "Perche': aggiungere segnali di rischio tramite score IF/LOF. Checkpoint: almeno qualche anomalia individuata, score definiti per tutti i campioni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606cc946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cluster_label  min_dist_centroid  iso_score  lof_score  anomaly_flag\n",
      "0              2           0.264878  -0.081139   1.001623             0\n",
      "1              2           0.175003  -0.094364   0.949224             0\n",
      "2              1           0.332965  -0.057322   1.059820             0\n",
      "3              2           0.399115  -0.027677   1.170746             0\n",
      "4              1           0.271966  -0.085961   1.032514             0\n",
      "Anomalie rilevate (union IF/LOF): 28\n"
     ]
    }
   ],
   "source": [
    "# Demo 3: features da anomaly detection\n",
    "# Scopo: aggiungere score IF/LOF e flag binario come feature.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso.fit(X_blobs_scaled)\n",
    "iso_scores = -iso.decision_function(X_blobs_scaled)  # piu' alto = piu' anomalo\n",
    "iso_flag = (iso.predict(X_blobs_scaled) == -1).astype(int)\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "lof_preds = lof.fit_predict(X_blobs_scaled)\n",
    "lof_scores = -lof.negative_outlier_factor_\n",
    "lof_flag = (lof_preds == -1).astype(int)\n",
    "\n",
    "anom_df = clust_df.copy()\n",
    "anom_df['iso_score'] = iso_scores\n",
    "anom_df['lof_score'] = lof_scores\n",
    "anom_df['anomaly_flag'] = ((iso_flag + lof_flag) > 0).astype(int)\n",
    "\n",
    "print(anom_df[['cluster_label','min_dist_centroid','iso_score','lof_score','anomaly_flag']].head())\n",
    "print(f\"Anomalie rilevate (union IF/LOF): {anom_df['anomaly_flag'].sum()}\")\n",
    "assert anom_df['anomaly_flag'].sum() > 0, \"Nessuna anomalia rilevata\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252e813",
   "metadata": {},
   "source": [
    "## Demo 4 - Feature selection unsupervised\n",
    "Perche': rimuovere feature quasi costanti o ridondanti per snellire il dataset. Checkpoint: numero feature ridotto, nessun NaN dopo la selezione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88016f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       base     noisy   low_var     other\n",
      "0  0.496714  0.455264  0.500757  0.368673\n",
      "1 -0.138264 -0.166273  0.499078 -0.393339\n",
      "2  0.647689  0.685053  0.500870  0.028745\n",
      "3  1.523030  1.553548  0.501356  1.278452\n",
      "4 -0.234153 -0.235198  0.500413  0.191099\n",
      "Feature dopo variance threshold: ['base', 'noisy', 'low_var', 'other']\n",
      "Feature rimosse per alta correlazione: ['noisy']\n",
      "Shape finale: (300, 3)\n"
     ]
    }
   ],
   "source": [
    "# Demo 4: feature selection unsupervised\n",
    "# Scopo: rimuovere feature quasi costanti e altamente correlate.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 300\n",
    "base = np.random.randn(n)\n",
    "noisy = base + np.random.normal(0, 0.05, n)  # altamente correlata\n",
    "low_var = np.ones(n) * 0.5 + np.random.normal(0, 1e-3, n)  # quasi costante\n",
    "other = np.random.randn(n)\n",
    "\n",
    "raw_df = pd.DataFrame({\n",
    "    'base': base,\n",
    "    'noisy': noisy,\n",
    "    'low_var': low_var,\n",
    "    'other': other\n",
    "})\n",
    "print(raw_df.head())\n",
    "\n",
    "scaler_sel = StandardScaler()\n",
    "X_sel = scaler_sel.fit_transform(raw_df)\n",
    "\n",
    "vt = VarianceThreshold(threshold=0.01)\n",
    "X_vt = vt.fit_transform(X_sel)\n",
    "cols_vt = raw_df.columns[vt.get_support()]\n",
    "print(f\"Feature dopo variance threshold: {list(cols_vt)}\")\n",
    "\n",
    "# Filtro di correlazione\n",
    "sel_df = pd.DataFrame(X_vt, columns=cols_vt)\n",
    "corr = sel_df.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "final_df = sel_df.drop(columns=to_drop)\n",
    "print(f\"Feature rimosse per alta correlazione: {to_drop}\")\n",
    "print(f\"Shape finale: {final_df.shape}\")\n",
    "assert final_df.shape[1] >= 1, \"Troppe feature rimosse\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274412c3",
   "metadata": {},
   "source": [
    "## Demo 5 - Pipeline completa\n",
    "Perche': combinare PCA, clustering e anomaly scores in una sola funzione riutilizzabile. Checkpoint: tutte le feature generate senza NaN, documentazione dei passaggi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a246d21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     feat_0    feat_1    feat_2    feat_3    feat_4       pc1       pc2  \\\n",
      "0  0.468270  1.340971  0.577778  1.508467 -0.846079  0.912879  0.043047   \n",
      "1 -0.617514 -0.952589  0.899108  1.451887  1.424678 -0.964266  2.274673   \n",
      "2  0.900779  1.246943  0.519948  0.724840 -0.696687  0.759260 -0.422689   \n",
      "3 -0.972408 -0.975656  0.659182  1.070862  2.384483 -1.309268  2.618381   \n",
      "4 -0.590719  1.193593  0.840604 -1.351147 -0.652050  1.943754 -0.318552   \n",
      "\n",
      "   cluster_label  iso_score  \n",
      "0              0  -0.078389  \n",
      "1              4  -0.077979  \n",
      "2              0  -0.066929  \n",
      "3              4   0.003107  \n",
      "4              3  -0.077650  \n"
     ]
    }
   ],
   "source": [
    "# Demo 5: pipeline completa di feature engineering\n",
    "# Scopo: combinare PCA, clustering e anomaly scores in un'unica funzione.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X_pipe, _ = make_blobs(n_samples=300, centers=5, n_features=5, cluster_std=1.2, random_state=42)\n",
    "\n",
    "scaler_pipe = StandardScaler()\n",
    "X_pipe_scaled = scaler_pipe.fit_transform(X_pipe)\n",
    "\n",
    "pca_pipe = PCA(n_components=2, random_state=42)\n",
    "X_pipe_pca = pca_pipe.fit_transform(X_pipe_scaled)\n",
    "\n",
    "kmeans_pipe = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "labels_pipe = kmeans_pipe.fit_predict(X_pipe_scaled)\n",
    "\n",
    "iso_pipe = IsolationForest(contamination=0.03, random_state=42)\n",
    "iso_scores_pipe = -iso_pipe.fit(X_pipe_scaled).decision_function(X_pipe_scaled)\n",
    "\n",
    "feat_df = pd.DataFrame(X_pipe_scaled, columns=[f'feat_{i}' for i in range(X_pipe.shape[1])])\n",
    "feat_df['pc1'] = X_pipe_pca[:,0]\n",
    "feat_df['pc2'] = X_pipe_pca[:,1]\n",
    "feat_df['cluster_label'] = labels_pipe\n",
    "feat_df['iso_score'] = iso_scores_pipe\n",
    "\n",
    "print(feat_df.head())\n",
    "assert not feat_df.isna().any().any(), \"Feature generate con NaN\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83e75a",
   "metadata": {},
   "source": [
    "# 5) Esercizi svolti (passo-passo)\n",
    "## Esercizio 27.1 - Feature engineering per segmentazione clienti\n",
    "Obiettivo: creare componenti PCA, label/distanze KMeans e anomaly score per un dataset clienti simulato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb183b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     recency  frequency    monetary     tenure  returns\n",
      "0  14.078043          3  162.631454  34.351004        0\n",
      "1  90.303643          3  204.711293  35.639688        2\n",
      "2  39.502371          5  104.136746  40.637036        0\n",
      "3  27.388277          6  334.970444  46.849769        0\n",
      "4   5.088746          5  375.864185  46.902086        0\n",
      "        pc1       pc2       pc3  cluster_label  dist_centroid  iso_score\n",
      "0 -1.096118 -0.233232  0.895017              1       1.083076  -0.149867\n",
      "1  1.243700 -0.257985 -0.430436              0       2.726563  -0.005324\n",
      "2 -0.176321  0.002998  1.331163              1       1.485975  -0.145230\n",
      "3  0.602547  0.002029  1.813016              1       1.959760  -0.115807\n",
      "4  0.097305 -0.389702  1.803214              1       2.080375  -0.116216\n",
      "Cluster trovati: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Esercizio 27.1: feature engineering segmentazione clienti\n",
    "# Passi: dataset clienti simulato, PCA, clustering, distanze, anomaly score.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 600\n",
    "recency = np.random.exponential(30, n)\n",
    "frequency = np.random.poisson(5, n)\n",
    "monetary = np.random.gamma(2, 100, n)\n",
    "tenure = np.random.uniform(3, 48, n)\n",
    "returns = np.random.binomial(5, 0.1, n)\n",
    "\n",
    "customers = pd.DataFrame({\n",
    "    'recency': recency,\n",
    "    'frequency': frequency,\n",
    "    'monetary': monetary,\n",
    "    'tenure': tenure,\n",
    "    'returns': returns\n",
    "})\n",
    "print(customers.head())\n",
    "assert not customers.isna().any().any(), \"NaN nel dataset clienti\"\n",
    "\n",
    "scaler_cust = StandardScaler()\n",
    "X_cust_scaled = scaler_cust.fit_transform(customers)\n",
    "\n",
    "pca_cust = PCA(n_components=3, random_state=42)\n",
    "X_cust_pca = pca_cust.fit_transform(X_cust_scaled)\n",
    "\n",
    "kmeans_cust = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_cust = kmeans_cust.fit_predict(X_cust_scaled)\n",
    "centroids_cust = kmeans_cust.cluster_centers_\n",
    "dists_cust = cdist(X_cust_scaled, centroids_cust).min(axis=1)\n",
    "\n",
    "iso_cust = IsolationForest(contamination=0.03, random_state=42)\n",
    "iso_scores_cust = -iso_cust.fit(customers).decision_function(customers)\n",
    "\n",
    "cust_feat = pd.DataFrame(X_cust_pca, columns=['pc1','pc2','pc3'])\n",
    "cust_feat['cluster_label'] = labels_cust\n",
    "cust_feat['dist_centroid'] = dists_cust\n",
    "cust_feat['iso_score'] = iso_scores_cust\n",
    "\n",
    "print(cust_feat.head())\n",
    "print(f\"Cluster trovati: {np.unique(labels_cust)}\")\n",
    "assert cust_feat.shape[0] == n, \"Numero di righe incoerente\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ea672f",
   "metadata": {},
   "source": [
    "## Esercizio 27.2 - Feature selection su dataset ridondante\n",
    "Obiettivo: applicare variance threshold e filtro di correlazione confrontando la dimensionalita' prima/dopo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc9fa8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       base  redundant   low_var     indep\n",
      "0  0.496714   0.464826  0.300938  0.125225\n",
      "1 -0.138264  -0.150252  0.299484 -0.429406\n",
      "2  0.647689   0.647793  0.300096  0.122298\n",
      "3  1.523030   1.523969  0.299538  0.543298\n",
      "4 -0.234153  -0.243155  0.299566  0.048860\n",
      "Dopo variance threshold: ['base', 'redundant', 'low_var', 'indep']\n",
      "Feature rimosse per correlazione: ['redundant']\n",
      "Shape finale: (400, 3)\n"
     ]
    }
   ],
   "source": [
    "# Esercizio 27.2: feature selection\n",
    "# Passi: dataset ridondante, variance threshold, filtro di correlazione.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 400\n",
    "base = np.random.randn(n)\n",
    "redundant = base + np.random.normal(0, 0.02, n)\n",
    "low_var = np.ones(n) * 0.3 + np.random.normal(0, 1e-3, n)\n",
    "indep = np.random.randn(n)\n",
    "\n",
    "feat_df = pd.DataFrame({\n",
    "    'base': base,\n",
    "    'redundant': redundant,\n",
    "    'low_var': low_var,\n",
    "    'indep': indep\n",
    "})\n",
    "print(feat_df.head())\n",
    "\n",
    "scaler_fs = StandardScaler()\n",
    "X_fs = scaler_fs.fit_transform(feat_df)\n",
    "\n",
    "vt = VarianceThreshold(threshold=0.01)\n",
    "X_fs_vt = vt.fit_transform(X_fs)\n",
    "cols_vt = feat_df.columns[vt.get_support()]\n",
    "print(f\"Dopo variance threshold: {list(cols_vt)}\")\n",
    "\n",
    "corr_df = pd.DataFrame(X_fs_vt, columns=cols_vt)\n",
    "corr = corr_df.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [c for c in upper.columns if any(upper[c] > 0.85)]\n",
    "final_cols = [c for c in corr_df.columns if c not in to_drop]\n",
    "final_df = corr_df[final_cols]\n",
    "print(f\"Feature rimosse per correlazione: {to_drop}\")\n",
    "print(f\"Shape finale: {final_df.shape}\")\n",
    "assert final_df.shape[1] >= 1, \"Tutte le feature rimosse\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876354e2",
   "metadata": {},
   "source": [
    "## Esercizio 27.3 - Multi-grain feature engineering\n",
    "Obiettivo: usare clustering a granularita' diverse (k=2,4,8) per creare feature che catturano pattern gerarchici.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3cd0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cluster_k2   dist_k2  cluster_k4   dist_k4  cluster_k8   dist_k8\n",
      "0           1  1.212824           3  0.892908           1  0.339002\n",
      "1           1  0.866778           3  0.293014           6  0.057093\n",
      "2           1  0.915563           3  0.294158           6  0.480133\n",
      "3           0  1.285964           2  0.433740           7  0.149177\n",
      "4           0  1.304341           2  0.336715           2  0.164887\n"
     ]
    }
   ],
   "source": [
    "# Esercizio 27.3: multi-grain feature engineering\n",
    "# Passi: clustering a diverse granularita' per creare piu' feature di appartenenza/distanza.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X_mg, _ = make_blobs(n_samples=400, centers=8, cluster_std=0.8, n_features=3, random_state=42)\n",
    "scaler_mg = StandardScaler()\n",
    "X_mg_scaled = scaler_mg.fit_transform(X_mg)\n",
    "\n",
    "ks = [2, 4, 8]\n",
    "features = {}\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_mg_scaled)\n",
    "    dists = cdist(X_mg_scaled, km.cluster_centers_).min(axis=1)\n",
    "    features[f'cluster_k{k}'] = labels\n",
    "    features[f'dist_k{k}'] = dists\n",
    "\n",
    "mg_df = pd.DataFrame(features)\n",
    "print(mg_df.head())\n",
    "assert mg_df.shape[0] == X_mg.shape[0], \"Shape incoerente\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd219a54",
   "metadata": {},
   "source": [
    "# 6) Conclusione operativa\n",
    "\n",
    "## 5 take-home messages\n",
    "\n",
    "| # | Messaggio | Perché importante |\n",
    "|---|-----------|-------------------|\n",
    "| 1 | **PCA per decorrelazione** | PC sono combinazioni lineari ortogonali |\n",
    "| 2 | **Clustering per segmentazione** | label e distanze catturano appartenenza |\n",
    "| 3 | **Anomaly scores per rischio** | IF/LOF come feature numeriche continue |\n",
    "| 4 | **VarianceThreshold per noise** | Elimina feature quasi costanti |\n",
    "| 5 | **Filtro correlazione per ridondanza** | Rimuovi colonne duplicate di informazione |\n",
    "\n",
    "---\n",
    "\n",
    "## Confronto sintetico: quali feature creare\n",
    "\n",
    "| Situazione | Feature consigliate | Perché |\n",
    "|------------|---------------------|--------|\n",
    "| Troppe feature correlate | PCA (PC1, PC2, ...) | Decorrelazione |\n",
    "| Vuoi segnali di gruppo | cluster_label, dist_centroid | Segmenti interpretativi |\n",
    "| Vuoi segnali di rischio | IF_score, LOF_score | Anomalie come numeriche |\n",
    "| Feature quasi costanti | VarianceThreshold | Eliminazione automatica |\n",
    "| Matrice dataset finale | Concatena + filtra | Pulizia finale |\n",
    "\n",
    "---\n",
    "\n",
    "## Reference card: metodi usati\n",
    "\n",
    "| Metodo | Input | Output | Feature prodotte |\n",
    "|--------|-------|--------|------------------|\n",
    "| `PCA(n_components)` | X scalato | transform → PC | PC1, PC2, ..., recon_error |\n",
    "| `KMeans(k)` | X scalato | predict → label | cluster_label |\n",
    "| `km.transform(X)` | X scalato | distanze | dist_to_c0, dist_to_c1, ... |\n",
    "| `GaussianMixture(k)` | X scalato | predict_proba | prob_c0, prob_c1, ... |\n",
    "| `IsolationForest` | X scalato | decision_function | if_score |\n",
    "| `LocalOutlierFactor` | X scalato | negative_outlier_factor_ | lof_score |\n",
    "| `VarianceThreshold(threshold)` | X | X filtrato | meno colonne |\n",
    "\n",
    "---\n",
    "\n",
    "## Errori comuni e debug rapido\n",
    "\n",
    "| Errore | Perché sbagliato | Fix |\n",
    "|--------|-----------------|-----|\n",
    "| Feature NaN dopo concat | Indici non allineati | Usa .reset_index(drop=True) |\n",
    "| Distanze tutte simili | k troppo alto o basso | Prova diversi k |\n",
    "| Anomaly score tutti 0 | contamination = 0 | Usa valore realistico (0.01-0.05) |\n",
    "| Troppe colonne rimosse | Soglia varianza/correlazione troppo alta | Abbassa soglia |\n",
    "| Non scalare prima | Feature engineering su scale diverse | StandardScaler sempre |\n",
    "\n",
    "---\n",
    "\n",
    "## Template completo Feature Engineering Unsupervised\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# 1) Carica e scala\n",
    "X = ...  # DataFrame o array\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2) Feature da PCA\n",
    "pca = PCA(n_components=5, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "pca_cols = [f'PC{i+1}' for i in range(5)]\n",
    "df_pca = pd.DataFrame(X_pca, columns=pca_cols)\n",
    "# Reconstruction error\n",
    "X_recon = pca.inverse_transform(X_pca)\n",
    "df_pca['recon_error'] = np.mean((X_scaled - X_recon)**2, axis=1)\n",
    "\n",
    "# 3) Feature da clustering\n",
    "km = KMeans(n_clusters=4, random_state=42, n_init='auto')\n",
    "df_cluster = pd.DataFrame({\n",
    "    'cluster_label': km.fit_predict(X_scaled)\n",
    "})\n",
    "dists = km.transform(X_scaled)\n",
    "for i in range(dists.shape[1]):\n",
    "    df_cluster[f'dist_c{i}'] = dists[:, i]\n",
    "\n",
    "# 4) Feature da GMM (soft probabilities)\n",
    "gmm = GaussianMixture(n_components=4, random_state=42)\n",
    "probs = gmm.fit(X_scaled).predict_proba(X_scaled)\n",
    "df_gmm = pd.DataFrame(probs, columns=[f'prob_c{i}' for i in range(4)])\n",
    "\n",
    "# 5) Feature da anomaly detection\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso.fit(X_scaled)\n",
    "df_anomaly = pd.DataFrame({\n",
    "    'if_score': iso.decision_function(X_scaled)\n",
    "})\n",
    "lof = LocalOutlierFactor(n_neighbors=30, contamination=0.05)\n",
    "lof.fit_predict(X_scaled)\n",
    "df_anomaly['lof_score'] = lof.negative_outlier_factor_\n",
    "\n",
    "# 6) Concatena tutto\n",
    "X_df = pd.DataFrame(X_scaled, columns=[f'X{i}' for i in range(X_scaled.shape[1])])\n",
    "df_full = pd.concat([X_df, df_pca, df_cluster, df_gmm, df_anomaly], axis=1)\n",
    "\n",
    "# 7) Selezione: VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_var = selector.fit_transform(df_full)\n",
    "print(f\"Feature dopo VarianceThreshold: {X_var.shape[1]} / {df_full.shape[1]}\")\n",
    "\n",
    "# 8) Selezione: rimuovi correlate > 0.95\n",
    "corr_matrix = df_full.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "df_final = df_full.drop(columns=to_drop)\n",
    "print(f\"Feature dopo filtro correlazione: {df_final.shape[1]} / {df_full.shape[1]}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Prossimi passi\n",
    "\n",
    "| Lezione | Argomento | Collegamento |\n",
    "|---------|-----------|--------------|\n",
    "| 28 | Progetto End-to-End Unsupervised | Applica tutto su dataset reale |\n",
    "| 29+ | AI, NLP, Deep Learning | Estensioni avanzate |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d144cb9d",
   "metadata": {},
   "source": [
    "# 7) Checklist di fine lezione\n",
    "- [ ] Ho scalato le feature prima di PCA, clustering e anomaly detection.\n",
    "- [ ] Ho calcolato varianza spiegata e reconstruction error per le componenti PCA.\n",
    "- [ ] Ho verificato label e distanze dei cluster e che non ci siano NaN.\n",
    "- [ ] Ho aggiunto almeno uno score di anomalia come feature.\n",
    "- [ ] Ho applicato variance threshold e filtro di correlazione se presenti feature ridondanti.\n",
    "- [ ] Ho documentato le trasformazioni per riapplicarle in produzione.\n",
    "\n",
    "Glossario (termini usati):\n",
    "- Componenti principali (PC): combinazioni lineari delle feature originali che massimizzano la varianza.\n",
    "- Loadings: pesi delle feature sulle componenti.\n",
    "- Reconstruction error: differenza tra dati originali e ricostruiti dopo PCA.\n",
    "- Cluster label: appartenenza a un centroide.\n",
    "- Distanza dal centroide: metrica di vicinanza al proprio cluster.\n",
    "- Probabilita' di appartenenza: output soft di GMM.\n",
    "- Isolation score / LOF score: misure di anomalia (alto/basso = piu' anomalo a seconda del segno).\n",
    "- Variance threshold: rimozione feature con varianza minima.\n",
    "- Filtro di correlazione: rimozione di feature fortemente correlate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Changelog didattico\n",
    "\n",
    "| Versione | Data | Modifiche |\n",
    "|----------|------|-----------|\n",
    "| 1.0 | 2024-01-22 | Creazione: PCA e clustering features |\n",
    "| 1.1 | 2024-02-01 | Aggiunto anomaly scores come feature |\n",
    "| 2.0 | 2024-02-10 | Integrata pipeline completa con selezione |\n",
    "| 2.1 | 2024-02-15 | Refactor con controlli e checkpoint |\n",
    "| **2.3** | **2024-12-19** | **ESPANSIONE COMPLETA:** mappa lezione 8 sezioni, tabella obiettivi, ASCII workflow creazione/selezione, 3 famiglie feature (PCA/cluster/anomaly), 5 take-home messages, template completo con GMM probabilità, reference card metodi, filtro correlazione automatico, errori comuni |\n",
    "\n",
    "---\n",
    "\n",
    "## Note per lo studente\n",
    "\n",
    "Questa lezione chiude il toolkit **feature engineering senza label**:\n",
    "\n",
    "| Tecnica | Cosa produce | Quando usarla |\n",
    "|---------|--------------|---------------|\n",
    "| PCA | PC, recon_error | Decorrelazione, compressione |\n",
    "| Clustering | label, distanze, prob | Segmentazione, appartenenza |\n",
    "| Anomaly | scores | Segnali di rischio |\n",
    "| Selezione | meno colonne | Pulizia, velocità |\n",
    "\n",
    "**Workflow tipico:**\n",
    "1. Scale → 2. Crea feature → 3. Concatena → 4. Filtra → 5. Usa nei modelli\n",
    "\n",
    "**Prossima tappa:** Lesson 28 - Progetto End-to-End che mette insieme tutto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
